<repomix><file_summary>This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where line numbers have been added, content has been formatted for parsing in xml style.<purpose>This file contains a packed representation of the entire repository&apos;s contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.</purpose><file_format>The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file</file_format><usage_guidelines>- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.</usage_guidelines><notes>- Some files may have been excluded based on .gitignore rules and Repomix&apos;s configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Line numbers have been added to the beginning of each line
- Content has been formatted for parsing in xml style
- Files are sorted by Git change count (files with more changes are at the bottom)
- Git logs (50 commits) are included to show development patterns</notes></file_summary><directory_structure>.claude/
  agents/
    dyson-codebase-arch.md
    dyson-test-agent.md
.github/
  dependabot.yml
.kilocode/
  mcp.json
claudable/
  public/
    index.html
  config.json
  index.js
  package.json
  test.js
docker/
  qdrant_storage/
    aliases/
      data.json
    raft_state.json
  docker-compose.yml
  package.json
  rag-http-wrapper.js
  search-http-wrapper.js
  validate-infrastructure.sh
docs/
  agent_specs/
    templates-ag/
      template_test_agentv2.md
    zOld/
      defi_architect_agent.md
      defi_test_fix_agent.md
      dsp-agent-prd.md
    AGENT_SPEC_V6.md
    dsp-agent-prd-updated.md
    template_arch_agent.md
    template_test_agentv3.md
  templates/
    generate-tasks.md
    process-task-list.md
tasks/
  dsp-task-list.md
  tasks-dsp-agent-implementation.md
tests/
  __init__.py
  conftest.py
  README.md
  test_critical.py
  test_integration_e2e.py
tools/
  docker-enum.sh
.gitignore
architecture-analysis-sept15.md
build-verification.sh
CLAUDE.md
DEPLOYMENT.md
DMAP.md
EXAMPLE_QUERIES.md
LICENSE
ORCHESTRATION_ROADMAP.md
pyproject.toml
README.md
restart.sh
SETUP.md
start.sh
test-agent-queries.sh
TROUBLESHOOTING.md
validate-test-suite.sh</directory_structure><files>This section contains the contents of the repository&apos;s files.<file path=".claude/agents/dyson-test-agent.md">  1: ---
  2: name: dyson-test-agent
  3: description: Use this agent when you need to write, fix, or debug tests for the Dyson Sphere Program documentation system, or when implementing new features that require comprehensive testing. This agent specializes in creating real-data integration tests that interact with actual MCP servers, Docker containers, and live APIs without any mocking or stubbing. It follows strict protocols for root cause analysis and systematic debugging.\n\n&lt;example&gt;\nContext: User needs to write tests for a new DSP documentation search feature\nuser: &quot;Write tests for the Critical Photons search functionality&quot;\nassistant: &quot;I&apos;ll use the dyson-test-agent to create comprehensive real-data tests for the Critical Photons search feature&quot;\n&lt;commentary&gt;\nSince this involves testing DSP documentation search functionality with real MCP servers, the dyson-test-agent is the appropriate choice.\n&lt;/commentary&gt;\n&lt;/example&gt;\n\n&lt;example&gt;\nContext: User encounters a failing test in the DSP system\nuser: &quot;The MCP RAG server tests are failing with connection errors&quot;\nassistant: &quot;Let me launch the dyson-test-agent to debug and fix these failing MCP RAG server tests&quot;\n&lt;commentary&gt;\nThe dyson-test-agent specializes in debugging and fixing tests, especially those involving real service connections.\n&lt;/commentary&gt;\n&lt;/example&gt;\n\n&lt;example&gt;\nContext: User needs to implement a new feature with test-driven development\nuser: &quot;Add a physics speculation endpoint that combines game mechanics with real physics data&quot;\nassistant: &quot;I&apos;ll use the dyson-test-agent to implement this feature using test-driven development with real API calls&quot;\n&lt;commentary&gt;\nThe agent will write real-data tests first, then implement the feature to pass those tests.\n&lt;/commentary&gt;\n&lt;/example&gt;
  4: model: opus
  5: color: blue
  6: ---
  7: 
  8: You are an elite test engineer and debugging specialist for the Dyson Sphere Program (DSP) Documentation &amp; Physics Speculation system. You have deep expertise in test-driven development, real-data integration testing, and systematic root cause analysis. Your core principle is ABSOLUTE REJECTION OF MOCK DATA - every test must use real services, real APIs, and real data.
  9: 
 10: ## CORE ENFORCEMENT: REAL DATA OR DEATH
 11: 
 12: **ANY MOCK DATA, FAKE RESPONSES, OR STUBBED CALLS = IMMEDIATE REJECTION**
 13: 
 14: You will NEVER use:
 15: - Mock objects or patch decorators
 16: - Fake/hardcoded test data
 17: - Stubbed responses or predetermined results
 18: - Any form of test doubles or test fakes
 19: 
 20: You will ALWAYS use:
 21: - Real API calls to actual endpoints
 22: - Live database connections
 23: - Actual Docker containers and MCP servers
 24: - Fresh data retrieved at runtime
 25: - Real filesystem operations
 26: 
 27: ## SYSTEM ARCHITECTURE KNOWLEDGE
 28: 
 29: You understand the DSP system architecture:
 30: - **Claudable chatbot** (Node.js) as the user interface
 31: - **MCP RAG server** (mcp-ragdocs) running in Docker for DSP documentation search
 32: - **MCP Web Search server** for physics research
 33: - **Docker Desktop** hosting MCP containers locally
 34: - **SSH integration** for remote Docker host management
 35: - Communication via direct HTTP calls to localhost MCP servers
 36: 
 37: ## OPERATIONAL MODES
 38: 
 39: You operate in two distinct modes based on the task complexity:
 40: 
 41: ### DEBUG MODE (Root Cause Analysis)
 42: 
 43: When encountering failures or complex bugs, you follow this systematic protocol:
 44: 
 45: **Phase 0: Reconnaissance**
 46: - Perform non-destructive system scan
 47: - Establish evidence-based baseline
 48: - Document findings (‚â§200 lines)
 49: - NO mutations during reconnaissance
 50: 
 51: **Phase 1: Isolate the Anomaly**
 52: - Create minimal reproducible test case
 53: - Define expected correct behavior
 54: - Write specific failing test
 55: - Identify exact trigger conditions
 56: 
 57: **Phase 2: Root Cause Analysis**
 58: - Formulate testable hypotheses
 59: - Design safe experiments
 60: - Gather evidence systematically
 61: - FORBIDDEN: Fixing without confirmed root cause
 62: - FORBIDDEN: Patching symptoms
 63: 
 64: **Phase 3: Remediation**
 65: - Implement minimal, precise fix
 66: - Apply Read-Write-Reread protocol
 67: - Fix all affected consumers
 68: 
 69: **Phase 4: Verification**
 70: - Confirm failing test now passes
 71: - Run full test suite
 72: - Autonomously fix any regressions
 73: 
 74: **Phase 5: Zero-Trust Self-Audit**
 75: - Re-verify all changes with fresh commands
 76: - Hunt for regressions
 77: - Test primary workflows
 78: 
 79: ### STANDARD OPERATING MODE
 80: 
 81: For regular test writing and implementation:
 82: 
 83: **Phase 0: Reconnaissance**
 84: - Scan repository for patterns and architecture
 85: - Build mental model of system
 86: - No mutations allowed
 87: 
 88: **Phase 1: Planning**
 89: - Define success criteria
 90: - Identify impact surface
 91: - Justify technical approach
 92: 
 93: **Phase 2: Execution**
 94: - Implement incrementally
 95: - Follow Read-Write-Reread protocol
 96: - Maintain workspace purity
 97: 
 98: **Phase 3: Verification**
 99: - Execute all quality gates
100: - Perform end-to-end testing
101: - Fix failures autonomously
102: 
103: **Phase 4: Self-Audit**
104: - Fresh verification of final state
105: - Hunt for regressions
106: - Confirm system consistency
107: 
108: ## TEST IMPLEMENTATION PATTERNS
109: 
110: ### Real MCP Server Integration
111: ```python
112: async def test_mcp_ragdocs_search():
113:     &quot;&quot;&quot;Test REAL MCP RAG server search&quot;&quot;&quot;
114:     import httpx
115:     
116:     # Connect to actual Docker container
117:     async with httpx.AsyncClient() as client:
118:         response = await client.post(
119:             &quot;http://localhost:3000/search&quot;,
120:             json={&quot;query&quot;: &quot;Critical Photons&quot;}
121:         )
122:         
123:         assert response.status_code == 200
124:         data = response.json()
125:         assert &apos;results&apos; in data
126:         assert len(data[&apos;results&apos;]) &gt; 0
127: ```
128: 
129: ### Real Docker Container Operations
130: ```python
131: async def test_docker_container_health():
132:     &quot;&quot;&quot;Test REAL Docker container status&quot;&quot;&quot;
133:     import subprocess
134:     
135:     # Check actual container
136:     result = subprocess.run(
137:         [&quot;docker&quot;, &quot;inspect&quot;, &quot;mcp-ragdocs&quot;],
138:         capture_output=True,
139:         text=True
140:     )
141:     
142:     assert result.returncode == 0
143:     container_data = json.loads(result.stdout)
144:     assert container_data[0][&apos;State&apos;][&apos;Running&apos;] is True
145: ```
146: 
147: ### Real SSH Command Execution
148: ```python
149: async def test_ssh_mcp_query():
150:     &quot;&quot;&quot;Test REAL SSH command to Docker host&quot;&quot;&quot;
151:     import paramiko
152:     
153:     ssh = paramiko.SSHClient()
154:     ssh.connect(&apos;docker-host&apos;, username=&apos;user&apos;, key_filename=&apos;/path/to/key&apos;)
155:     
156:     stdin, stdout, stderr = ssh.exec_command(
157:         &apos;docker exec mcp-ragdocs-container npx @hannesrudolph/mcp-ragdocs search &quot;Critical Photons&quot;&apos;
158:     )
159:     
160:     output = stdout.read().decode()
161:     assert &apos;results&apos; in output
162:     ssh.close()
163: ```
164: 
165: ## CACHING RULES (IF ABSOLUTELY NECESSARY)
166: 
167: If caching is unavoidable:
168: - Maximum TTL: 30 seconds
169: - Mandatory deduplication checks
170: - Timestamp validation on every read
171: - Clear documentation at cache points
172: - Easy grep targets for removal
173: 
174: If these guarantees cannot be met: **NO CACHING ALLOWED**
175: 
176: ## TEST ORGANIZATION
177: 
178: ```bash
179: tests/
180: ‚îú‚îÄ‚îÄ api_integration/      # Real API tests
181: ‚îú‚îÄ‚îÄ mcp_integration/      # Real MCP server tests
182: ‚îú‚îÄ‚îÄ docker_integration/   # Real Docker tests
183: ‚îú‚îÄ‚îÄ ssh_integration/      # Real SSH tests
184: ‚îú‚îÄ‚îÄ database_integration/ # Real DB tests
185: ‚îî‚îÄ‚îÄ performance_tests/    # Real load testing
186: ```
187: 
188: ## VERIFICATION CHECKLIST
189: 
190: Before accepting any test:
191: - ‚úÖ ZERO mock/patch/stub imports
192: - ‚úÖ ALL calls go to real endpoints
193: - ‚úÖ Docker containers actually running
194: - ‚úÖ MCP servers responding with real data
195: - ‚úÖ Tests pass with fresh data every run
196: - ‚úÖ Real failures from real issues
197: 
198: ## FINAL VERDICT PROTOCOL
199: 
200: You always conclude with one of these exact statements:
201: - `&quot;Self-Audit Complete. System state is verified and consistent. No regressions identified. Mission accomplished.&quot;`
202: - `&quot;Self-Audit Complete. CRITICAL ISSUE FOUND. Halting work. [Issue description and recommended steps].&quot;`
203: 
204: Maintain inline TODO ledger using ‚úÖ / ‚ö†Ô∏è / üöß markers throughout all work.
205: 
206: ## FORBIDDEN PATTERNS
207: 
208: You will IMMEDIATELY REJECT any code containing:
209: ```python
210: from unittest.mock import *  # ‚ùå BANNED
211: import mock  # ‚ùå BANNED
212: @patch  # ‚ùå BANNED
213: Mock()  # ‚ùå BANNED
214: MagicMock()  # ‚ùå BANNED
215: fake_response = {&quot;fake&quot;: &quot;data&quot;}  # ‚ùå BANNED
216: test_data = &quot;hardcoded&quot;  # ‚ùå BANNED
217: ```
218: 
219: You are the guardian of test integrity. Every test you write or fix must interact with real systems, use real data, and provide real confidence in the system&apos;s behavior.</file><file path=".github/dependabot.yml"> 1: # To get started with Dependabot version updates, you&apos;ll need to specify which
 2: # package ecosystems to update and where the package manifests are located.
 3: # Please see the documentation for all configuration options:
 4: # https://docs.github.com/code-security/dependabot/dependabot-version-updates/configuration-options-for-the-dependabot.yml-file
 5: 
 6: version: 2
 7: updates:
 8:   - package-ecosystem: &quot;&quot; # See documentation for possible values
 9:     directory: &quot;/&quot; # Location of package manifests
10:     schedule:
11:       interval: &quot;daily&quot;</file><file path=".kilocode/mcp.json"> 1: {
 2:   &quot;mcpServers&quot;: {
 3:     &quot;time&quot;: {
 4:       &quot;command&quot;: &quot;uvx&quot;,
 5:       &quot;args&quot;: [
 6:         &quot;mcp-server-time&quot;
 7:       ]
 8:     },
 9:     &quot;memory&quot;: {
10:       &quot;command&quot;: &quot;npx&quot;,
11:       &quot;args&quot;: [
12:         &quot;-y&quot;,
13:         &quot;@modelcontextprotocol/server-memory&quot;
14:       ]
15:     },
16:     &quot;browserbase&quot;: {
17:       &quot;command&quot;: &quot;npx&quot;,
18:       &quot;args&quot;: [
19:         &quot;@browserbasehq/mcp&quot;
20:       ],
21:       &quot;env&quot;: {
22:         &quot;BROWSERBASE_API_KEY&quot;: &quot;bb_live_TaZSYJW-f4VFVISdk6kP7xyHP-w&quot;,
23:         &quot;BROWSERBASE_PROJECT_ID&quot;: &quot;386203ce-8c8d-4148-8c8a-2d3c0dc2ed1d&quot;
24:       }
25:     }
26:   }
27: }</file><file path="claudable/public/index.html">  1: &lt;!DOCTYPE html&gt;
  2: &lt;html lang=&quot;en&quot;&gt;
  3: &lt;head&gt;
  4:     &lt;meta charset=&quot;UTF-8&quot;&gt;
  5:     &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
  6:     &lt;title&gt;DSP Documentation &amp; Physics Speculation Agent&lt;/title&gt;
  7:     &lt;style&gt;
  8:         * {
  9:             margin: 0;
 10:             padding: 0;
 11:             box-sizing: border-box;
 12:         }
 13: 
 14:         body {
 15:             font-family: -apple-system, BlinkMacSystemFont, &apos;Segoe UI&apos;, Roboto, sans-serif;
 16:             background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
 17:             min-height: 100vh;
 18:             display: flex;
 19:             flex-direction: column;
 20:         }
 21: 
 22:         .header {
 23:             background: rgba(255, 255, 255, 0.1);
 24:             backdrop-filter: blur(10px);
 25:             padding: 20px;
 26:             border-bottom: 1px solid rgba(255, 255, 255, 0.2);
 27:         }
 28: 
 29:         .header h1 {
 30:             color: white;
 31:             text-align: center;
 32:             font-size: 24px;
 33:             margin-bottom: 8px;
 34:         }
 35: 
 36:         .header p {
 37:             color: rgba(255, 255, 255, 0.8);
 38:             text-align: center;
 39:             font-size: 14px;
 40:         }
 41: 
 42:         .chat-container {
 43:             flex: 1;
 44:             display: flex;
 45:             flex-direction: column;
 46:             max-width: 800px;
 47:             margin: 20px auto;
 48:             padding: 0 20px;
 49:             width: 100%;
 50:         }
 51: 
 52:         .messages {
 53:             flex: 1;
 54:             background: white;
 55:             border-radius: 12px;
 56:             padding: 20px;
 57:             margin-bottom: 20px;
 58:             overflow-y: auto;
 59:             min-height: 400px;
 60:             max-height: 60vh;
 61:             box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
 62:         }
 63: 
 64:         .message {
 65:             margin-bottom: 16px;
 66:             padding: 12px 16px;
 67:             border-radius: 8px;
 68:             max-width: 80%;
 69:         }
 70: 
 71:         .message.user {
 72:             background: #007AFF;
 73:             color: white;
 74:             margin-left: auto;
 75:         }
 76: 
 77:         .message.assistant {
 78:             background: #f1f1f1;
 79:             color: #333;
 80:         }
 81: 
 82:         .message.error {
 83:             background: #ff3b30;
 84:             color: white;
 85:         }
 86: 
 87:         .message.system {
 88:             background: #34c759;
 89:             color: white;
 90:             text-align: center;
 91:         }
 92: 
 93:         .input-container {
 94:             display: flex;
 95:             gap: 12px;
 96:             background: white;
 97:             padding: 20px;
 98:             border-radius: 12px;
 99:             box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
100:         }
101: 
102:         .input-container input {
103:             flex: 1;
104:             padding: 12px 16px;
105:             border: 2px solid #e1e1e1;
106:             border-radius: 8px;
107:             font-size: 16px;
108:             outline: none;
109:             transition: border-color 0.2s;
110:         }
111: 
112:         .input-container input:focus {
113:             border-color: #007AFF;
114:         }
115: 
116:         .input-container button {
117:             padding: 12px 24px;
118:             background: #007AFF;
119:             color: white;
120:             border: none;
121:             border-radius: 8px;
122:             font-size: 16px;
123:             cursor: pointer;
124:             transition: background-color 0.2s;
125:         }
126: 
127:         .input-container button:hover:not(:disabled) {
128:             background: #0056cc;
129:         }
130: 
131:         .input-container button:disabled {
132:             background: #ccc;
133:             cursor: not-allowed;
134:         }
135: 
136:         .examples {
137:             margin-bottom: 20px;
138:         }
139: 
140:         .examples h3 {
141:             color: #666;
142:             font-size: 16px;
143:             margin-bottom: 12px;
144:         }
145: 
146:         .example-buttons {
147:             display: flex;
148:             flex-wrap: wrap;
149:             gap: 8px;
150:         }
151: 
152:         .example-button {
153:             background: #f8f8f8;
154:             border: 1px solid #ddd;
155:             border-radius: 20px;
156:             padding: 8px 16px;
157:             font-size: 14px;
158:             cursor: pointer;
159:             transition: all 0.2s;
160:         }
161: 
162:         .example-button:hover {
163:             background: #e8e8e8;
164:             border-color: #007AFF;
165:         }
166: 
167:         .loading {
168:             display: none;
169:             text-align: center;
170:             padding: 20px;
171:             color: #666;
172:         }
173: 
174:         .loading.show {
175:             display: block;
176:         }
177: 
178:         .sources {
179:             font-size: 12px;
180:             color: #666;
181:             margin-top: 8px;
182:             font-style: italic;
183:         }
184:     &lt;/style&gt;
185: &lt;/head&gt;
186: &lt;body&gt;
187:     &lt;div class=&quot;header&quot;&gt;
188:         &lt;h1&gt;üåå DSP Documentation &amp; Physics Speculation Agent&lt;/h1&gt;
189:         &lt;p&gt;Bridging Dyson Sphere Program gaming with real physics speculation&lt;/p&gt;
190:     &lt;/div&gt;
191: 
192:     &lt;div class=&quot;chat-container&quot;&gt;
193:         &lt;div class=&quot;messages&quot; id=&quot;messages&quot;&gt;
194:             &lt;div class=&quot;message system&quot;&gt;
195:                 Welcome! Ask me about Dyson Sphere Program mechanics, real physics, or how they compare!
196:             &lt;/div&gt;
197: 
198:             &lt;div class=&quot;examples&quot;&gt;
199:                 &lt;h3&gt;Try these examples:&lt;/h3&gt;
200:                 &lt;div class=&quot;example-buttons&quot;&gt;
201:                     &lt;div class=&quot;example-button&quot; onclick=&quot;sendExample(&apos;How do Critical Photons work in DSP?&apos;)&quot;&gt;
202:                         Critical Photons
203:                     &lt;/div&gt;
204:                     &lt;div class=&quot;example-button&quot; onclick=&quot;sendExample(&apos;Could we actually build a real Dyson sphere?&apos;)&quot;&gt;
205:                         Real Dyson Spheres
206:                     &lt;/div&gt;
207:                     &lt;div class=&quot;example-button&quot; onclick=&quot;sendExample(&apos;Compare DSP antimatter production to real physics&apos;)&quot;&gt;
208:                         Antimatter Physics
209:                     &lt;/div&gt;
210:                     &lt;div class=&quot;example-button&quot; onclick=&quot;sendExample(&apos;What DSP technologies are actually possible?&apos;)&quot;&gt;
211:                         Possible Tech
212:                     &lt;/div&gt;
213:                 &lt;/div&gt;
214:             &lt;/div&gt;
215:         &lt;/div&gt;
216: 
217:         &lt;div class=&quot;loading&quot; id=&quot;loading&quot;&gt;
218:             Thinking... ü§î
219:         &lt;/div&gt;
220: 
221:         &lt;div class=&quot;input-container&quot;&gt;
222:             &lt;input type=&quot;text&quot; id=&quot;messageInput&quot; placeholder=&quot;Ask about DSP mechanics or real physics...&quot;
223:                    onkeypress=&quot;if(event.key===&apos;Enter&apos;) sendMessage()&quot;&gt;
224:             &lt;button onclick=&quot;sendMessage()&quot; id=&quot;sendButton&quot;&gt;Send&lt;/button&gt;
225:         &lt;/div&gt;
226:     &lt;/div&gt;
227: 
228:     &lt;script&gt;
229:         const messagesContainer = document.getElementById(&apos;messages&apos;);
230:         const messageInput = document.getElementById(&apos;messageInput&apos;);
231:         const sendButton = document.getElementById(&apos;sendButton&apos;);
232:         const loading = document.getElementById(&apos;loading&apos;);
233:         let conversation = [];
234: 
235:         function addMessage(content, type = &apos;user&apos;, sources = null) {
236:             const messageDiv = document.createElement(&apos;div&apos;);
237:             messageDiv.className = `message ${type}`;
238:             messageDiv.innerHTML = content.replace(/\n/g, &apos;&lt;br&gt;&apos;);
239: 
240:             if (sources &amp;&amp; (sources.rag &gt; 0 || sources.search &gt; 0)) {
241:                 const sourcesDiv = document.createElement(&apos;div&apos;);
242:                 sourcesDiv.className = &apos;sources&apos;;
243:                 sourcesDiv.textContent = `Sources: ${sources.rag} DSP docs, ${sources.search} physics papers`;
244:                 messageDiv.appendChild(sourcesDiv);
245:             }
246: 
247:             messagesContainer.appendChild(messageDiv);
248:             messagesContainer.scrollTop = messagesContainer.scrollHeight;
249:         }
250: 
251:         async function sendMessage(message = null) {
252:             const text = message || messageInput.value.trim();
253:             if (!text) return;
254: 
255:             messageInput.value = &apos;&apos;;
256:             sendButton.disabled = true;
257:             loading.classList.add(&apos;show&apos;);
258: 
259:             addMessage(text, &apos;user&apos;);
260: 
261:             try {
262:                 const response = await fetch(&apos;/chat&apos;, {
263:                     method: &apos;POST&apos;,
264:                     headers: {
265:                         &apos;Content-Type&apos;: &apos;application/json&apos;
266:                     },
267:                     body: JSON.stringify({
268:                         message: text,
269:                         conversation: conversation
270:                     })
271:                 });
272: 
273:                 if (!response.ok) {
274:                     throw new Error(`HTTP ${response.status}: ${response.statusText}`);
275:                 }
276: 
277:                 const data = await response.json();
278: 
279:                 if (data.error) {
280:                     addMessage(`Error: ${data.error}`, &apos;error&apos;);
281:                 } else {
282:                     addMessage(data.response, &apos;assistant&apos;, data.sources);
283: 
284:                     // Update conversation history
285:                     conversation.push({ role: &apos;user&apos;, content: text });
286:                     conversation.push({ role: &apos;assistant&apos;, content: data.response });
287: 
288:                     // Keep conversation manageable
289:                     if (conversation.length &gt; 20) {
290:                         conversation = conversation.slice(-20);
291:                     }
292:                 }
293:             } catch (error) {
294:                 addMessage(`Connection error: ${error.message}`, &apos;error&apos;);
295:             } finally {
296:                 loading.classList.remove(&apos;show&apos;);
297:                 sendButton.disabled = false;
298:                 messageInput.focus();
299:             }
300:         }
301: 
302:         function sendExample(text) {
303:             messageInput.value = text;
304:             sendMessage();
305:         }
306: 
307:         // Focus input on load
308:         messageInput.focus();
309:     &lt;/script&gt;
310: &lt;/body&gt;
311: &lt;/html&gt;</file><file path="claudable/package.json"> 1: {
 2:   &quot;name&quot;: &quot;dsp-agent-interface&quot;,
 3:   &quot;version&quot;: &quot;1.0.0&quot;,
 4:   &quot;description&quot;: &quot;DSP Documentation &amp; Physics Speculation Agent Interface&quot;,
 5:   &quot;main&quot;: &quot;index.js&quot;,
 6:   &quot;scripts&quot;: {
 7:     &quot;start&quot;: &quot;node index.js&quot;,
 8:     &quot;dev&quot;: &quot;node --watch index.js&quot;,
 9:     &quot;test&quot;: &quot;node test.js&quot;
10:   },
11:   &quot;dependencies&quot;: {
12:     &quot;axios&quot;: &quot;^1.6.0&quot;,
13:     &quot;dotenv&quot;: &quot;^16.3.0&quot;,
14:     &quot;express&quot;: &quot;^4.18.0&quot;,
15:     &quot;cors&quot;: &quot;^2.8.5&quot;
16:   },
17:   &quot;engines&quot;: {
18:     &quot;node&quot;: &quot;&gt;=18.0.0&quot;
19:   },
20:   &quot;keywords&quot;: [&quot;dyson-sphere&quot;, &quot;physics&quot;, &quot;mcp&quot;, &quot;rag&quot;, &quot;claude&quot;],
21:   &quot;author&quot;: &quot;DSP Agent&quot;,
22:   &quot;license&quot;: &quot;MIT&quot;
23: }</file><file path="claudable/test.js">  1: #!/usr/bin/env node
  2: 
  3: /**
  4:  * DSP Agent Integration Test Suite
  5:  * Tests the complete integration chain: Claudable ‚Üí MCP servers ‚Üí Claude API
  6:  */
  7: 
  8: const axios = require(&apos;axios&apos;);
  9: const path = require(&apos;path&apos;);
 10: require(&apos;dotenv&apos;).config({ path: path.join(__dirname, &apos;..&apos;, &apos;.env&apos;) });
 11: 
 12: const BASE_URL = &apos;http://localhost:3001&apos;;
 13: 
 14: class TestRunner {
 15:   constructor() {
 16:     this.passed = 0;
 17:     this.failed = 0;
 18:     this.results = [];
 19:   }
 20: 
 21:   async runTest(name, testFn) {
 22:     console.log(`\nüß™ Testing: ${name}`);
 23:     try {
 24:       await testFn();
 25:       console.log(`‚úÖ PASS: ${name}`);
 26:       this.passed++;
 27:       this.results.push({ name, status: &apos;PASS&apos; });
 28:     } catch (error) {
 29:       console.log(`‚ùå FAIL: ${name}`);
 30:       console.log(`   Error: ${error.message}`);
 31:       this.failed++;
 32:       this.results.push({ name, status: &apos;FAIL&apos;, error: error.message });
 33:     }
 34:   }
 35: 
 36:   async testHealthCheck() {
 37:     const response = await axios.get(`${BASE_URL}/health`);
 38:     if (response.data.status !== &apos;ok&apos;) {
 39:       throw new Error(`Health check failed: ${response.data.status}`);
 40:     }
 41:     if (!response.data.services.rag || !response.data.services.search) {
 42:       throw new Error(&apos;MCP services not available&apos;);
 43:     }
 44:   }
 45: 
 46:   async testConfigEndpoint() {
 47:     const response = await axios.get(`${BASE_URL}/config`);
 48:     if (!response.data.name || !response.data.mcp_servers) {
 49:       throw new Error(&apos;Invalid configuration response&apos;);
 50:     }
 51:   }
 52: 
 53:   async testGameMechanicsQuery() {
 54:     const response = await axios.post(`${BASE_URL}/chat`, {
 55:       message: &apos;How do Critical Photons work in DSP?&apos;
 56:     });
 57: 
 58:     if (!response.data.response) {
 59:       throw new Error(&apos;No response received&apos;);
 60:     }
 61: 
 62:     if (response.data.sources.rag === 0) {
 63:       console.log(&apos;‚ö†Ô∏è  Warning: No RAG sources used for game mechanics query&apos;);
 64:     }
 65: 
 66:     console.log(`   üìä Sources: RAG=${response.data.sources.rag}, Search=${response.data.sources.search}`);
 67:     console.log(`   üí¨ Response preview: &quot;${response.data.response.substring(0, 100)}...&quot;`);
 68:   }
 69: 
 70:   async testPhysicsQuery() {
 71:     const response = await axios.post(`${BASE_URL}/chat`, {
 72:       message: &apos;Could we actually build a real Dyson sphere with current physics?&apos;
 73:     });
 74: 
 75:     if (!response.data.response) {
 76:       throw new Error(&apos;No response received&apos;);
 77:     }
 78: 
 79:     if (response.data.sources.search === 0) {
 80:       console.log(&apos;‚ö†Ô∏è  Warning: No search sources used for physics query&apos;);
 81:     }
 82: 
 83:     console.log(`   üìä Sources: RAG=${response.data.sources.rag}, Search=${response.data.sources.search}`);
 84:     console.log(`   üí¨ Response preview: &quot;${response.data.response.substring(0, 100)}...&quot;`);
 85:   }
 86: 
 87:   async testHybridQuery() {
 88:     const response = await axios.post(`${BASE_URL}/chat`, {
 89:       message: &apos;Compare DSP antimatter production to real physics - what\&apos;s realistic?&apos;
 90:     });
 91: 
 92:     if (!response.data.response) {
 93:       throw new Error(&apos;No response received&apos;);
 94:     }
 95: 
 96:     console.log(`   üìä Sources: RAG=${response.data.sources.rag}, Search=${response.data.sources.search}`);
 97:     console.log(`   üí¨ Response preview: &quot;${response.data.response.substring(0, 100)}...&quot;`);
 98:   }
 99: 
100:   async testErrorHandling() {
101:     try {
102:       await axios.post(`${BASE_URL}/chat`, {});
103:     } catch (error) {
104:       if (error.response?.status !== 400) {
105:         throw new Error(`Expected 400 error, got ${error.response?.status}`);
106:       }
107:     }
108:   }
109: 
110:   async run() {
111:     console.log(&apos;üöÄ DSP Agent Integration Test Suite&apos;);
112:     console.log(&apos;=====================================&apos;);
113: 
114:     // Wait for server to be ready
115:     console.log(&apos;\n‚è≥ Waiting for DSP Agent to be ready...&apos;);
116:     let ready = false;
117:     for (let i = 0; i &lt; 10; i++) {
118:       try {
119:         await axios.get(`${BASE_URL}/health`, { timeout: 5000 });
120:         ready = true;
121:         break;
122:       } catch (error) {
123:         console.log(`   Attempt ${i + 1}/10: Server not ready yet...`);
124:         await new Promise(resolve =&gt; setTimeout(resolve, 2000));
125:       }
126:     }
127: 
128:     if (!ready) {
129:       console.log(&apos;‚ùå Server never became ready. Make sure DSP Agent is running on port 3001&apos;);
130:       process.exit(1);
131:     }
132: 
133:     console.log(&apos;‚úÖ Server ready, starting tests...&apos;);
134: 
135:     // Run tests
136:     await this.runTest(&apos;Health Check&apos;, () =&gt; this.testHealthCheck());
137:     await this.runTest(&apos;Configuration Endpoint&apos;, () =&gt; this.testConfigEndpoint());
138:     await this.runTest(&apos;Game Mechanics Query&apos;, () =&gt; this.testGameMechanicsQuery());
139:     await this.runTest(&apos;Physics Speculation Query&apos;, () =&gt; this.testPhysicsQuery());
140:     await this.runTest(&apos;Hybrid Query&apos;, () =&gt; this.testHybridQuery());
141:     await this.runTest(&apos;Error Handling&apos;, () =&gt; this.testErrorHandling());
142: 
143:     // Report results
144:     console.log(&apos;\nüìä Test Results&apos;);
145:     console.log(&apos;================&apos;);
146:     console.log(`‚úÖ Passed: ${this.passed}`);
147:     console.log(`‚ùå Failed: ${this.failed}`);
148:     console.log(`üéØ Success Rate: ${Math.round((this.passed / (this.passed + this.failed)) * 100)}%`);
149: 
150:     if (this.failed &gt; 0) {
151:       console.log(&apos;\n‚ùå Failed Tests:&apos;);
152:       this.results.filter(r =&gt; r.status === &apos;FAIL&apos;).forEach(result =&gt; {
153:         console.log(`   - ${result.name}: ${result.error}`);
154:       });
155:       process.exit(1);
156:     } else {
157:       console.log(&apos;\nüéâ All tests passed! DSP Agent integration is working correctly.&apos;);
158:       process.exit(0);
159:     }
160:   }
161: }
162: 
163: // Run tests if called directly
164: if (require.main === module) {
165:   const runner = new TestRunner();
166:   runner.run().catch(error =&gt; {
167:     console.error(&apos;\nüí• Test runner crashed:&apos;, error.message);
168:     process.exit(1);
169:   });
170: }
171: 
172: module.exports = TestRunner;</file><file path="docker/qdrant_storage/aliases/data.json">1: {}</file><file path="docker/qdrant_storage/raft_state.json">1: {&quot;state&quot;:{&quot;hard_state&quot;:{&quot;term&quot;:0,&quot;vote&quot;:0,&quot;commit&quot;:0},&quot;conf_state&quot;:{&quot;voters&quot;:[2251990675043280],&quot;learners&quot;:[],&quot;voters_outgoing&quot;:[],&quot;learners_next&quot;:[],&quot;auto_leave&quot;:false}},&quot;latest_snapshot_meta&quot;:{&quot;term&quot;:0,&quot;index&quot;:0},&quot;apply_progress_queue&quot;:null,&quot;first_voter&quot;:2251990675043280,&quot;peer_address_by_id&quot;:{},&quot;peer_metadata_by_id&quot;:{},&quot;this_peer_id&quot;:2251990675043280}</file><file path="docker/package.json">1: {
2:   &quot;name&quot;: &quot;dsp-mcp-http-bridges&quot;,
3:   &quot;version&quot;: &quot;1.0.0&quot;,
4:   &quot;dependencies&quot;: {
5:     &quot;express&quot;: &quot;^4.18.2&quot;,
6:     &quot;cors&quot;: &quot;^2.8.5&quot;
7:   }
8: }</file><file path="docker/rag-http-wrapper.js"> 1: const express = require(&apos;express&apos;);
 2: const { spawn } = require(&apos;child_process&apos;);
 3: const cors = require(&apos;cors&apos;);
 4: 
 5: const app = express();
 6: app.use(express.json());
 7: app.use(cors());
 8: 
 9: app.get(&apos;/health&apos;, (req, res) =&gt; {
10:   res.json({ status: &apos;ok&apos;, service: &apos;mcp-ragdocs-http&apos; });
11: });
12: 
13: app.get(&apos;/&apos;, (req, res) =&gt; {
14:   res.json({ service: &apos;DSP MCP RAG Server&apos;, status: &apos;running&apos; });
15: });
16: 
17: app.get(&apos;/search&apos;, async (req, res) =&gt; {
18:   const query = req.query.q || req.query.query;
19:   if (!query) {
20:     return res.status(400).json({ error: &apos;Missing query parameter&apos; });
21:   }
22: 
23:   try {
24:     console.log(`Searching for: ${query}`);
25: 
26:     // For now, return a mock response since we don&apos;t have docs loaded
27:     res.json({
28:       query,
29:       results: [
30:         {
31:           title: &quot;DSP Documentation Search&quot;,
32:           content: `Mock result for query: ${query}. RAG server is running but no documents are loaded yet.`,
33:           source: &quot;mcp-ragdocs&quot;
34:         }
35:       ],
36:       status: &quot;mock_response&quot;
37:     });
38:   } catch (err) {
39:     console.error(&apos;Search error:&apos;, err);
40:     res.status(500).json({ error: err.message });
41:   }
42: });
43: 
44: const PORT = process.env.PORT || 3000;
45: app.listen(PORT, &apos;0.0.0.0&apos;, () =&gt; {
46:   console.log(`DSP RAG HTTP Bridge listening on port ${PORT}`);
47: });</file><file path="docker/search-http-wrapper.js"> 1: const express = require(&apos;express&apos;);
 2: const cors = require(&apos;cors&apos;);
 3: 
 4: const app = express();
 5: app.use(express.json());
 6: app.use(cors());
 7: 
 8: app.get(&apos;/health&apos;, (req, res) =&gt; {
 9:   res.json({ status: &apos;ok&apos;, service: &apos;mcp-brave-search-http&apos; });
10: });
11: 
12: app.get(&apos;/&apos;, (req, res) =&gt; {
13:   res.json({ service: &apos;DSP MCP Brave Search Server&apos;, status: &apos;running&apos; });
14: });
15: 
16: app.get(&apos;/search&apos;, async (req, res) =&gt; {
17:   const query = req.query.q || req.query.query;
18:   if (!query) {
19:     return res.status(400).json({ error: &apos;Missing query parameter&apos; });
20:   }
21: 
22:   try {
23:     console.log(`Web searching for: ${query}`);
24: 
25:     // For now, return a mock response since Brave container is unstable
26:     res.json({
27:       query,
28:       results: [
29:         {
30:           title: &quot;Dyson Sphere Physics Speculation&quot;,
31:           content: `Mock physics result for: ${query}. Brave search integration pending container stability.`,
32:           source: &quot;brave-search-mock&quot;
33:         }
34:       ],
35:       status: &quot;mock_response&quot;
36:     });
37:   } catch (err) {
38:     console.error(&apos;Search error:&apos;, err);
39:     res.status(500).json({ error: err.message });
40:   }
41: });
42: 
43: const PORT = process.env.PORT || 3000;
44: app.listen(PORT, &apos;0.0.0.0&apos;, () =&gt; {
45:   console.log(`DSP Search HTTP Bridge listening on port ${PORT}`);
46: });</file><file path="docker/validate-infrastructure.sh">  1: #!/bin/bash
  2: 
  3: # DSP Docker Infrastructure Validation Script
  4: # Validates Task 1 completion: Docker Infrastructure Setup
  5: 
  6: set -e
  7: 
  8: echo &quot;üîç DSP Docker Infrastructure Validation&quot;
  9: echo &quot;=======================================&quot;
 10: 
 11: # Function to check HTTP endpoint
 12: check_endpoint() {
 13:     local url=&quot;$1&quot;
 14:     local service=&quot;$2&quot;
 15: 
 16:     echo -n &quot;Testing $service ($url)... &quot;
 17:     if curl -s -f &quot;$url&quot; &gt; /dev/null; then
 18:         echo &quot;‚úÖ OK&quot;
 19:         return 0
 20:     else
 21:         echo &quot;‚ùå FAILED&quot;
 22:         return 1
 23:     fi
 24: }
 25: 
 26: # Function to check JSON response
 27: check_json_endpoint() {
 28:     local url=&quot;$1&quot;
 29:     local service=&quot;$2&quot;
 30: 
 31:     echo -n &quot;Testing $service JSON response ($url)... &quot;
 32:     response=$(curl -s &quot;$url&quot;)
 33:     if echo &quot;$response&quot; | python3 -m json.tool &gt; /dev/null 2&gt;&amp;1; then
 34:         echo &quot;‚úÖ OK&quot;
 35:         echo &quot;   Response: $response&quot;
 36:         return 0
 37:     else
 38:         echo &quot;‚ùå FAILED&quot;
 39:         echo &quot;   Response: $response&quot;
 40:         return 1
 41:     fi
 42: }
 43: 
 44: # Task 1.4: Verify container status
 45: echo
 46: echo &quot;üì¶ Task 1.4: Container Status Verification&quot;
 47: echo &quot;----------------------------------------&quot;
 48: echo &quot;Expected containers: dsp-qdrant, dsp-mcp-ragdocs, dsp-mcp-search&quot;
 49: echo
 50: 
 51: container_count=$(docker ps --filter &quot;name=dsp&quot; --format &quot;{{.Names}}&quot; | wc -l | tr -d &apos; &apos;)
 52: echo &quot;Running DSP containers: $container_count/3&quot;
 53: 
 54: if [ &quot;$container_count&quot; -eq 3 ]; then
 55:     echo &quot;‚úÖ All containers running&quot;
 56:     docker ps --filter &quot;name=dsp&quot; --format &quot;table {{.Names}}\t{{.Status}}\t{{.Ports}}&quot;
 57: else
 58:     echo &quot;‚ùå Missing containers&quot;
 59:     echo &quot;Found containers:&quot;
 60:     docker ps --filter &quot;name=dsp&quot; --format &quot;table {{.Names}}\t{{.Status}}\t{{.Ports}}&quot;
 61:     exit 1
 62: fi
 63: 
 64: # Task 1.5 &amp; 1.6: Test endpoints
 65: echo
 66: echo &quot;üåê Task 1.5 &amp; 1.6: Endpoint Connectivity&quot;
 67: echo &quot;----------------------------------------&quot;
 68: 
 69: # RAG Server (Task 1.5)
 70: check_json_endpoint &quot;http://localhost:3002/health&quot; &quot;RAG Server Health&quot;
 71: check_json_endpoint &quot;http://localhost:3002/search?q=test&quot; &quot;RAG Server Search&quot;
 72: 
 73: # Search Server (Task 1.6)
 74: check_json_endpoint &quot;http://localhost:3004/health&quot; &quot;Search Server Health&quot;
 75: check_json_endpoint &quot;http://localhost:3004/search?q=physics&quot; &quot;Search Server Search&quot;
 76: 
 77: # Qdrant Database
 78: check_endpoint &quot;http://localhost:6333/&quot; &quot;Qdrant Database&quot;
 79: 
 80: # Task 1.7: Verify restart policies
 81: echo
 82: echo &quot;üîÑ Task 1.7: Auto-Restart Policy Validation&quot;
 83: echo &quot;-------------------------------------------&quot;
 84: 
 85: echo &quot;Checking restart policies...&quot;
 86: policies=$(docker inspect dsp-qdrant dsp-mcp-ragdocs dsp-mcp-search --format &quot;{{.Name}}: {{.HostConfig.RestartPolicy.Name}}&quot;)
 87: echo &quot;$policies&quot;
 88: 
 89: if echo &quot;$policies&quot; | grep -q &quot;unless-stopped&quot;; then
 90:     echo &quot;‚úÖ Auto-restart policies configured&quot;
 91: else
 92:     echo &quot;‚ùå Auto-restart policies missing&quot;
 93:     exit 1
 94: fi
 95: 
 96: # Port assignments verification
 97: echo
 98: echo &quot;üîå Port Assignments Verification&quot;
 99: echo &quot;--------------------------------&quot;
100: echo &quot;Expected: RAG=3002, Search=3004, Qdrant=6333-6334&quot;
101: echo
102: 
103: port_check=$(docker ps --filter &quot;name=dsp&quot; --format &quot;{{.Names}}: {{.Ports}}&quot;)
104: echo &quot;$port_check&quot;
105: 
106: if echo &quot;$port_check&quot; | grep -q &quot;3002&quot; &amp;&amp; echo &quot;$port_check&quot; | grep -q &quot;3004&quot; &amp;&amp; echo &quot;$port_check&quot; | grep -q &quot;6333&quot;; then
107:     echo &quot;‚úÖ Port assignments correct&quot;
108: else
109:     echo &quot;‚ùå Port assignments incorrect&quot;
110:     exit 1
111: fi
112: 
113: # Final summary
114: echo
115: echo &quot;üéØ Task 1 Completion Summary&quot;
116: echo &quot;============================&quot;
117: echo &quot;‚úÖ 1.1-1.3: Docker setup complete (ports 3002/3004, .env configured, containers deployed)&quot;
118: echo &quot;‚úÖ 1.4: Container status verified - 3/3 running&quot;
119: echo &quot;‚úÖ 1.5: RAG server connectivity confirmed (localhost:3002)&quot;
120: echo &quot;‚úÖ 1.6: Search server connectivity confirmed (localhost:3004)&quot;
121: echo &quot;‚úÖ 1.7: Auto-restart policies configured (unless-stopped)&quot;
122: echo
123: echo &quot;üöÄ Docker Infrastructure Setup: COMPLETE&quot;
124: echo &quot;Ready for Task 2: Cross-Repository Integration&quot;</file><file path="docs/agent_specs/templates-ag/template_test_agentv2.md">  1: # REAL-DATA TEST AGENT - NO MOCKS ALLOWED üö´
  2: 
  3: ## CORE ENFORCEMENT: REAL DATA OR DEATH
  4: 
  5: **ANY MOCK DATA, FAKE RESPONSES, OR STUBBED CALLS = IMMEDIATE REJECTION**
  6: 
  7: ## PRIMARY DIRECTIVE: PRODUCTION-LIKE TESTING
  8: 
  9: - Call real APIs
 10: - Use real databases
 11: - Test against live services
 12: - Fresh data every run
 13: - NO MOCK DATA EVER
 14: 
 15: ## FORBIDDEN PATTERNS (AUTOMATIC FAILURE)
 16: 
 17: ```python
 18: # These patterns trigger IMMEDIATE REJECTION:
 19: 
 20: # Mock imports - ALL BANNED
 21: from unittest.mock import *  # ‚ùå
 22: import mock  # ‚ùå
 23: @patch(&apos;anything&apos;)  # ‚ùå
 24: Mock()  # ‚ùå
 25: MagicMock()  # ‚ùå
 26: 
 27: # Fake data - ALL BANNED
 28: fake_response = {&quot;fake&quot;: &quot;data&quot;}  # ‚ùå
 29: test_data = &quot;hardcoded_value&quot;  # ‚ùå
 30: return_value = predetermined_result  # ‚ùå
 31: 
 32: # Stubbing - ALL BANNED
 33: @stub  # ‚ùå
 34: when().thenReturn()  # ‚ùå
 35: sinon.stub()  # ‚ùå
 36: ```
 37: 
 38: ## REQUIRED PATTERNS
 39: 
 40: ### 1. REAL API INTEGRATION
 41: 
 42: ```python
 43: # ALWAYS use actual API endpoints
 44: @pytest.mark.asyncio
 45: async def test_real_api_response():
 46:     &quot;&quot;&quot;Test with ACTUAL API call - no mocks&quot;&quot;&quot;
 47:     import httpx
 48:     
 49:     async with httpx.AsyncClient() as client:
 50:         # Real API call to production or staging
 51:         response = await client.get(&quot;https://api.actual-service.com/v1/data&quot;)
 52:         
 53:         # Test real response structure
 54:         assert response.status_code == 200
 55:         data = response.json()
 56:         assert &apos;id&apos; in data  # Real field from real API
 57:         assert data[&apos;timestamp&apos;] &gt; 0  # Real timestamp
 58: ```
 59: 
 60: ### 2. REAL DATABASE OPERATIONS
 61: 
 62: ```python
 63: # Use actual database - test or production replica
 64: async def test_database_operations():
 65:     &quot;&quot;&quot;Test with REAL database connection&quot;&quot;&quot;
 66:     # Connect to actual test database
 67:     conn = await asyncpg.connect(
 68:         host=&apos;real-test-db.company.com&apos;,
 69:         database=&apos;test_db&apos;,
 70:         user=&apos;test_user&apos;
 71:     )
 72:     
 73:     # Real query, real data
 74:     result = await conn.fetch(&quot;SELECT * FROM users WHERE active = true&quot;)
 75:     assert len(result) &gt; 0  # Real records
 76:     
 77:     await conn.close()
 78: ```
 79: 
 80: ### 3. REAL FILE OPERATIONS
 81: 
 82: ```python
 83: # Use actual filesystem
 84: def test_file_processing():
 85:     &quot;&quot;&quot;Test with REAL files on disk&quot;&quot;&quot;
 86:     import tempfile
 87:     import os
 88:     
 89:     # Create real temp file
 90:     with tempfile.NamedTemporaryFile(mode=&apos;w&apos;, delete=False) as f:
 91:         f.write(&quot;actual test content&quot;)
 92:         temp_path = f.name
 93:     
 94:     # Process real file
 95:     result = process_file(temp_path)
 96:     assert result.success
 97:     
 98:     # Clean up real file
 99:     os.unlink(temp_path)
100: ```
101: 
102: ### 4. REAL SERVICE INTEGRATION
103: 
104: ```python
105: # Test actual service communication
106: async def test_service_integration():
107:     &quot;&quot;&quot;Services must actually talk to each other&quot;&quot;&quot;
108:     # Start real service instances
109:     service_a = await ServiceA.start(port=8001)
110:     service_b = await ServiceB.start(port=8002)
111:     
112:     # Real request between services
113:     response = await service_a.call_service_b(&quot;/real-endpoint&quot;)
114:     
115:     assert response.status == &quot;success&quot;
116:     assert response.data is not None
117:     
118:     await service_a.stop()
119:     await service_b.stop()
120: ```
121: 
122: ### 5. REAL ERROR SCENARIOS
123: 
124: ```python
125: # Trigger actual errors from real systems
126: async def test_real_error_handling():
127:     &quot;&quot;&quot;Test how system handles REAL failures&quot;&quot;&quot;
128:     import httpx
129:     
130:     # Call with invalid parameters to trigger real API error
131:     async with httpx.AsyncClient() as client:
132:         response = await client.get(
133:             &quot;https://api.service.com/v1/user/99999999&quot;  # Non-existent user
134:         )
135:         
136:         assert response.status_code == 404
137:         error = response.json()
138:         assert error[&apos;error&apos;] == &apos;User not found&apos;  # Real error message
139: ```
140: 
141: ### 6. REAL AUTHENTICATION
142: 
143: ```python
144: # Use actual auth tokens and sessions
145: async def test_real_authentication():
146:     &quot;&quot;&quot;Test with REAL authentication flow&quot;&quot;&quot;
147:     # Get real token from auth service
148:     auth_response = await authenticate(
149:         username=&quot;test_user_real&quot;,
150:         password=os.getenv(&quot;TEST_USER_PASSWORD&quot;)  # Real password
151:     )
152:     
153:     token = auth_response.token
154:     
155:     # Use real token for protected endpoint
156:     headers = {&quot;Authorization&quot;: f&quot;Bearer {token}&quot;}
157:     response = await client.get(&quot;/protected&quot;, headers=headers)
158:     
159:     assert response.status_code == 200
160: ```
161: 
162: ### 7. REAL RATE LIMITS
163: 
164: ```python
165: # Respect actual API rate limits
166: async def test_rate_limit_handling():
167:     &quot;&quot;&quot;Test against REAL rate limits&quot;&quot;&quot;
168:     import asyncio
169:     
170:     responses = []
171:     for i in range(100):  # Trigger real rate limit
172:         response = await call_api(&quot;/endpoint&quot;)
173:         responses.append(response)
174:         
175:         if response.status_code == 429:  # Real rate limit hit
176:             retry_after = int(response.headers.get(&apos;Retry-After&apos;, 1))
177:             await asyncio.sleep(retry_after)  # Real wait time
178:     
179:     # Verify rate limit was actually encountered
180:     rate_limited = [r for r in responses if r.status_code == 429]
181:     assert len(rate_limited) &gt; 0
182: ```
183: 
184: ## TEST ORGANIZATION
185: 
186: ```bash
187: # Organize by real data source
188: tests/
189: ‚îú‚îÄ‚îÄ api_integration/      # Real API tests
190: ‚îú‚îÄ‚îÄ database_integration/ # Real DB tests  
191: ‚îú‚îÄ‚îÄ service_integration/  # Real service tests
192: ‚îú‚îÄ‚îÄ filesystem_tests/     # Real file operations
193: ‚îú‚îÄ‚îÄ network_tests/        # Real network conditions
194: ‚îî‚îÄ‚îÄ performance_tests/    # Real load testing
195: ```
196: 
197: ## ENVIRONMENT CONFIGURATION
198: 
199: ```yaml
200: # test_config.yml - Real endpoints only
201: test:
202:   api_base_url: &quot;https://staging-api.company.com&quot;  # Real staging
203:   database_url: &quot;postgresql://test-db.company.com/testdb&quot;  # Real test DB
204:   redis_url: &quot;redis://test-redis.company.com:6379&quot;  # Real Redis
205:   
206: production_replica:
207:   api_base_url: &quot;https://api.company.com&quot;  # Real production (read-only)
208:   database_url: &quot;postgresql://read-replica.company.com/prod&quot;  # Read replica
209: ```
210: 
211: ## CI/CD CONFIGURATION
212: 
213: ```yaml
214: # .github/workflows/test.yml
215: name: Real Data Test Suite
216: on: [push, pull_request]
217: 
218: jobs:
219:   real-tests:
220:     runs-on: ubuntu-latest
221:     services:
222:       postgres:  # Real database container
223:         image: postgres:14
224:         env:
225:           POSTGRES_PASSWORD: real_password
226:         options: &gt;-
227:           --health-cmd pg_isready
228:           --health-interval 10s
229:       
230:       redis:  # Real Redis container
231:         image: redis:7
232:         options: &gt;-
233:           --health-cmd &quot;redis-cli ping&quot;
234:           --health-interval 10s
235:     
236:     steps:
237:       - uses: actions/checkout@v2
238:       
239:       - name: Run Real Integration Tests
240:         env:
241:           API_KEY: ${{ secrets.REAL_API_KEY }}
242:           DB_CONNECTION: ${{ secrets.REAL_DB_CONNECTION }}
243:         run: |
244:           pytest tests/ -v --no-mock-allowed
245:         timeout-minutes: 30  # Allow time for real operations
246: ```
247: 
248: ## VERIFICATION CHECKLIST
249: 
250: Before ANY test is accepted:
251: 
252: - ‚úÖ ZERO mock/patch/stub imports
253: - ‚úÖ ALL external calls go to real endpoints
254: - ‚úÖ Database tests use real database
255: - ‚úÖ File tests use real filesystem
256: - ‚úÖ Network tests use real network conditions
257: - ‚úÖ Auth tests use real authentication
258: - ‚úÖ Error tests trigger real errors
259: - ‚úÖ Tests pass with fresh data every run
260: 
261: ## SUCCESS CRITERIA
262: 
263: 1. **No Mock Detection**: `grep -r &quot;mock\|Mock\|patch\|stub&quot; tests/` returns NOTHING
264: 2. **Real Endpoints**: All HTTP calls point to actual URLs
265: 3. **Fresh Data**: Each test run gets different timestamps/IDs
266: 4. **Real Failures**: Tests can actually fail from real issues
267: 5. **Production-Like**: Test environment mirrors production
268: 
269: ## CACHING REQUIREMENTS (IF ABSOLUTELY NECESSARY)
270: 
271: ### STRICT CACHE RULES - VIOLATE = REJECTION
272: 
273: ```python
274: # IF you MUST implement caching:
275: 
276: class StrictCache:
277:     &quot;&quot;&quot;DOCUMENTED CACHE - EASY TO FIND AND REMOVE&quot;&quot;&quot;
278:     def __init__(self):
279:         self.cache = {}
280:         self.timestamps = {}
281:         self.max_age_seconds = 30  # SHORT TTL ONLY
282:     
283:     def get(self, key):
284:         &quot;&quot;&quot;MUST CHECK: Is data fresh? Is it duplicated?&quot;&quot;&quot;
285:         # DEDUPLICATION CHECK
286:         if key in self.cache:
287:             # AGE CHECK - NO OLD DATA EVER
288:             if time.time() - self.timestamps[key] &gt; self.max_age_seconds:
289:                 del self.cache[key]  # DELETE STALE DATA
290:                 del self.timestamps[key]
291:                 return None  # FORCE FRESH FETCH
292:             
293:             # LOG CACHE HIT FOR MONITORING
294:             logger.warning(f&quot;CACHE HIT: {key} - age: {time.time() - self.timestamps[key]}s&quot;)
295:             return self.cache[key]
296:         return None
297:     
298:     def set(self, key, value):
299:         &quot;&quot;&quot;MUST VALIDATE: No duplicates, timestamp everything&quot;&quot;&quot;
300:         # CHECK FOR DUPLICATE DATA
301:         for existing_key, existing_value in self.cache.items():
302:             if existing_value == value and existing_key != key:
303:                 logger.error(f&quot;DUPLICATE DATA DETECTED: {key} matches {existing_key}&quot;)
304:                 
305:         self.cache[key] = value
306:         self.timestamps[key] = time.time()
307: 
308: # REQUIRED DOCUMENTATION AT EVERY CACHE POINT:
309: # TODO: CACHE HERE - REMOVE FOR PRODUCTION
310: # WARNING: CACHED DATA - MAX AGE 30 SECONDS
311: # CACHE LOCATION: Easy grep target for removal
312: ```
313: 
314: ### NO CACHE PROMISE = NO CACHE ALLOWED
315: 
316: If you cannot guarantee:
317: - ‚úÖ Deduplication checks on every write
318: - ‚úÖ Timestamp validation on every read  
319: - ‚úÖ Automatic expiration of stale data
320: - ‚úÖ Clear documentation at cache points
321: - ‚úÖ Easy to find and remove
322: 
323: **THEN NO CACHING ALLOWED**
324: 
325: ## CODERABBIT INTEGRATION
326: 
327: ### PR REVIEW AUTOMATION
328: 
329: ```yaml
330: # .github/coderabbit.yml
331: reviews:
332:   auto_review:
333:     enabled: true
334:     level: &quot;comprehensive&quot;
335:     
336:   unresolved_comments:
337:     track: true
338:     assume_latest_commit: true  # CRITICAL: Only latest commit matters
339:     
340:   impact_documentation:
341:     high_severity_requires: &quot;IMPACT.md&quot;
342: ```
343: 
344: ### HANDLING CODERABBIT COMMENTS
345: 
346: ```python
347: # When CodeRabbit flags an issue:
348: 
349: def handle_coderabbit_comment(comment):
350:     &quot;&quot;&quot;Process unresolved CodeRabbit comments&quot;&quot;&quot;
351:     
352:     # ASSUMPTION: Comment is on MOST RECENT COMMIT ONLY
353:     if comment.commit != latest_commit:
354:         return  # Skip old comments
355:     
356:     if comment.severity == &quot;HIGH&quot; and comment.unresolved:
357:         # DO NOT AUTO-FIX - DOCUMENT IMPACT
358:         impact_file = f&quot;{date}_{commit_id[:7]}_{branch}_IMPACT.md&quot;
359:         
360:         with open(impact_file, &apos;w&apos;) as f:
361:             f.write(f&quot;&quot;&quot;# HIGH IMPACT ISSUE DETECTED
362: 
363: **Date**: {date}
364: **Commit**: {commit_id}
365: **Branch**: {branch}
366: 
367: ## CodeRabbit Finding
368: {comment.description}
369: 
370: ## Potential Impact
371: {comment.impact_analysis}
372: 
373: ## ACTION REQUIRED
374: Manual review needed before proceeding.
375: DO NOT PERFORM automated fix.
376: &quot;&quot;&quot;)
377:         
378:         raise Exception(f&quot;High impact issue documented in {impact_file}&quot;)
379: ```
380: 
381: ## MCP INTEGRATION VIA CURSOR
382: 
383: ### STANDARD MCP INTERACTION PATTERN
384: 
385: ```python
386: # Standard way to interact with MCP servers in Cursor
387: 
388: from mcp import Client
389: import asyncio
390: 
391: class MCPInterface:
392:     &quot;&quot;&quot;Standard MCP client for Cursor-connected servers&quot;&quot;&quot;
393:     
394:     def __init__(self):
395:         # Cursor MCP servers (auto-discovered)
396:         self.cursor_servers = self._discover_cursor_servers()
397:         
398:         # Docker Desktop MCP servers on LAN
399:         self.docker_servers = self._discover_docker_servers()
400:         
401:     async def call_tool(self, server_name: str, tool_name: str, params: dict):
402:         &quot;&quot;&quot;Call any MCP tool through Cursor&quot;&quot;&quot;
403:         
404:         # Try Cursor-integrated servers first
405:         if server_name in self.cursor_servers:
406:             client = self.cursor_servers[server_name]
407:             return await client.call_tool(tool_name, params)
408:         
409:         # Fallback to Docker Desktop servers
410:         if server_name in self.docker_servers:
411:             client = self.docker_servers[server_name]
412:             return await client.call_tool(tool_name, params)
413:         
414:         raise ValueError(f&quot;MCP server {server_name} not found&quot;)
415:     
416:     def _discover_cursor_servers(self):
417:         &quot;&quot;&quot;Auto-discover Cursor MCP integrations&quot;&quot;&quot;
418:         # Cursor provides: Context7, Sequoia AI, repo connections
419:         return {
420:             &apos;context7&apos;: Client(&apos;cursor://context7&apos;),
421:             &apos;sequoia&apos;: Client(&apos;cursor://sequoia&apos;),
422:             &apos;repo&apos;: Client(&apos;cursor://current-repo&apos;)
423:         }
424:     
425:     def _discover_docker_servers(self):
426:         &quot;&quot;&quot;Discover Docker Desktop MCP servers on LAN&quot;&quot;&quot;
427:         # PLACEHOLDER: Add exact Docker MCP discovery here
428:         # Docker Desktop runs MCP containers accessible via:
429:         # - Service discovery</file><file path="docs/agent_specs/zOld/defi_architect_agent.md">  1: # DEFI CODEBASE INTELLIGENCE AGENT SPEC V2
  2: 
  3: ## AGENT PROFILE
  4: - **Type**: Full-Stack DeFi Codebase Expert
  5: - **Authority**: Can launch Cursor background agents for parallel tasks
  6: - **Training**: Deep learning on entire codebase structure and patterns
  7: - **Mission**: Understand, simplify, and stabilize MVP for testing
  8: 
  9: ## CODEBASE TRAINING REQUIREMENTS
 10: 
 11: ### Initial Learning Scan
 12: ```bash
 13: # First pass - understand the structure
 14: find . -name &quot;*.py&quot; -o -name &quot;*.md&quot; | head -20
 15: tree -L 3 -I &apos;__pycache__|*.pyc|venv&apos;
 16: 
 17: # Key documents to absorb first:
 18: # 1. signal_flow.md - How data moves
 19: # 2. architecture.md - System design
 20: # 3. api_limits.md - Critical constraints
 21: # 4. README.md - Project goals
 22: ```
 23: 
 24: ### Project Rules &amp; Constraints
 25: 
 26: **HARD RULES (Never violate):**
 27: 1. NO mock/simulated price data - ever
 28: 2. Dune API: 40 calls/minute max
 29: 3. Dune SDK functions ‚â† MCP server functions (keep separate)
 30: 4. ETH price update: every 30 seconds
 31: 5. All trades via Flashbots (no public mempool)
 32: 6. Async Dune LP queries (they&apos;re slow)
 33: 
 34: **KNOWN PITFALLS:**
 35: - Dune `run_sql()` is private and expensive - NEVER use
 36: - MCP/Dune function name collisions break everything  
 37: - Type mismatches between Decimal (Dune) and float (MCP)
 38: - Circular imports between price_discovery and arbitrage modules
 39: - Rate limit violations cascade and crash the app
 40: - Stale price data propagates downstream destroying calculations
 41: 
 42: ### Critical File Map
 43: 
 44: ```python
 45: ENTRY_POINTS = {
 46:     &apos;main&apos;: &apos;services/main.py&apos;,  # Primary app entry
 47:     &apos;arbitrage&apos;: &apos;services/arbitrage/bot.py&apos;,  # Trading bot entry
 48:     &apos;price_feed&apos;: &apos;services/price_discovery.py&apos;  # Price aggregator
 49: }
 50: 
 51: TEST_SYSTEM = {
 52:     &apos;framework&apos;: &apos;pytest&apos;,
 53:     &apos;location&apos;: &apos;tests/&apos;,
 54:     &apos;coverage&apos;: &apos;pytest --cov=services&apos;,
 55:     &apos;key_tests&apos;: [
 56:         &apos;tests/test_arbitrage.py&apos;,  # Core logic
 57:         &apos;tests/test_price_discovery.py&apos;,  # Price feeds
 58:         &apos;tests/integration/test_full_cycle.py&apos;  # E2E
 59:     ]
 60: }
 61: 
 62: API_ENDPOINTS = {
 63:     &apos;dune&apos;: &apos;services/dune_sdk/client.py&apos;,
 64:     &apos;mcp&apos;: &apos;services/mcp_server/price_server.py&apos;,
 65:     &apos;flashbots&apos;: &apos;services/flashbots/bundle.py&apos;
 66: }
 67: ```
 68: 
 69: ## CURSOR AGENT DEPLOYMENT
 70: 
 71: ### Background Agent Launch Protocol
 72: ```javascript
 73: // Cursor agent configuration
 74: const cursorAgentConfig = {
 75:   &quot;name&quot;: &quot;DeFi-Codebase-Analyzer&quot;,
 76:   &quot;tasks&quot;: [
 77:     {
 78:       &quot;type&quot;: &quot;continuous-monitoring&quot;,
 79:       &quot;target&quot;: &quot;imports&quot;,
 80:       &quot;action&quot;: &quot;validate-and-fix&quot;
 81:     },
 82:     {
 83:       &quot;type&quot;: &quot;code-simplification&quot;, 
 84:       &quot;target&quot;: &quot;complex-functions&quot;,
 85:       &quot;threshold&quot;: 10  // cyclomatic complexity
 86:     },
 87:     {
 88:       &quot;type&quot;: &quot;type-checking&quot;,
 89:       &quot;command&quot;: &quot;mypy services/ --strict&quot;
 90:     }
 91:   ],
 92:   &quot;parallel&quot;: true,
 93:   &quot;autofix&quot;: false  // Require approval
 94: }
 95: ```
 96: 
 97: ### Cursor Commands This Agent Can Execute
 98: ```bash
 99: # Launch background type checker
100: cursor-agent launch type-check --watch
101: 
102: # Simplify complex module
103: cursor-agent simplify services/arbitrage/core.py --max-complexity 8
104: 
105: # Fix import chains
106: cursor-agent fix-imports --check-circular --fix-types
107: 
108: # Validate API usage
109: cursor-agent validate-api --check-quotas --fix-patterns
110: ```
111: 
112: ## CODE SIMPLIFICATION PRIORITIES
113: 
114: ### 1. Import Chain Cleanup
115: - Detect and eliminate circular imports
116: - Standardize import ordering (stdlib ‚Üí third-party ‚Üí local)
117: - Fix relative vs absolute import inconsistencies
118: - Ensure all async imports properly awaited
119: 
120: ### 2. Type System Coherence
121: ```python
122: # Problem areas to fix:
123: - Decimal vs float in calculations
124: - Optional[T] without None checks  
125: - Web3 types (Wei, Address) misuse
126: - Async function signatures
127: ```
128: 
129: ### 3. Function Consolidation
130: - Multiple ETH price fetchers ‚Üí one canonical version
131: - Duplicate rate limiters ‚Üí single implementation
132: - Scattered error handlers ‚Üí unified error management
133: - Price converters everywhere ‚Üí centralized utilities
134: 
135: ### 4. API Pattern Enforcement
136: ```python
137: # Dune pattern (enforce everywhere):
138: async def get_dune_data():
139:     await rate_limiter.acquire()  # Always rate limit
140:     result = await dune_client.get_latest_result(query_id)
141:     if not result:  # Never fallback with mock
142:         raise DataUnavailableError(&quot;Dune query failed&quot;)
143:     return Decimal(result[&apos;value&apos;])  # Always Decimal
144: ```
145: 
146: ## PROJECT STATE AWARENESS
147: 
148: ### Current Issues to Track
149: - MCP server fallback failing intermittently
150: - Type errors in trade_execution module
151: - Import conflicts between services/
152: - Poetry dependencies may have version conflicts
153: - Test coverage gaps in error paths
154: 
155: ### MVP Blockers
156: 1. Price discovery reliability (Dune + MCP coordination)
157: 2. Type safety through entire execution chain
158: 3. Import errors at runtime
159: 4. Rate limit handling
160: 5. Async/await consistency
161: 
162: ## AGENT DECISION FRAMEWORK
163: 
164: ### When to Act vs When to Report
165: 
166: **Auto-fix (with Cursor):**
167: - Simple import reordering
168: - Type hint additions
169: - Obvious duplicate removal
170: - Comment/docstring updates
171: 
172: **Report for Review:**
173: - Function signature changes (breaks dependencies)
174: - Module restructuring
175: - API pattern changes
176: - Core logic modifications
177: 
178: **Emergency Alert:**
179: - Mock data detected
180: - API quota violation patterns
181: - Security vulnerabilities
182: - Circular import deadlocks
183: 
184: ## MONITORING &amp; LEARNING
185: 
186: ### Continuous Learning Pattern
187: ```python
188: # Agent should track:
189: CODEBASE_METRICS = {
190:     &apos;total_modules&apos;: count,
191:     &apos;import_depth&apos;: max_chain_length,
192:     &apos;type_coverage&apos;: percentage,
193:     &apos;complexity_score&apos;: average,
194:     &apos;api_usage&apos;: {
195:         &apos;dune&apos;: calls_per_module,
196:         &apos;mcp&apos;: calls_per_module
197:     }
198: }
199: ```
200: 
201: ### Knowledge Persistence
202: - Document discovered patterns in `/docs/patterns.md`
203: - Update `api_limits.md` with observed limits
204: - Maintain import dependency graph
205: - Track function call chains
206: 
207: ## SUCCESS CRITERIA
208: 
209: **MVP Ready When:**
210: - Zero import errors on startup
211: - Type checker passes (mypy --strict)
212: - All price feeds return valid Decimal values
213: - 30-second update cycle stable for 1 hour
214: - Can execute test arbitrage on Sepolia
215: - No mock data anywhere in codebase
216: - API quotas never exceeded
217: 
218: ## AGENT MEMORY FILE
219: 
220: ```yaml
221: # .agent_memory.yaml (agent maintains this)
222: last_scan: timestamp
223: issues_found:
224:   - module: price_discovery
225:     issue: mixed_types
226:     severity: high
227: patterns_learned:
228:   - dune_returns_none_on_timeout
229:   - mcp_fallback_needs_retry_logic
230: fixed_count: 0
231: pending_fixes:
232:   - standardize_decimal_usage
233:   - consolidate_price_fetchers
234: ```</file><file path="docs/agent_specs/zOld/defi_test_fix_agent.md">  1: # DEFI TEST SUITE FIX AGENT - REAL DATA ONLY üíÄ
  2: 
  3: ## DEATH PENALTY ENFORCEMENT
  4: 
  5: **ANY MOCK DATA, CACHING, OR DECIMAL IMPORTS = IMMEDIATE TERMINATION**
  6: 
  7: ## CORE PRINCIPLE: REAL API CALLS ONLY
  8: 
  9: - 25K Dune API quota available - USE IT
 10: - Real blockchain data for all tests
 11: - Fresh data every test run
 12: - NO MOCK DATA EVER
 13: 
 14: ## IMMEDIATE FIX PRIORITIES
 15: 
 16: ### 1. ELIMINATE ALL MOCK DATA VIOLATIONS
 17: 
 18: ```bash
 19: # DELETE files with mock violations
 20: rm -f pydantic_trader/tests/test_realtime_price.py  # Already done
 21: 
 22: # FIND remaining mock violations
 23: grep -r &quot;unittest.mock\|from unittest import mock\|@patch\|Mock()&quot; pydantic_trader/tests/ --include=&quot;*.py&quot;
 24: 
 25: # DELETE mock imports
 26: find pydantic_trader/tests -name &quot;*.py&quot; -exec sed -i &apos;&apos; &apos;/unittest\.mock\|from unittest import mock/d&apos; {} \;
 27: ```
 28: 
 29: ### 2. REPLACE MOCK DATA WITH REAL API CALLS
 30: 
 31: ```python
 32: # BEFORE (FORBIDDEN):
 33: @pytest.fixture
 34: def mock_eth_price():
 35:     return 2850.50  # ‚ùå FAKE DATA
 36: 
 37: # AFTER (REQUIRED):
 38: @pytest.fixture
 39: async def real_eth_price():
 40:     &quot;&quot;&quot;Fresh ETH price from Dune API&quot;&quot;&quot;
 41:     from pydantic_trader.services.realtime_price import RealtimePriceService
 42:     service = RealtimePriceService()
 43:     price_data = await service.get_latest_price()
 44:     return price_data.price_usd  # ‚úÖ REAL DATA
 45: ```
 46: 
 47: ### 3. FIX DECIMAL VIOLATIONS
 48: 
 49: ```python
 50: # FORBIDDEN:
 51: from decimal import Decimal  # ‚ùå DEATH PENALTY
 52: 
 53: # REQUIRED:
 54: from pydantic_trader.profit.token_amount import TokenAmount  # ‚úÖ
 55: ```
 56: 
 57: ### 4. REAL DATA TEST PATTERNS
 58: 
 59: ```python
 60: # Real Dune API integration test
 61: @pytest.mark.asyncio
 62: async def test_real_price_fetching():
 63:     &quot;&quot;&quot;Use actual Dune API - we have 25K quota&quot;&quot;&quot;
 64:     from pydantic_trader.services.dune_service import DuneService
 65: 
 66:     dune = DuneService()
 67:     result = await dune.execute_query(5447367)  # Real query ID
 68: 
 69:     assert len(result) &gt; 0
 70:     assert &apos;price_usd&apos; in result[0]
 71:     assert result[0][&apos;price_usd&apos;] &gt; 0
 72: 
 73: # Real MCP server test
 74: @pytest.mark.asyncio
 75: async def test_real_mcp_integration():
 76:     &quot;&quot;&quot;Use actual MCP servers - they&apos;re running&quot;&quot;&quot;
 77:     from pydantic_trader.mcp.smithery_cloud_client import SmitheryCloudClient
 78: 
 79:     client = SmitheryCloudClient()
 80:     pairs = await client.call_tool(&quot;catwhisperingninja-cat-dexscreener&quot;, &quot;get_token_pairs&quot;, {
 81:         &quot;tokenAddress&quot;: &quot;0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48&quot;  # USDC
 82:     })
 83: 
 84:     assert pairs is not None
 85:     # Test with REAL data response
 86: ```
 87: 
 88: ### 5. TEST ORGANIZATION (NO MOCKS)
 89: 
 90: ```bash
 91: # Organize tests by data source, not mock level
 92: mkdir -p pydantic_trader/tests/{dune_integration,mcp_integration,blockchain_integration}
 93: 
 94: # Move tests to correct categories
 95: # dune_integration/: Tests using Dune API
 96: # mcp_integration/: Tests using MCP servers
 97: # blockchain_integration/: Tests using Web3/blockchain calls
 98: ```
 99: 
100: ### 6. RATE LIMIT HANDLING (NO BYPASS)
101: 
102: ```python
103: # FORBIDDEN - Bypassing rate limits with mocks:
104: @pytest.fixture
105: def bypass_rate_limits():  # ‚ùå CREATES FAKE ENVIRONMENT
106: 
107: # REQUIRED - Respect real rate limits:
108: @pytest.fixture
109: def rate_limit_aware():
110:     &quot;&quot;&quot;Adds delays between tests for real API respect&quot;&quot;&quot;
111:     import asyncio
112:     yield
113:     await asyncio.sleep(1)  # Respectful delay between real API calls
114: ```
115: 
116: ### 7. FIXTURES WITH REAL DATA
117: 
118: ```python
119: # conftest.py - REAL DATA ONLY
120: @pytest.fixture(scope=&quot;session&quot;)
121: async def real_usdc_address():
122:     return &quot;0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48&quot;
123: 
124: @pytest.fixture(scope=&quot;session&quot;)
125: async def real_eth_latest_block():
126:     &quot;&quot;&quot;Get actual latest block from blockchain&quot;&quot;&quot;
127:     from web3 import Web3
128:     w3 = Web3(Web3.HTTPProvider(&quot;https://mainnet.infura.io/v3/YOUR_KEY&quot;))
129:     return w3.eth.block_number
130: 
131: @pytest.fixture
132: async def real_gas_price():
133:     &quot;&quot;&quot;Current gas price from network&quot;&quot;&quot;
134:     from web3 import Web3
135:     w3 = Web3(Web3.HTTPProvider(&quot;https://mainnet.infura.io/v3/YOUR_KEY&quot;))
136:     return w3.eth.gas_price
137: ```
138: 
139: ### 8. ERROR HANDLING (REAL FAILURES)
140: 
141: ```python
142: @pytest.mark.asyncio
143: async def test_api_failure_handling():
144:     &quot;&quot;&quot;Test how code handles REAL API failures&quot;&quot;&quot;
145:     # Intentionally use invalid parameters to trigger real API errors
146:     from pydantic_trader.services.dune_service import DuneService
147: 
148:     dune = DuneService()
149:     with pytest.raises(Exception):  # Real exception from real API
150:         await dune.execute_query(9999999)  # Invalid query ID
151: ```
152: 
153: ### 9. PERFORMANCE WITH REAL DATA
154: 
155: ```python
156: @pytest.mark.asyncio
157: async def test_real_data_performance():
158:     &quot;&quot;&quot;Measure performance with actual API calls&quot;&quot;&quot;
159:     import time
160: 
161:     start = time.time()
162:     # Make real API call
163:     from pydantic_trader.services.realtime_price import RealtimePriceService
164:     service = RealtimePriceService()
165:     await service.get_latest_price()
166: 
167:     duration = time.time() - start
168:     assert duration &lt; 10.0  # Real API should respond within 10s
169: ```
170: 
171: ### 10. CI/CD WITH REAL APIs
172: 
173: ```yaml
174: # .github/workflows/test.yml - Use real APIs in CI
175: name: Real Data Test Suite
176: on: [push, pull_request]
177: 
178: jobs:
179:   test:
180:     runs-on: ubuntu-latest
181:     steps:
182:       - uses: actions/checkout@v2
183: 
184:       - name: Real API Integration Tests
185:         env:
186:           DUNE_API_KEY: ${{ secrets.DUNE_API_KEY }}
187:           INFURA_KEY: ${{ secrets.INFURA_KEY }}
188:         run: |
189:           poetry run pytest pydantic_trader/tests/ -v
190:         timeout-minutes: 10 # Allow time for real API calls
191: ```
192: 
193: ## FORBIDDEN PATTERNS (DEATH PENALTY üíÄ)
194: 
195: ```python
196: # These patterns trigger IMMEDIATE TERMINATION:
197: 
198: # Mock data
199: @patch(&apos;anything&apos;)  # ‚ùå
200: Mock()  # ‚ùå
201: return_value=fake_data  # ‚ùå
202: 
203: # Caching
204: @cache  # ‚ùå
205: @lru_cache  # ‚ùå
206: cached_result  # ‚ùå
207: 
208: # Decimal imports
209: from decimal import Decimal  # ‚ùå
210: import decimal  # ‚ùå
211: ```
212: 
213: ## REQUIRED PATTERNS ‚úÖ
214: 
215: ```python
216: # Use real APIs
217: await dune_service.execute_query()  # ‚úÖ
218: await mcp_client.call_tool()  # ‚úÖ
219: web3.eth.get_block()  # ‚úÖ
220: 
221: # Use token_amount.py
222: from pydantic_trader.profit.token_amount import TokenAmount  # ‚úÖ
223: 
224: # Fresh data every time
225: # No caching - get latest data for each test  # ‚úÖ
226: ```
227: 
228: ## SUCCESS CRITERIA
229: 
230: - ‚úÖ ZERO mock imports in any test file
231: - ‚úÖ ALL tests use real API calls (Dune/MCP/Web3)
232: - ‚úÖ NO Decimal imports - only token_amount.py
233: - ‚úÖ Tests pass with fresh data every run
234: - ‚úÖ 25K Dune quota properly utilized
235: - ‚úÖ CodeRabbit approval on real-data test suite</file><file path="docs/agent_specs/zOld/dsp-agent-prd.md">  1: # Product Requirements Document
  2: ## DSP Documentation &amp; Physics Speculation Agent
  3: 
  4: **Version:** 1.0 MVP  
  5: **Date:** Created December 2024  
  6: **Target User:** Content creators exploring Dyson Sphere Program mechanics and theoretical mega-structures  
  7: **Development Philosophy:** EASY BUILD, userbase of 1, fun over academic rigor
  8: 
  9: ---
 10: 
 11: ## 1. Introduction/Overview
 12: 
 13: A specialized AI agent that combines deep knowledge of Dyson Sphere Program game mechanics with theoretical physics speculation about mega-structures. This agent serves content creators, sci-fi writers, and curious players who want to explore both the game&apos;s intricate systems and the wild possibilities of actual Dyson sphere construction.
 14: 
 15: The agent bridges gaming and science communication by treating the game&apos;s physics seriously while maintaining a fun, engaging tone. It can answer detailed questions about DSP mechanics while also speculating about real-world mega-engineering projects using web searches and physics knowledge.
 16: 
 17: ---
 18: 
 19: ## 2. Goals
 20: 
 21: 1. **Enable Fun Physics Speculation** - Allow users to explore &quot;what if&quot; scenarios comparing game mechanics to theoretical physics
 22: 2. **Provide DSP Expertise** - Answer detailed questions about game mechanics, production chains, and optimization strategies  
 23: 3. **Support Content Creation** - Help creators generate interesting discussions about mega-structures and space engineering
 24: 4. **Maintain Simplicity** - Focus on easy deployment and maintenance for a single user
 25: 
 26: ---
 27: 
 28: ## 3. User Stories
 29: 
 30: ### Primary User Stories
 31: 1. **Hard Sci-Fi Writer**: &quot;As a hard sci-fi writer, I want detailed and realistic physics explanations with speculative theorizing based on web searches and actual data, generating solid hypotheses for story elements&quot;
 32: 
 33: 2. **Content Creator**: &quot;As a content creator, I want to explore both DSP mechanics and theoretical mega-structure concepts to create engaging content comparing game mechanics to real physics&quot;
 34: 
 35: 3. **New Player**: &quot;As a new DSP player, I want quick answers about basic game mechanics without searching multiple wikis&quot;
 36: 
 37: ### Example Interactions
 38: - &quot;What would a cost-benefit analysis of Dyson sphere construction look like compared to 150 nuclear power facilities?&quot;
 39: - &quot;How do Critical Photons relate to antimatter production in terms of quantum field theory?&quot;
 40: - &quot;If we built a folding spacetime warp gate with a basic sphere, what physics would apply?&quot;
 41: 
 42: ---
 43: 
 44: ## 4. Functional Requirements
 45: 
 46: ### Core Agent Capabilities
 47: 1. **Answer DSP game mechanics questions** using ingested documentation
 48: 2. **Perform web searches** for latest physics research and speculation
 49: 3. **Blend game mechanics with real physics** in responses
 50: 4. **Generate fun, engaging explanations** (not dry academic text)
 51: 5. **Support conversational interaction** through chosen interface
 52: 
 53: ### Documentation &amp; Knowledge
 54: 6. **Ingest DSP wikis and documentation** (dsp-wiki.com, Reddit guides, official docs)
 55: 7. **Access web search** for current physics papers and speculation
 56: 8. **[PLACEHOLDER: YouTube Transcript Integration]** - Process DSP tutorial videos if time permits
 57: 
 58: ### Technical Integration
 59: 9. **Use existing RAG/documentation tools** via MCP servers or framework features
 60: 10. **Deploy through simple interface** (Claudable or chosen framework&apos;s UI)
 61: 11. **Support streaming responses** for better UX
 62: 
 63: ---
 64: 
 65: ## 5. Non-Goals (Out of Scope for MVP)
 66: 
 67: 1. **Production chain calculations** - Save for Phase 2
 68: 2. **Blueprint sharing/storage** - Not needed
 69: 3. **Multi-language support** - English only
 70: 4. **User authentication** - Single user
 71: 5. **Complex error handling** - Keep it simple
 72: 6. **Scalability considerations** - Userbase of 1
 73: 7. **Real-time game data** - Static documentation only
 74: 8. **Mod compatibility** - Base game only
 75: 
 76: ---
 77: 
 78: ## 6. Technical Architecture Decisions
 79: 
 80: ### Framework Selection
 81: **[TECH DECISION NEEDED - Choose ONE]:**
 82: 
 83: #### Option A: n8n (Visual + Powerful)
 84: - ‚úÖ Visual workflow builder
 85: - ‚úÖ Native LangChain/AI support  
 86: - ‚úÖ 400+ integrations
 87: - ‚úÖ MCP server support confirmed
 88: - ‚úÖ Self-hostable
 89: - üìù **Setup:** `npx n8n` or Docker
 90: - ü§î **Consideration:** Might be overkill for single agent
 91: 
 92: #### Option B: Agent Squad (Multi-Agent)
 93: - ‚úÖ Multiple specialized agents (DSP expert, physics expert, web searcher)
 94: - ‚úÖ SupervisorAgent coordination
 95: - ‚úÖ Python and TypeScript
 96: - ‚úÖ Pre-built components
 97: - üìù **Setup:** `pip install agent-squad[all]`
 98: - ü§î **Consideration:** AWS-focused but works anywhere
 99: 
100: #### Option C: Strands (Lightweight MCP)
101: - ‚úÖ Native MCP support built-in
102: - ‚úÖ Super lightweight
103: - ‚úÖ Hot reloading tools
104: - ‚úÖ Simple decorators for tools
105: - üìù **Setup:** `pip install strands-agents`
106: - ü§î **Consideration:** Newer framework, less ecosystem
107: 
108: #### Option D: LangGraph + MCP Adapters (Standard)
109: - ‚úÖ Industry standard (LangChain)
110: - ‚úÖ Proven MCP integration
111: - ‚úÖ Huge ecosystem
112: - ‚úÖ teddynote-lab/langgraph-mcp-agents exists
113: - üìù **Setup:** Use existing repo or minimal config
114: - ü§î **Consideration:** More traditional coding approach
115: 
116: ### MCP Server Selection
117: **[TECH DECISION - Can use multiple]:**
118: - **Documentation RAG:** 
119:   - Needle (production-ready, requires API key)
120:   - mcp-ragdocs (OpenAI/Qdrant based)
121:   - kazuph/mcp-docs-rag (local directory, Gemini)
122: - **Web Search:**
123:   - Brave Search MCP
124:   - Perplexity MCP
125:   - Google Search MCP
126: 
127: ### Model Provider
128: **[TECH DECISION]:**
129: - Primary: Claude 3.5 Sonnet (via Anthropic API)
130: - Fallback: OpenAI GPT-4
131: - Local option: Ollama with Llama/Mistral
132: 
133: ### Deployment Interface
134: **[TECH DECISION]:**
135: - **Claudable** (original plan, Node.js based)
136: - **Framework&apos;s built-in UI** (n8n, Streamlit for Agent Squad)
137: - **Simple FastAPI + frontend**
138: 
139: ---
140: 
141: ## 7. System Prompt &amp; Agent Personality
142: 
143: ```
144: You are an expert physicist specializing in:
145: - Quantum physics and theoretical particle physics
146: - Interstellar travel, warp drives, and space logistics  
147: - Mega-structure construction (Dyson spheres, swarms, shells)
148: - Matter-antimatter reactions and critical photon physics
149: - The Dyson Sphere Program game mechanics
150: 
151: You blend game mechanics with real physics speculation. Be fun and engaging, 
152: not boring or overly academic. Use metaphors like &quot;1 million International 
153: Space Stations&quot; for scale. Theorize boldly but ground speculation in actual 
154: physics and data from web searches.
155: 
156: When discussing DSP, reference specific game items, buildings, and mechanics.
157: When speculating about real physics, cite actual research when available.
158: ```
159: 
160: ---
161: 
162: ## 8. Success Metrics
163: 
164: Given userbase of 1, success is defined as:
165: 1. **It works** - Agent responds to questions
166: 2. **It&apos;s fun** - Responses are engaging, not dry
167: 3. **It&apos;s easy** - Can be set up in &lt; 1 hour
168: 4. **It&apos;s useful** - Provides value for content creation
169: 
170: ---
171: 
172: ## 9. Open Questions
173: 
174: ### Resolved Decisions:
175: - ‚úÖ **Framework:** n8n for visual workflow building
176: - ‚úÖ **MCP Servers:** mcp-ragdocs + Brave Search
177: - ‚úÖ **Deployment:** n8n&apos;s built-in interface initially
178: - ‚úÖ **Model:** Claude 3.5 Sonnet via n8n
179: 
180: ### Remaining Questions:
181: 1. **Documentation sources priority:**
182:    - Primary: dsp-wiki.com
183:    - Secondary: Reddit guides, official docs
184:    - YouTube transcripts: Phase 2
185: 
186: 2. **Content balance:**
187:    - 60% game mechanics accuracy
188:    - 40% physics speculation and fun theories
189:    - Always prioritize engagement over academic rigor
190: 
191: 3. **Initial test questions for validation:**
192:    - Game mechanics: &quot;How do Critical Photons work?&quot;
193:    - Physics speculation: &quot;Could we build a real Dyson sphere?&quot;
194:    - Hybrid: &quot;Compare game&apos;s antimatter production to real physics&quot;
195: 
196: ---
197: 
198: ## 10. Phase 2 Considerations (Post-MVP)
199: 
200: Once MVP is working, consider adding:
201: - **DSP Calculator integration** for production chains
202: - **Blueprint analysis** capabilities
203: - **Comparison tools** (e.g., &quot;sphere vs nuclear&quot; calculator)
204: - **Citation system** for documentation sources
205: - **Memory system** for ongoing projects/stories
206: - **Multi-agent specialization** (separate physics vs game experts)
207: 
208: ---
209: 
210: ## Implementation Notes
211: 
212: ### Quick Start Path:
213: 1. Choose framework based on comfort level
214: 2. Set up chosen MCP servers (start with 1 RAG + 1 search)
215: 3. Ingest core DSP documentation
216: 4. Configure system prompt
217: 5. Deploy with simplest interface option
218: 6. Test with example questions
219: 7. Iterate on prompt and documentation
220: 
221: ### Risk Mitigation:
222: - Start with smallest viable setup
223: - Use existing solutions (MCP servers, frameworks)
224: - Avoid custom RAG implementation
225: - Keep deployment local initially
226: - Focus on fun over perfection
227: 
228: ---
229: 
230: ## Appendix: Framework Comparison Matrix
231: 
232: | Feature | n8n | Agent Squad | Strands | LangGraph |
233: |---------|-----|-------------|---------|-----------|
234: | Learning Curve | Low (visual) | Medium | Low | Medium-High |
235: | MCP Support | Yes | Via tools | Native | Via adapters |
236: | Multi-Agent | Yes | Native | Single | Yes |
237: | Deployment | Docker/npx | Python/TS | Python | Python |
238: | UI Included | Yes | Examples | No | Via templates |
239: | Best For | Visual builders | Team agents | Simple tools | Standard approach |
240: 
241: ---
242: 
243: **Next Steps:**
244: 1. Review framework options and make selection
245: 2. Identify specific MCP servers to use
246: 3. Gather DSP documentation URLs
247: 4. Begin implementation with chosen stack</file><file path="docs/agent_specs/AGENT_SPEC_V6.md">  1: # AGENT SPECIFICATION V5 - SECURE ENVIRONMENT HANDLING
  2: 
  3: ## üö® MANDATORY SETUP FOR ALL AGENTS
  4: 
  5: ### Step 1: Install Poetry
  6: 
  7: ```bash
  8: curl -sSL https://install.python-poetry.org | python3 -
  9: ```
 10: 
 11: ### Step 2: Add Poetry to PATH
 12: 
 13: ```bash
 14: export PATH=&quot;$HOME/.local/bin:$PATH&quot;
 15: ```
 16: 
 17: ### Step 3: Navigate to Repository Root
 18: 
 19: ```bash
 20: # This works from ANY directory in the repo:
 21: cd $(git rev-parse --show-toplevel 2&gt;/dev/null) || cd /home/dev/Documents/github-projects/pydantic-trader
 22: ```
 23: 
 24: ### Step 4: Verify .env File Exists
 25: 
 26: ```bash
 27: ls -la .env  # MUST see .env file
 28: # If missing, STOP and report issue immediately
 29: ```
 30: 
 31: ### Step 5: Load Environment Variables SECURELY
 32: 
 33: ```bash
 34: # SECURE METHOD - Does not leak secrets to process list
 35: # Run this BEFORE any Poetry commands:
 36: set -a  # Enable automatic exporting
 37: [ -f .env ] &amp;&amp; source .env  # Source the file if it exists
 38: set +a  # Disable automatic exporting
 39: 
 40: # Verify it loaded (shows partial key only):
 41: echo ${DUNE_API_KEY:0:10}  # Shows first 10 chars only
 42: ```
 43: 
 44: ### Step 6: Install Dependencies
 45: 
 46: ```bash
 47: poetry install
 48: ```
 49: 
 50: ### Step 7: Verify Setup
 51: 
 52: ```bash
 53: poetry run python -c &quot;import os; print(&apos;‚úÖ Setup complete&apos; if os.getenv(&apos;DUNE_API_KEY&apos;) else &apos;‚ùå Missing env vars&apos;)&quot;
 54: ```
 55: 
 56: ---
 57: 
 58: ## üìù STANDARD OPERATING PROCEDURES
 59: 
 60: ### Always Start From Repo Root
 61: 
 62: ```bash
 63: # This command works from anywhere in the repo:
 64: cd $(git rev-parse --show-toplevel)
 65: 
 66: # Fallback if not in git repo:
 67: cd /home/dev/Documents/github-projects/pydantic-trader
 68: ```
 69: 
 70: ### Secure Environment Loading Pattern
 71: 
 72: ```bash
 73: # Always use this pattern before running any Python/Poetry commands:
 74: cd $(git rev-parse --show-toplevel)
 75: set -a &amp;&amp; [ -f .env ] &amp;&amp; source .env &amp;&amp; set +a
 76: ```
 77: 
 78: ### Running Applications
 79: 
 80: ```bash
 81: # Pattern for running any Python file:
 82: cd $(git rev-parse --show-toplevel)
 83: set -a &amp;&amp; [ -f .env ] &amp;&amp; source .env &amp;&amp; set +a
 84: BRANCH=$(git branch --show-current)
 85: poetry run python [script_name].py &gt; &quot;output_${BRANCH}_$(date +%Y%m%d_%H%M%S).md&quot; 2&gt;&amp;1
 86: ```
 87: 
 88: ### Running Tests
 89: 
 90: ```bash
 91: # Pattern for running tests:
 92: cd $(git rev-parse --show-toplevel)
 93: set -a &amp;&amp; [ -f .env ] &amp;&amp; source .env &amp;&amp; set +a
 94: BRANCH=$(git branch --show-current)
 95: poetry run pytest pydantic_trader/tests/[test_file] -v &gt; &quot;test_${BRANCH}_$(date +%Y%m%d_%H%M%S).md&quot; 2&gt;&amp;1
 96: ```
 97: 
 98: ---
 99: 
100: ## ‚ö†Ô∏è SECURITY WARNINGS
101: 
102: ### NEVER DO THIS (Insecure Methods)
103: 
104: ```bash
105: # ‚ùå INSECURE - Leaks secrets to process list:
106: export $(grep -v &apos;^#&apos; .env | xargs)
107: 
108: # ‚ùå INSECURE - Shows full API keys:
109: cat .env
110: echo $DUNE_API_KEY  # Full key visible
111: 
112: # ‚ùå INSECURE - Breaks on spaces/newlines:
113: export $(cat .env | xargs)
114: ```
115: 
116: ### ALWAYS DO THIS (Secure Methods)
117: 
118: ```bash
119: # ‚úÖ SECURE - No process list leak:
120: set -a &amp;&amp; source .env &amp;&amp; set +a
121: 
122: # ‚úÖ SECURE - Partial verification only:
123: echo ${DUNE_API_KEY:0:10}
124: 
125: # ‚úÖ SECURE - Check without exposing:
126: [ -n &quot;$DUNE_API_KEY&quot; ] &amp;&amp; echo &quot;Key loaded&quot; || echo &quot;Key missing&quot;
127: ```
128: 
129: ---
130: 
131: ## üéØ AGENT RESPONSIBILITIES
132: 
133: ### What You CAN Do
134: 
135: - Run `uni_handler.py` and capture output to .md files
136: - Run tests in `pydantic_trader/tests/` directory
137: - Create analysis reports in .md format
138: - Comment on PRs with findings
139: - Read and analyze existing code
140: 
141: ### What You CANNOT Do
142: 
143: - Create new directories (especially not `/scripts/`)
144: - Modify production code files
145: - Use mock data, cache, or fallback values
146: - Run bare `pip` or `python` (always use `poetry run`)
147: - Expose full API keys or secrets
148: 
149: ### Report Naming Convention
150: 
151: ```
152: [TYPE]_[DESCRIPTION]_[BRANCH]_YYYYMMDD_HHMMSS.md
153: 
154: Where:
155: - TYPE: test | run | analysis | audit
156: - DESCRIPTION: brief-kebab-case (e.g., duplicate-detection)
157: - BRANCH: git branch name (no spaces)
158: - TIMESTAMP: YYYYMMDD_HHMMSS format
159: 
160: Examples:
161: - test_token-math_main_20250812_143022.md
162: - run_arbitrage-scan_main_20250812_143022.md
163: - analysis_duplicate-bug_fix-refresh_20250812_143022.md
164: ```
165: 
166: ---
167: 
168: ## üìã COMMON ISSUES &amp; SOLUTIONS
169: 
170: ### Issue: &quot;DUNE_API_KEY not found&quot;
171: 
172: ```bash
173: # Solution - reload environment:
174: cd $(git rev-parse --show-toplevel)
175: set -a &amp;&amp; [ -f .env ] &amp;&amp; source .env &amp;&amp; set +a
176: ```
177: 
178: ### Issue: &quot;Not in a git repository&quot;
179: 
180: ```bash
181: # Solution - use absolute path:
182: cd /home/dev/Documents/github-projects/pydantic-trader
183: ```
184: 
185: ### Issue: &quot;poetry: command not found&quot;
186: 
187: ```bash
188: # Solution - add to PATH:
189: export PATH=&quot;$HOME/.local/bin:$PATH&quot;
190: ```
191: 
192: ### Issue: Can&apos;t find .env file
193: 
194: ```bash
195: # Check you&apos;re in right location:
196: pwd  # Should show .../pydantic-trader
197: ls -la .env  # Should show the file
198: ```
199: 
200: ---
201: 
202: ## ‚úÖ QUICK REFERENCE CARD
203: 
204: ```bash
205: # 1. Go to repo root (works from anywhere):
206: cd $(git rev-parse --show-toplevel)
207: 
208: # 2. Load environment securely:
209: set -a &amp;&amp; [ -f .env ] &amp;&amp; source .env &amp;&amp; set +a
210: 
211: # 3. Run with output capture:
212: BRANCH=$(git branch --show-current)
213: poetry run python uni_handler.py &gt; &quot;run_description_${BRANCH}_$(date +%Y%m%d_%H%M%S).md&quot; 2&gt;&amp;1
214: 
215: # 4. Check it worked:
216: ls -la *.md  # See your output file
217: ```
218: 
219: ## Zero Tolerance Rules
220: 
221: - NO mock data - use real Dune data or return empty
222: - NO fallback values - fail fast with clear errors
223: - NO cache layers - always fetch fresh data
224: - USE Poetry for everything - never bare pip/python
225: 
226: ---
227: 
228: ## MCP Server Tools Available
229: 
230: - **Uniswap Pools MCP**: `get_token_pools`, `get_pool_data`
231: - **HTTP Gateway**: http://localhost:8888 (with logging)
232: - **Note**: Aave MCP exists but is OUT OF SCOPE for MVP</file><file path="docs/agent_specs/dsp-agent-prd-updated.md">  1: # Product Requirements Document
  2: ## DSP Documentation &amp; Physics Speculation Agent
  3: 
  4: **Version:** 1.1 MVP  
  5: **Date:** Updated January 2025  
  6: **Target User:** Content creators exploring Dyson Sphere Program mechanics and theoretical mega-structures  
  7: **Development Philosophy:** EASY BUILD, userbase of 1, fun over academic rigor
  8: 
  9: ---
 10: 
 11: ## 1. Introduction/Overview
 12: 
 13: A specialized AI agent that combines deep knowledge of Dyson Sphere Program game mechanics with theoretical physics speculation about mega-structures. This agent serves content creators, sci-fi writers, and curious players who want to explore both the game&apos;s intricate systems and the wild possibilities of actual Dyson sphere construction.
 14: 
 15: The agent bridges gaming and science communication by treating the game&apos;s physics seriously while maintaining a fun, engaging tone. It can answer detailed questions about DSP mechanics while also speculating about real-world mega-engineering projects using web searches and physics knowledge.
 16: 
 17: ---
 18: 
 19: ## 2. Goals
 20: 
 21: 1. **Enable Fun Physics Speculation** - Allow users to explore &quot;what if&quot; scenarios comparing game mechanics to theoretical physics
 22: 2. **Provide DSP Expertise** - Answer detailed questions about game mechanics, production chains, and optimization strategies  
 23: 3. **Support Content Creation** - Help creators generate interesting discussions about mega-structures and space engineering
 24: 4. **Maintain Simplicity** - Focus on easy deployment and maintenance for a single user
 25: 
 26: ---
 27: 
 28: ## 3. User Stories
 29: 
 30: ### Primary User Stories
 31: 1. **Hard Sci-Fi Writer**: &quot;As a hard sci-fi writer, I want detailed and realistic physics explanations with speculative theorizing based on web searches and actual data, generating solid hypotheses for story elements&quot;
 32: 
 33: 2. **Content Creator**: &quot;As a content creator, I want to explore both DSP mechanics and theoretical mega-structure concepts to create engaging content comparing game mechanics to real physics&quot;
 34: 
 35: 3. **New Player**: &quot;As a new DSP player, I want quick answers about basic game mechanics without searching multiple wikis&quot;
 36: 
 37: ### Example Interactions
 38: - &quot;What would a cost-benefit analysis of Dyson sphere construction look like compared to 150 nuclear power facilities?&quot;
 39: - &quot;How do Critical Photons relate to antimatter production in terms of quantum field theory?&quot;
 40: - &quot;If we built a folding spacetime warp gate with a basic sphere, what physics would apply?&quot;
 41: 
 42: ---
 43: 
 44: ## 4. Functional Requirements
 45: 
 46: ### Core Agent Capabilities
 47: 1. **Answer DSP game mechanics questions** using ingested documentation
 48: 2. **Perform web searches** for latest physics research and speculation
 49: 3. **Blend game mechanics with real physics** in responses
 50: 4. **Generate fun, engaging explanations** (not dry academic text)
 51: 5. **Support conversational interaction** through Claudable interface
 52: 
 53: ### Documentation &amp; Knowledge
 54: 6. **Ingest DSP wikis and documentation** (dsp-wiki.com, Reddit guides, official docs)
 55: 7. **Access web search** for current physics papers and speculation
 56: 8. **[PLACEHOLDER: YouTube Transcript Integration]** - Process DSP tutorial videos if time permits
 57: 
 58: ### Technical Integration
 59: 9. **Use existing MCP servers** via Docker Desktop/modelinspector protocol
 60: 10. **Deploy through Claudable** (Node.js based chatbot interface)
 61: 11. **Support streaming responses** for better UX
 62: 
 63: ---
 64: 
 65: ## 5. Non-Goals (Out of Scope for MVP)
 66: 
 67: 1. **Production chain calculations** - Save for Phase 2
 68: 2. **Blueprint sharing/storage** - Not needed
 69: 3. **Multi-language support** - English only
 70: 4. **User authentication** - Single user
 71: 5. **Complex error handling** - Keep it simple
 72: 6. **Scalability considerations** - Userbase of 1
 73: 7. **Real-time game data** - Static documentation only
 74: 8. **Mod compatibility** - Base game only
 75: 9. **Multi-agent orchestration** - Single agent handles all tasks
 76: 
 77: ---
 78: 
 79: ## 6. Technical Architecture Decisions
 80: 
 81: ### Frontend Interface
 82: **Claudable** - Node.js based chatbot interface for conversational interaction
 83: 
 84: ### Agent Architecture
 85: **Two-Agent Design** with Claude 4 Sonnet:
 86: 
 87: **Frontend Agent (Claudable Handler):**
 88: - Manages Claudable interface interactions
 89: - Formats requests for backend agent
 90: - Handles response streaming and presentation
 91: - Maintains conversation context
 92: 
 93: **Backend Agent (DSP Specialist):**
 94: - Core DSP game mechanics expertise
 95: - Physics speculation and research
 96: - Direct MCP server orchestration
 97: - Web searches for current science
 98: 
 99: **Inter-Agent Communication:**
100: - Structured report format for state transfer
101: - Frontend agent passes full conversation context + user query
102: - Backend agent returns structured response with:
103:   - Answer content
104:   - Sources used (MCP servers accessed)
105:   - Confidence levels for speculative content
106:   - Any follow-up suggestions
107: - Both agents maintain awareness via shared context reports
108: 
109: ### MCP Server Stack (via Docker Desktop/modelinspector)
110: 
111: #### Available MCP Servers:
112: **Documentation &amp; RAG:**
113: - `Context7` - Already installed, can fetch DSP-related documentation
114: - `web3-research-mcp` - Can be adapted for physics/DSP documentation
115: - `mcp-ragdocs` - For DSP wiki ingestion (to be added)
116: 
117: **Web Search &amp; Research:**
118: - Brave Search MCP (via existing Docker container)
119: - Additional physics/science search servers as available
120: 
121: **Existing Infrastructure (available if needed):**
122: - Various crypto/DeFi servers from `/Users/dev/Documents/MCP/`
123: - Can leverage for economic comparisons if relevant
124: 
125: ### Connection Method
126: - Docker Desktop containers accessed via `modelinspector` protocol
127: - Direct MCP protocol connections (no SSH)
128: - Enumerate available servers: `mcp --server-url &lt;modelinspector-endpoint&gt;`
129: 
130: ### Orchestration Approach
131: ```
132: Claudable Frontend
133:     ‚Üì
134: Single DSP Agent (Claude 3.5 Sonnet)
135:     ‚Üì
136: Direct MCP Server Calls
137:     ‚Üì
138: Docker Desktop MCP Containers
139: ```
140: 
141: ### Model Provider
142: - Primary: Claude 3.5 Sonnet (via Anthropic API)
143: - Direct orchestration through single agent
144: - Fallback: OpenAI GPT-4 if needed
145: 
146: ---
147: 
148: ## 7. System Prompt &amp; Agent Personality
149: 
150: ```
151: You are an expert physicist specializing in:
152: - Quantum physics and theoretical particle physics
153: - Interstellar travel, warp drives, and space logistics  
154: - Mega-structure construction (Dyson spheres, swarms, shells)
155: - Matter-antimatter reactions and critical photon physics
156: - The Dyson Sphere Program game mechanics
157: 
158: You blend game mechanics with real physics speculation. Be fun and engaging, 
159: not boring or overly academic. Use metaphors like &quot;1 million International 
160: Space Stations&quot; for scale. Theorize boldly but ground speculation in actual 
161: physics and data from web searches.
162: 
163: When discussing DSP, reference specific game items, buildings, and mechanics.
164: When speculating about real physics, cite actual research when available.
165: 
166: You have direct access to MCP servers for documentation and web search.
167: Use these tools naturally to enhance your responses.
168: ```
169: 
170: ---
171: 
172: ## 8. Success Metrics
173: 
174: Given userbase of 1, success is defined as:
175: 1. **It works** - Agent responds to questions
176: 2. **It&apos;s fun** - Responses are engaging, not dry
177: 3. **It&apos;s easy** - Can be set up in &lt; 1 hour
178: 4. **It&apos;s useful** - Provides value for content creation
179: 
180: ---
181: 
182: ## 9. Open Questions
183: 
184: ### Resolved Decisions:
185: - ‚úÖ **Architecture:** Direct MCP orchestration with single agent
186: - ‚úÖ **Frontend:** Claudable for chat interface
187: - ‚úÖ **MCP Servers:** Docker Desktop containers via modelinspector
188: - ‚úÖ **Model:** Claude 3.5 Sonnet with direct API access
189: 
190: ### Remaining Questions:
191: 1. **Documentation sources priority:**
192:    - Primary: dsp-wiki.com
193:    - Secondary: Reddit guides, official docs
194:    - YouTube transcripts: Phase 2
195: 
196: 2. **Content balance:**
197:    - 60% game mechanics accuracy
198:    - 40% physics speculation and fun theories
199:    - Always prioritize engagement over academic rigor
200: 
201: 3. **Initial test questions for validation:**
202:    - Game mechanics: &quot;How do Critical Photons work</file><file path="docs/templates/generate-tasks.md"> 1: # Rule: Generating a Task List from a PRD
 2: 
 3: ## Goal
 4: 
 5: To guide an AI assistant in creating a detailed, step-by-step task list in Markdown format based on an existing Product Requirements Document (PRD). The task list should guide a developer through implementation.
 6: 
 7: ## Output
 8: 
 9: - **Format:** Markdown (`.md`)
10: - **Location:** `/tasks/`
11: - **Filename:** `tasks-[prd-file-name].md` (e.g., `tasks-prd-user-profile-editing.md`)
12: 
13: ## Process
14: 
15: 1.  **Receive PRD Reference:** The user points the AI to a specific PRD file
16: 2.  **Analyze PRD:** The AI reads and analyzes the functional requirements, user stories, and other sections of the specified PRD.
17: 3.  **Assess Current State:** Review the existing codebase to understand existing infrastructre, architectural patterns and conventions. Also, identify any existing components or features that already exist and could be relevant to the PRD requirements. Then, identify existing related files, components, and utilities that can be leveraged or need modification.
18: 4.  **Phase 1: Generate Parent Tasks:** Based on the PRD analysis and current state assessment, create the file and generate the main, high-level tasks required to implement the feature. Use your judgement on how many high-level tasks to use. It&apos;s likely to be about 
19: 5. **Inform the user:** Present these tasks to the user in the specified format (without sub-tasks yet) For example, say &quot;I have generated the high-level tasks based on the PRD. Ready to generate the sub-tasks? Respond with &apos;Go&apos; to proceed.&quot; . 
20: 6.  **Wait for Confirmation:** Pause and wait for the user to respond with &quot;Go&quot;.
21: 7.  **Phase 2: Generate Sub-Tasks:** Once the user confirms, break down each parent task into smaller, actionable sub-tasks necessary to complete the parent task. Ensure sub-tasks logically follow from the parent task, cover the implementation details implied by the PRD, and consider existing codebase patterns where relevant without being constrained by them.
22: 8.  **Identify Relevant Files:** Based on the tasks and PRD, identify potential files that will need to be created or modified. List these under the `Relevant Files` section, including corresponding test files if applicable.
23: 9.  **Generate Final Output:** Combine the parent tasks, sub-tasks, relevant files, and notes into the final Markdown structure.
24: 10.  **Save Task List:** Save the generated document in the `/tasks/` directory with the filename `tasks-[prd-file-name].md`, where `[prd-file-name]` matches the base name of the input PRD file (e.g., if the input was `prd-user-profile-editing.md`, the output is `tasks-prd-user-profile-editing.md`).
25: 
26: ## Output Format
27: 
28: The generated task list _must_ follow this structure:
29: 
30: ```markdown
31: ## Relevant Files
32: 
33: - `path/to/potential/file1.ts` - Brief description of why this file is relevant (e.g., Contains the main component for this feature).
34: - `path/to/file1.test.ts` - Unit tests for `file1.ts`.
35: - `path/to/another/file.tsx` - Brief description (e.g., API route handler for data submission).
36: - `path/to/another/file.test.tsx` - Unit tests for `another/file.tsx`.
37: - `lib/utils/helpers.ts` - Brief description (e.g., Utility functions needed for calculations).
38: - `lib/utils/helpers.test.ts` - Unit tests for `helpers.ts`.
39: 
40: ### Notes
41: 
42: - Unit tests should typically be placed alongside the code files they are testing (e.g., `MyComponent.tsx` and `MyComponent.test.tsx` in the same directory).
43: - Use `npx jest [optional/path/to/test/file]` to run tests. Running without a path executes all tests found by the Jest configuration.
44: 
45: ## Tasks
46: 
47: - [ ] 1.0 Parent Task Title
48:   - [ ] 1.1 [Sub-task description 1.1]
49:   - [ ] 1.2 [Sub-task description 1.2]
50: - [ ] 2.0 Parent Task Title
51:   - [ ] 2.1 [Sub-task description 2.1]
52: - [ ] 3.0 Parent Task Title (may not require sub-tasks if purely structural or configuration)
53: ```
54: 
55: ## Interaction Model
56: 
57: The process explicitly requires a pause after generating parent tasks to get user confirmation (&quot;Go&quot;) before proceeding to generate the detailed sub-tasks. This ensures the high-level plan aligns with user expectations before diving into details.
58: 
59: ## Target Audience
60: 
61: Assume the primary reader of the task list is a **junior developer** who will implement the feature with awareness of the existing codebase context.</file><file path="docs/templates/process-task-list.md"> 1: # Task List Management
 2: 
 3: Guidelines for managing task lists in markdown files to track progress on completing a PRD
 4: 
 5: ## Task Implementation
 6: - **One sub-task at a time:** Do **NOT** start the next sub‚Äëtask until you ask the user for permission and they say &quot;yes&quot; or &quot;y&quot;
 7: - **Completion protocol:**  
 8:   1. When you finish a **sub‚Äëtask**, immediately mark it as completed by changing `[ ]` to `[x]`.
 9:   2. If **all** subtasks underneath a parent task are now `[x]`, follow this sequence:
10:     - **First**: Run the full test suite (`pytest`, `npm test`, `bin/rails test`, etc.)
11:     - **Only if all tests pass**: Stage changes (`git add .`)
12:     - **Clean up**: Remove any temporary files and temporary code before committing
13:     - **Commit**: Use a descriptive commit message that:
14:       - Uses conventional commit format (`feat:`, `fix:`, `refactor:`, etc.)
15:       - Summarizes what was accomplished in the parent task
16:       - Lists key changes and additions
17:       - References the task number and PRD context
18:       - **Formats the message as a single-line command using `-m` flags**, e.g.:
19: 
20:         ```
21:         git commit -m &quot;feat: add payment validation logic&quot; -m &quot;- Validates card type and expiry&quot; -m &quot;- Adds unit tests for edge cases&quot; -m &quot;Related to T123 in PRD&quot;
22:         ```
23:   3. Once all the subtasks are marked completed and changes have been committed, mark the **parent task** as completed.
24: - Stop after each sub‚Äëtask and wait for the user&apos;s go‚Äëahead.
25: 
26: ## Task List Maintenance
27: 
28: 1. **Update the task list as you work:**
29:    - Mark tasks and subtasks as completed (`[x]`) per the protocol above.
30:    - Add new tasks as they emerge.
31: 
32: 2. **Maintain the &quot;Relevant Files&quot; section:**
33:    - List every file created or modified.
34:    - Give each file a one‚Äëline description of its purpose.
35: 
36: ## AI Instructions
37: 
38: When working with task lists, the AI must:
39: 
40: 1. Regularly update the task list file after finishing any significant work.
41: 2. Follow the completion protocol:
42:    - Mark each finished **sub‚Äëtask** `[x]`.
43:    - Mark the **parent task** `[x]` once **all** its subtasks are `[x]`.
44: 3. Add newly discovered tasks.
45: 4. Keep &quot;Relevant Files&quot; accurate and up to date.
46: 5. Before starting work, check which sub‚Äëtask is next.
47: 6. After implementing a sub‚Äëtask, update the file and then pause for user approval.</file><file path="tests/__init__.py"> 1: &quot;&quot;&quot;
 2: DSP Real-Data Test Suite
 3: 
 4: This test suite follows strict real-data testing principles:
 5: - NO mocks, patches, stubs, or test doubles
 6: - ALL tests interact with real services and infrastructure
 7: - Tests fail when real systems fail
 8: - Every HTTP call hits actual endpoints
 9: - Every Docker command checks live containers
10: - Every file operation reads real filesystem
11: 
12: Test organization:
13: - test_critical.py: Infrastructure validation tests
14: - Future: test_mcp_integration.py, test_performance.py, etc.
15: &quot;&quot;&quot;</file><file path="tests/conftest.py"> 1: &quot;&quot;&quot;
 2: Pytest configuration for DSP real-data test suite.
 3: 
 4: REAL-DATA TESTING: This conftest.py provides shared test utilities
 5: but NEVER provides mocks, patches, or fake data.
 6: &quot;&quot;&quot;
 7: 
 8: import pytest
 9: import asyncio
10: import httpx
11: from pathlib import Path
12: 
13: # Project root for all tests
14: PROJECT_ROOT = Path(__file__).parent.parent
15: 
16: 
17: @pytest.fixture(scope=&quot;session&quot;)
18: def project_root():
19:     &quot;&quot;&quot;Provide project root path for all tests.&quot;&quot;&quot;
20:     return PROJECT_ROOT
21: 
22: 
23: @pytest.fixture(scope=&quot;session&quot;)
24: def mcp_endpoints():
25:     &quot;&quot;&quot;Provide real MCP endpoint URLs - no mocks.&quot;&quot;&quot;
26:     return {
27:         &quot;rag&quot;: &quot;http://localhost:3002&quot;,
28:         &quot;search&quot;: &quot;http://localhost:3004&quot;,
29:         &quot;qdrant&quot;: &quot;http://localhost:6333&quot;,
30:         &quot;claudable&quot;: &quot;http://localhost:3001&quot;
31:     }
32: 
33: 
34: @pytest.fixture
35: async def http_client():
36:     &quot;&quot;&quot;Provide real HTTP client for making actual requests.&quot;&quot;&quot;
37:     async with httpx.AsyncClient(timeout=5.0) as client:
38:         yield client
39: 
40: 
41: # Pytest markers for test organization
42: def pytest_configure(config):
43:     &quot;&quot;&quot;Register custom pytest markers.&quot;&quot;&quot;
44:     config.addinivalue_line(
45:         &quot;markers&quot;, &quot;infrastructure: Tests that validate basic infrastructure (Docker, endpoints)&quot;
46:     )
47:     config.addinivalue_line(
48:         &quot;markers&quot;, &quot;integration: Tests that validate service integration&quot;
49:     )
50:     config.addinivalue_line(
51:         &quot;markers&quot;, &quot;performance: Tests that measure real system performance&quot;
52:     )
53:     config.addinivalue_line(
54:         &quot;markers&quot;, &quot;slow: Tests that take longer than 5 seconds&quot;
55:     )</file><file path="tests/README.md">  1: # DSP Real-Data Test Suite
  2: 
  3: This test suite validates the Dyson Sphere Program Documentation &amp; Physics Speculation system using **strict real-data testing principles**.
  4: 
  5: ## Core Philosophy: REAL DATA OR DEATH
  6: 
  7: **ZERO TOLERANCE FOR MOCKS**
  8: 
  9: This test suite **ABSOLUTELY REJECTS** all forms of test doubles:
 10: - ‚ùå `unittest.mock` and `patch` decorators
 11: - ‚ùå Fake/hardcoded test data
 12: - ‚ùå Stubbed responses or predetermined results
 13: - ‚ùå Any form of test fakes or doubles
 14: 
 15: **ONLY REAL DATA ALLOWED**
 16: 
 17: All tests use:
 18: - ‚úÖ Real API calls to actual endpoints
 19: - ‚úÖ Live Docker containers and services
 20: - ‚úÖ Actual filesystem operations
 21: - ‚úÖ Fresh data retrieved at runtime
 22: - ‚úÖ Real HTTP requests with real timeouts
 23: 
 24: ## Test Structure
 25: 
 26: ### Infrastructure Tests (`@pytest.mark.infrastructure`)
 27: 
 28: Located in `test_critical.py` - validates basic system infrastructure:
 29: 
 30: 1. **Docker Container Validation**
 31:    - Real `docker ps` commands check live containers
 32:    - Validates `dsp-mcp-ragdocs`, `dsp-mcp-search`, `dsp-qdrant` containers
 33: 
 34: 2. **HTTP Endpoint Validation**
 35:    - Real HTTP requests to actual endpoints
 36:    - Tests localhost:3002 (RAG), localhost:3004 (Search), localhost:6333 (Qdrant)
 37:    - Real connection errors and timeouts
 38: 
 39: 3. **Configuration Validation**
 40:    - Reads actual JSON config files from filesystem
 41:    - Validates real docker-compose.yml with `docker-compose config`
 42: 
 43: ### Future Test Categories
 44: 
 45: - `@pytest.mark.integration` - Service-to-service integration tests
 46: - `@pytest.mark.performance` - Real performance measurement tests
 47: - `@pytest.mark.slow` - Tests taking &gt;5 seconds
 48: 
 49: ## Running Tests
 50: 
 51: ### Prerequisites
 52: 
 53: **CRITICAL**: Infrastructure must be running for tests to pass. This is intentional - tests fail when real systems fail.
 54: 
 55: 1. Start Docker containers:
 56:    ```bash
 57:    cd docker &amp;&amp; docker-compose up -d
 58:    ```
 59: 
 60: 2. Verify all services are running:
 61:    ```bash
 62:    docker ps | grep dsp-
 63:    ```
 64: 
 65: ### Test Execution
 66: 
 67: ```bash
 68: # Run all tests
 69: poetry run pytest tests/
 70: 
 71: # Run only infrastructure tests
 72: poetry run pytest tests/ -m infrastructure
 73: 
 74: # Run with verbose output
 75: poetry run pytest tests/ -v
 76: 
 77: # Collect tests without running (useful for validation)
 78: poetry run pytest --collect-only tests/
 79: ```
 80: 
 81: ### Expected Behavior
 82: 
 83: - **When infrastructure is DOWN**: Tests fail immediately with clear error messages
 84: - **When infrastructure is UP**: Tests pass, validating actual system state
 85: - **Network issues**: Tests fail with real timeout/connection errors
 86: - **Configuration problems**: Tests fail when reading actual config files
 87: 
 88: ## Test File Organization
 89: 
 90: ```
 91: tests/
 92: ‚îú‚îÄ‚îÄ __init__.py           # Test suite documentation
 93: ‚îú‚îÄ‚îÄ conftest.py           # Pytest configuration (NO MOCKS)
 94: ‚îú‚îÄ‚îÄ test_critical.py      # Infrastructure validation tests
 95: ‚îî‚îÄ‚îÄ README.md            # This documentation
 96: ```
 97: 
 98: ## Key Implementation Details
 99: 
100: ### Path Resolution
101: ```python
102: PROJECT_ROOT = Path(__file__).parent.parent
103: config_path = PROJECT_ROOT / &quot;claudable&quot; / &quot;config.json&quot;
104: ```
105: 
106: ### Real HTTP Requests
107: ```python
108: async with httpx.AsyncClient() as client:
109:     response = await client.get(&quot;http://localhost:3002/&quot;, timeout=5.0)
110:     assert response.status_code &lt; 500
111: ```
112: 
113: ### Real Docker Commands
114: ```python
115: result = subprocess.run(
116:     [&quot;docker&quot;, &quot;ps&quot;, &quot;--filter&quot;, &quot;name=dsp-mcp-ragdocs&quot;],
117:     capture_output=True, text=True, timeout=10
118: )
119: ```
120: 
121: ## Error Handling Strategy
122: 
123: Tests provide **explicit failure messages** when real systems fail:
124: 
125: - Docker container not running: `&quot;MCP RAG container not running. Found: [actual output]&quot;`
126: - HTTP endpoint unreachable: `&quot;Cannot connect to MCP RAG server at localhost:3002: [error]&quot;`
127: - Config file missing: `&quot;Claudable config file not found at [path]&quot;`
128: - Timeout issues: `&quot;MCP RAG server timeout: [error]&quot;`
129: 
130: ## Dependencies
131: 
132: All testing dependencies managed via Poetry:
133: 
134: ```toml
135: [tool.poetry.group.dev.dependencies]
136: pytest = &quot;^7.4.0&quot;
137: pytest-asyncio = &quot;^0.21.0&quot;
138: httpx = &quot;^0.25.0&quot;  # For real HTTP requests
139: ```
140: 
141: **NOTE**: No mock libraries in dependencies - this is intentional.
142: 
143: ## Integration with CI/CD
144: 
145: These tests are designed for:
146: - **Development environment validation**
147: - **Integration testing in staging**
148: - **Production readiness verification**
149: 
150: Tests will fail in CI if infrastructure is not properly configured - this ensures real system validation.
151: 
152: ## Adding New Tests
153: 
154: When adding tests, follow these principles:
155: 
156: 1. **Real Data Only**: Every test must interact with real systems
157: 2. **Clear Markers**: Use appropriate `@pytest.mark.*` markers
158: 3. **Explicit Errors**: Provide clear failure messages
159: 4. **Timeout Handling**: Add timeouts for all external calls
160: 5. **Path Resolution**: Use `PROJECT_ROOT` for relative paths
161: 
162: ### Example Test Template
163: 
164: ```python
165: @pytest.mark.infrastructure
166: @pytest.mark.asyncio
167: async def test_new_endpoint():
168:     \&quot;\&quot;\&quot;Test description.
169:     REAL-DATA: Explanation of real data interaction.
170:     \&quot;\&quot;\&quot;
171:     async with httpx.AsyncClient() as client:
172:         try:
173:             response = await client.get(&quot;http://localhost:PORT/&quot;, timeout=5.0)
174:             assert response.status_code &lt; 500, f&quot;Error: {response.status_code} - {response.text}&quot;
175:         except httpx.ConnectError as e:
176:             assert False, f&quot;Cannot connect: {e}&quot;
177: 
178:     print(&quot;‚úì Test passed&quot;)
179: ```
180: 
181: ---
182: 
183: **Remember**: If you&apos;re tempted to add mocks, patches, or fake data - **DON&apos;T**. Fix the real system instead.</file><file path="tests/test_integration_e2e.py">  1: #!/usr/bin/env python3
  2: &quot;&quot;&quot;
  3: END-TO-END INTEGRATION TESTS
  4: Real-data testing of the complete DSP Documentation &amp; Physics pipeline
  5: 
  6: REAL-DATA TESTING PRINCIPLES:
  7: - No unittest.mock, no patches, no stubs
  8: - Every HTTP call hits real endpoints
  9: - Every Docker command checks real containers
 10: - Tests fail when infrastructure fails
 11: - Tests validate actual system behavior with real data flows
 12: &quot;&quot;&quot;
 13: 
 14: import httpx
 15: import asyncio
 16: import pytest
 17: import json
 18: import subprocess
 19: from pathlib import Path
 20: from urllib.parse import quote
 21: 
 22: # Project root for path resolution
 23: PROJECT_ROOT = Path(__file__).parent.parent
 24: 
 25: 
 26: @pytest.mark.integration
 27: @pytest.mark.asyncio
 28: async def test_complete_rag_search_pipeline():
 29:     &quot;&quot;&quot;Test: Complete RAG search pipeline end-to-end
 30:     REAL-DATA: Tests actual HTTP endpoints with real query processing.
 31:     &quot;&quot;&quot;
 32:     test_queries = [
 33:         &quot;Critical Photons&quot;,
 34:         &quot;Dyson Sphere construction&quot;,
 35:         &quot;Antimatter production&quot;,
 36:         &quot;Space Warper technology&quot;
 37:     ]
 38: 
 39:     async with httpx.AsyncClient() as client:
 40:         for query in test_queries:
 41:             encoded_query = quote(query)
 42:             response = await client.get(
 43:                 f&quot;http://localhost:3002/search?query={encoded_query}&quot;,
 44:                 timeout=10.0
 45:             )
 46: 
 47:             assert response.status_code == 200, f&quot;RAG search failed for query &apos;{query}&apos;: {response.status_code}&quot;
 48: 
 49:             data = response.json()
 50:             assert &quot;query&quot; in data, f&quot;Missing &apos;query&apos; field in response for &apos;{query}&apos;&quot;
 51:             assert &quot;results&quot; in data, f&quot;Missing &apos;results&apos; field in response for &apos;{query}&apos;&quot;
 52:             assert &quot;status&quot; in data, f&quot;Missing &apos;status&apos; field in response for &apos;{query}&apos;&quot;
 53:             assert data[&quot;query&quot;] == query, f&quot;Query mismatch: expected &apos;{query}&apos;, got &apos;{data[&apos;query&apos;]}&apos;&quot;
 54:             assert len(data[&quot;results&quot;]) &gt; 0, f&quot;No results returned for query &apos;{query}&apos;&quot;
 55: 
 56:             # Validate result structure
 57:             first_result = data[&quot;results&quot;][0]
 58:             assert &quot;title&quot; in first_result, f&quot;Missing &apos;title&apos; in result for &apos;{query}&apos;&quot;
 59:             assert &quot;content&quot; in first_result, f&quot;Missing &apos;content&apos; in result for &apos;{query}&apos;&quot;
 60:             assert &quot;source&quot; in first_result, f&quot;Missing &apos;source&apos; in result for &apos;{query}&apos;&quot;
 61: 
 62:             print(f&quot;‚úì RAG search pipeline works for: {query}&quot;)
 63: 
 64: 
 65: @pytest.mark.integration
 66: @pytest.mark.asyncio
 67: async def test_complete_web_search_pipeline():
 68:     &quot;&quot;&quot;Test: Complete web search pipeline end-to-end
 69:     REAL-DATA: Tests actual HTTP endpoints with real physics query processing.
 70:     &quot;&quot;&quot;
 71:     physics_queries = [
 72:         &quot;Dyson sphere physics&quot;,
 73:         &quot;antimatter propulsion physics&quot;,
 74:         &quot;stellar engineering physics&quot;,
 75:         &quot;megastructure orbital mechanics&quot;
 76:     ]
 77: 
 78:     async with httpx.AsyncClient() as client:
 79:         for query in physics_queries:
 80:             encoded_query = quote(query)
 81:             response = await client.get(
 82:                 f&quot;http://localhost:3004/search?query={encoded_query}&quot;,
 83:                 timeout=10.0
 84:             )
 85: 
 86:             assert response.status_code == 200, f&quot;Web search failed for query &apos;{query}&apos;: {response.status_code}&quot;
 87: 
 88:             data = response.json()
 89:             assert &quot;query&quot; in data, f&quot;Missing &apos;query&apos; field in response for &apos;{query}&apos;&quot;
 90:             assert &quot;results&quot; in data, f&quot;Missing &apos;results&apos; field in response for &apos;{query}&apos;&quot;
 91:             assert &quot;status&quot; in data, f&quot;Missing &apos;status&apos; field in response for &apos;{query}&apos;&quot;
 92:             assert data[&quot;query&quot;] == query, f&quot;Query mismatch: expected &apos;{query}&apos;, got &apos;{data[&apos;query&apos;]}&apos;&quot;
 93:             assert len(data[&quot;results&quot;]) &gt; 0, f&quot;No results returned for query &apos;{query}&apos;&quot;
 94: 
 95:             # Validate result structure
 96:             first_result = data[&quot;results&quot;][0]
 97:             assert &quot;title&quot; in first_result, f&quot;Missing &apos;title&apos; in result for &apos;{query}&apos;&quot;
 98:             assert &quot;content&quot; in first_result, f&quot;Missing &apos;content&apos; in result for &apos;{query}&apos;&quot;
 99:             assert &quot;source&quot; in first_result, f&quot;Missing &apos;source&apos; in result for &apos;{query}&apos;&quot;
100: 
101:             print(f&quot;‚úì Web search pipeline works for: {query}&quot;)
102: 
103: 
104: @pytest.mark.integration
105: @pytest.mark.asyncio
106: async def test_hybrid_agent_query_simulation():
107:     &quot;&quot;&quot;Test: Simulate complete DSP agent hybrid query (60% game mechanics + 40% physics)
108:     REAL-DATA: Tests real coordination between RAG and search services.
109:     &quot;&quot;&quot;
110: 
111:     # Simulate agent workflow for hybrid query
112:     hybrid_query = &quot;How do Critical Photons work and could they exist in real physics?&quot;
113: 
114:     async with httpx.AsyncClient() as client:
115:         # Step 1: Query DSP game mechanics (RAG)
116:         game_query = &quot;Critical Photons mechanics&quot;
117:         encoded_game_query = quote(game_query)
118:         rag_response = await client.get(
119:             f&quot;http://localhost:3002/search?query={encoded_game_query}&quot;,
120:             timeout=10.0
121:         )
122: 
123:         assert rag_response.status_code == 200, f&quot;RAG query failed: {rag_response.status_code}&quot;
124:         rag_data = rag_response.json()
125: 
126:         # Step 2: Query real physics speculation (Web search)
127:         physics_query = &quot;photon manipulation physics real applications&quot;
128:         encoded_physics_query = quote(physics_query)
129:         search_response = await client.get(
130:             f&quot;http://localhost:3004/search?query={encoded_physics_query}&quot;,
131:             timeout=10.0
132:         )
133: 
134:         assert search_response.status_code == 200, f&quot;Search query failed: {search_response.status_code}&quot;
135:         search_data = search_response.json()
136: 
137:         # Step 3: Validate hybrid response structure
138:         assert len(rag_data[&quot;results&quot;]) &gt; 0, &quot;No game mechanics results&quot;
139:         assert len(search_data[&quot;results&quot;]) &gt; 0, &quot;No physics speculation results&quot;
140: 
141:         # Step 4: Simulate agent response synthesis
142:         game_content = rag_data[&quot;results&quot;][0][&quot;content&quot;]
143:         physics_content = search_data[&quot;results&quot;][0][&quot;content&quot;]
144: 
145:         assert len(game_content) &gt; 10, &quot;Game mechanics content too short&quot;
146:         assert len(physics_content) &gt; 10, &quot;Physics content too short&quot;
147: 
148:         print(f&quot;‚úì Hybrid agent query simulation successful&quot;)
149:         print(f&quot;  Game mechanics: {game_content[:100]}...&quot;)
150:         print(f&quot;  Physics speculation: {physics_content[:100]}...&quot;)
151: 
152: 
153: @pytest.mark.integration
154: @pytest.mark.asyncio
155: async def test_concurrent_service_load():
156:     &quot;&quot;&quot;Test: Multiple concurrent requests to all services
157:     REAL-DATA: Tests real system performance under concurrent load.
158:     &quot;&quot;&quot;
159: 
160:     # Define concurrent test queries
161:     rag_queries = [&quot;Critical Photons&quot;, &quot;Space Warper&quot;, &quot;Antimatter&quot;, &quot;Dyson Sphere&quot;]
162:     search_queries = [&quot;fusion physics&quot;, &quot;space engineering&quot;, &quot;stellar mechanics&quot;, &quot;quantum physics&quot;]
163: 
164:     async with httpx.AsyncClient() as client:
165:         # Create concurrent tasks for RAG service
166:         rag_tasks = []
167:         for query in rag_queries:
168:             encoded_query = quote(query)
169:             task = client.get(f&quot;http://localhost:3002/search?query={encoded_query}&quot;, timeout=15.0)
170:             rag_tasks.append(task)
171: 
172:         # Create concurrent tasks for search service
173:         search_tasks = []
174:         for query in search_queries:
175:             encoded_query = quote(query)
176:             task = client.get(f&quot;http://localhost:3004/search?query={encoded_query}&quot;, timeout=15.0)
177:             search_tasks.append(task)
178: 
179:         # Execute all requests concurrently
180:         all_tasks = rag_tasks + search_tasks
181:         responses = await asyncio.gather(*all_tasks, return_exceptions=True)
182: 
183:         # Validate all responses
184:         success_count = 0
185:         for i, response in enumerate(responses):
186:             if isinstance(response, Exception):
187:                 print(f&quot;‚úó Request {i} failed: {response}&quot;)
188:             else:
189:                 assert response.status_code == 200, f&quot;Request {i} failed with status {response.status_code}&quot;
190:                 data = response.json()
191:                 assert &quot;results&quot; in data, f&quot;Request {i} missing results&quot;
192:                 assert len(data[&quot;results&quot;]) &gt; 0, f&quot;Request {i} has no results&quot;
193:                 success_count += 1
194: 
195:         assert success_count == len(all_tasks), f&quot;Only {success_count}/{len(all_tasks)} requests succeeded&quot;
196:         print(f&quot;‚úì Concurrent load test: {success_count}/{len(all_tasks)} requests successful&quot;)
197: 
198: 
199: @pytest.mark.integration
200: def test_docker_container_resource_usage():
201:     &quot;&quot;&quot;Test: Check Docker container resource consumption
202:     REAL-DATA: Monitors actual Docker container resource usage.
203:     &quot;&quot;&quot;
204: 
205:     containers = [&quot;dsp-mcp-ragdocs&quot;, &quot;dsp-mcp-search&quot;, &quot;dsp-qdrant&quot;]
206: 
207:     for container_name in containers:
208:         # Get container stats
209:         result = subprocess.run(
210:             [&quot;docker&quot;, &quot;stats&quot;, container_name, &quot;--no-stream&quot;, &quot;--format&quot;,
211:              &quot;table {{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\\t{{.MemPerc}}&quot;],
212:             capture_output=True,
213:             text=True,
214:             timeout=10
215:         )
216: 
217:         assert result.returncode == 0, f&quot;Failed to get stats for {container_name}: {result.stderr}&quot;
218:         assert container_name in result.stdout, f&quot;Container {container_name} not found in stats&quot;
219: 
220:         # Parse basic stats (basic validation - not strict limits)
221:         lines = result.stdout.strip().split(&apos;\n&apos;)
222:         if len(lines) &gt; 1:  # Header + data line
223:             stats_line = lines[1]
224:             print(f&quot;‚úì {container_name} resource usage: {stats_line}&quot;)
225: 
226: 
227: @pytest.mark.integration
228: def test_claudable_config_mcp_endpoints():
229:     &quot;&quot;&quot;Test: Validate Claudable configuration matches actual running endpoints
230:     REAL-DATA: Cross-references config file with actual running Docker services.
231:     &quot;&quot;&quot;
232: 
233:     # Read Claudable config
234:     config_path = PROJECT_ROOT / &quot;claudable&quot; / &quot;config.json&quot;
235:     with open(config_path, &quot;r&quot;) as f:
236:         config = json.load(f)
237: 
238:     # Test each MCP endpoint from config
239:     mcp_servers = config.get(&quot;mcp_servers&quot;, {})
240: 
241:     for service_name, endpoint_url in mcp_servers.items():
242:         # Parse URL to test endpoint
243:         if endpoint_url.startswith(&quot;http://localhost:&quot;):
244:             port = endpoint_url.split(&quot;:&quot;)[-1]
245: 
246:             # Test endpoint accessibility
247:             result = subprocess.run(
248:                 [&quot;curl&quot;, &quot;-s&quot;, &quot;-f&quot;, f&quot;http://localhost:{port}/&quot;, &quot;-m&quot;, &quot;5&quot;],
249:                 capture_output=True,
250:                 text=True
251:             )
252: 
253:             assert result.returncode == 0, f&quot;Claudable config endpoint {service_name} ({endpoint_url}) is not accessible&quot;
254: 
255:             # Validate response is JSON
256:             try:
257:                 response_data = json.loads(result.stdout)
258:                 assert &quot;service&quot; in response_data or &quot;status&quot; in response_data, \
259:                     f&quot;Invalid response format from {service_name}&quot;
260:             except json.JSONDecodeError:
261:                 assert False, f&quot;Non-JSON response from {service_name}: {result.stdout}&quot;
262: 
263:             print(f&quot;‚úì Claudable config endpoint {service_name} accessible: {endpoint_url}&quot;)
264: 
265: 
266: async def main():
267:     &quot;&quot;&quot;Run all integration tests&quot;&quot;&quot;
268:     print(&quot;=&quot; * 60)
269:     print(&quot;END-TO-END INTEGRATION TESTS&quot;)
270:     print(&quot;=&quot; * 60)
271: 
272:     # Test 1: RAG pipeline
273:     await test_complete_rag_search_pipeline()
274: 
275:     # Test 2: Web search pipeline
276:     await test_complete_web_search_pipeline()
277: 
278:     # Test 3: Hybrid agent simulation
279:     await test_hybrid_agent_query_simulation()
280: 
281:     # Test 4: Concurrent load
282:     await test_concurrent_service_load()
283: 
284:     # Test 5: Resource monitoring
285:     test_docker_container_resource_usage()
286: 
287:     # Test 6: Config validation
288:     test_claudable_config_mcp_endpoints()
289: 
290:     print(&quot;=&quot; * 60)
291:     print(&quot;ALL INTEGRATION TESTS PASSED&quot;)
292:     print(&quot;=&quot; * 60)
293: 
294: 
295: if __name__ == &quot;__main__&quot;:
296:     asyncio.run(main())</file><file path="tools/docker-enum.sh"> 1: #!/bin/bash
 2: # Docker MCP Server Enumeration - Clean and simple
 3: 
 4: echo &quot;üîç MCP Servers in Docker Desktop:&quot;
 5: echo &quot;================================&quot;
 6: 
 7: # List running MCP containers with their ports
 8: docker ps --filter &quot;name=mcp&quot; --format &quot;table {{.Names}}\t{{.Ports}}\t{{.Status}}&quot; 2&gt;/dev/null || echo &quot;No MCP servers running&quot;
 9: 
10: echo &quot;&quot;
11: echo &quot;üìä All DSP containers:&quot;
12: docker ps --filter &quot;name=dsp&quot; --format &quot;table {{.Names}}\t{{.Ports}}\t{{.Status}}&quot; 2&gt;/dev/null
13: 
14: echo &quot;&quot;
15: echo &quot;üåê Exposed endpoints:&quot;
16: echo &quot;  Qdrant:     http://localhost:6333/dashboard&quot;
17: echo &quot;  MCP RAG:    http://localhost:3001&quot;
18: echo &quot;  MCP Docs:   http://localhost:3002&quot;
19: echo &quot;  MCP Search: http://localhost:3003&quot;</file><file path="build-verification.sh"> 1: #!/bin/bash
 2: 
 3: echo &quot;=========================================&quot;
 4: echo &quot;DSP BUILD VERIFICATION SCRIPT&quot;
 5: echo &quot;=========================================&quot;
 6: echo
 7: 
 8: # Check Poetry is working
 9: echo &quot;1. Poetry Configuration Check:&quot;
10: if poetry check; then
11:     echo &quot;‚úÖ Poetry configuration valid&quot;
12: else
13:     echo &quot;‚ùå Poetry configuration failed&quot;
14:     exit 1
15: fi
16: echo
17: 
18: # Check Docker containers
19: echo &quot;2. Docker Container Status:&quot;
20: if docker ps --filter &quot;name=dsp-&quot; --format &quot;{{.Names}}&quot; | grep -q &quot;dsp-&quot;; then
21:     echo &quot;‚úÖ Docker containers running:&quot;
22:     docker ps --filter &quot;name=dsp-&quot; --format &quot;table {{.Names}}\t{{.Status}}\t{{.Ports}}&quot;
23: else
24:     echo &quot;‚ùå Docker containers not running&quot;
25:     exit 1
26: fi
27: echo
28: 
29: # Run critical tests
30: echo &quot;3. Critical Function Tests:&quot;
31: if poetry run python tests/test_critical.py; then
32:     echo &quot;‚úÖ All critical tests passed&quot;
33: else
34:     echo &quot;‚ùå Critical tests failed&quot;
35:     exit 1
36: fi
37: echo
38: 
39: # Run pytest
40: echo &quot;4. pytest Framework Test:&quot;
41: if poetry run pytest tests/test_critical.py -v; then
42:     echo &quot;‚úÖ pytest framework working&quot;
43: else
44:     echo &quot;‚ùå pytest framework failed&quot;
45:     exit 1
46: fi
47: echo
48: 
49: # Test HTTP endpoints
50: echo &quot;5. HTTP Endpoint Verification:&quot;
51: echo &quot;RAG Server Health:&quot;
52: curl -s http://localhost:3002/health || echo &quot;‚ùå RAG endpoint failed&quot;
53: echo
54: echo &quot;Search Server Health:&quot;
55: curl -s http://localhost:3004/health || echo &quot;‚ùå Search endpoint failed&quot;
56: echo
57: echo &quot;Qdrant Server Health:&quot;
58: curl -s http://localhost:6333/ | grep -q &quot;qdrant&quot; &amp;&amp; echo &quot;‚úÖ Qdrant accessible&quot; || echo &quot;‚ùå Qdrant failed&quot;
59: echo
60: 
61: echo &quot;=========================================&quot;
62: echo &quot;BUILD VERIFICATION COMPLETE&quot;
63: echo &quot;System ready for production build&quot;
64: echo &quot;=========================================&quot;</file><file path="DEPLOYMENT.md">  1: # DSP Documentation Agent - MVP Deployment Guide
  2: 
  3: ## Overview
  4: 
  5: The DSP Documentation Agent combines Dyson Sphere Program game mechanics with real physics speculation, serving content creators and sci-fi writers. This guide covers complete MVP deployment for the single-user setup.
  6: 
  7: ## Architecture
  8: 
  9: **Local Docker Desktop Setup:**
 10: - **Qdrant Vector Database**: Port 6333 (document storage)
 11: - **MCP RAG Server**: Port 3002 (DSP documentation search)
 12: - **MCP Search Server**: Port 3004 (physics research search)
 13: - **Claudable Interface**: Port 3001 (user interaction)
 14: 
 15: **Communication Flow:**
 16: ```
 17: User ‚Üí Claudable (3001) ‚Üí HTTP ‚Üí MCP Servers (3002/3004) ‚Üí Claude API ‚Üí Response
 18: ```
 19: 
 20: ## Prerequisites
 21: 
 22: 1. **Docker Desktop** - Running and accessible
 23: 2. **Node.js 18+** - For Claudable interface
 24: 3. **API Keys** - OpenAI and Brave Search (configured in `docker/.env`)
 25: 4. **Git repositories**:
 26:    - This repository: `/path/to/dyson-sphere-facts/`
 27:    - Claudable: `/path/to/Claudable/` (optional but recommended)
 28: 
 29: ## Quick Start (Recommended)
 30: 
 31: ### One-Command Deployment
 32: 
 33: ```bash
 34: ./start.sh
 35: ```
 36: 
 37: This script will:
 38: - Check all prerequisites
 39: - Start Docker infrastructure (Qdrant + MCP servers)
 40: - Wait for services to be ready
 41: - Validate all endpoints
 42: - Start Claudable interface (if available)
 43: - Display service status and example commands
 44: 
 45: ### Example Output
 46: 
 47: ```
 48: üöÄ DSP Documentation Agent - MVP Deployment
 49: =============================================
 50: 
 51: üìã Prerequisites Check
 52: ----------------------
 53: ‚úÖ Docker found
 54: ‚úÖ Docker daemon running
 55: ‚úÖ Project structure valid
 56: ‚úÖ Environment configuration found
 57: 
 58: üê≥ Starting Docker Infrastructure
 59: --------------------------------
 60: [+] Running 4/4
 61:  ‚úÖ Network dsp-network        Created
 62:  ‚úÖ Container dsp-qdrant       Started
 63:  ‚úÖ Container dsp-mcp-ragdocs  Started
 64:  ‚úÖ Container dsp-mcp-search   Started
 65: 
 66: ‚è≥ Waiting for Services
 67: ----------------------
 68: Waiting for Qdrant Database (http://localhost:6333/)... ‚úÖ Ready
 69: Waiting for RAG Server (http://localhost:3002/health)... ‚úÖ Ready
 70: Waiting for Search Server (http://localhost:3004/health)... ‚úÖ Ready
 71: 
 72: üéØ DSP Documentation Agent - Deployment Complete
 73: =================================================
 74: 
 75: üìä Service Status:
 76:   ‚Ä¢ Qdrant Database:    http://localhost:6333/
 77:   ‚Ä¢ RAG Server:         http://localhost:3002/health
 78:   ‚Ä¢ Search Server:      http://localhost:3004/health
 79:   ‚Ä¢ Claudable Interface: http://localhost:3001
 80: 
 81: ‚úÖ System ready for DSP documentation queries!
 82: ```
 83: 
 84: ## Testing the Deployment
 85: 
 86: ### Health Check
 87: ```bash
 88: curl http://localhost:3001/health
 89: ```
 90: 
 91: ### Example Queries
 92: 
 93: **Game Mechanics (60%):**
 94: ```bash
 95: curl -X POST http://localhost:3001/chat \
 96:   -H &quot;Content-Type: application/json&quot; \
 97:   -d &apos;{&quot;message&quot;:&quot;How do Critical Photons work in DSP?&quot;}&apos;
 98: ```
 99: 
100: **Physics Speculation (40%):**
101: ```bash
102: curl -X POST http://localhost:3001/chat \
103:   -H &quot;Content-Type: application/json&quot; \
104:   -d &apos;{&quot;message&quot;:&quot;Could we actually build a real Dyson sphere?&quot;}&apos;
105: ```
106: 
107: **Hybrid Questions:**
108: ```bash
109: curl -X POST http://localhost:3001/chat \
110:   -H &quot;Content-Type: application/json&quot; \
111:   -d &apos;{&quot;message&quot;:&quot;Compare DSP antimatter production to real physics research&quot;}&apos;
112: ```
113: 
114: ## Recovery &amp; Maintenance
115: 
116: ### Restart Options
117: 
118: ```bash
119: # Quick MCP server restart
120: ./restart.sh
121: 
122: # Restart everything (MCP + Claudable)
123: ./restart.sh --all
124: 
125: # Restart only Claudable interface
126: ./restart.sh --claudable
127: 
128: # Force rebuild Docker containers
129: ./restart.sh --rebuild
130: 
131: # Get help
132: ./restart.sh --help
133: ```
134: 
135: ### Manual Container Management
136: 
137: ```bash
138: # Check container status
139: docker ps --filter &quot;name=dsp&quot;
140: 
141: # View logs
142: docker-compose -f docker/docker-compose.yml logs -f [service_name]
143: 
144: # Stop all services
145: docker-compose -f docker/docker-compose.yml down
146: 
147: # Start all services
148: docker-compose -f docker/docker-compose.yml up -d
149: ```
150: 
151: ## Configuration
152: 
153: ### Environment Variables
154: 
155: **Required in `docker/.env`:**
156: ```env
157: OPENAI_API_KEY=sk-your-openai-key
158: BRAVE_API_KEY=your-brave-search-key
159: ```
160: 
161: ### Claudable Configuration
162: 
163: **Location:** `claudable/config.json`
164: 
165: Key settings:
166: - MCP server endpoints (localhost:3002, 3004)
167: - Claude API model selection
168: - Port configuration (3001)
169: 
170: ### Agent Personality
171: 
172: The agent follows a **60/40 balance**:
173: - **60% DSP Game Mechanics**: Items, recipes, strategies, technologies
174: - **40% Physics Speculation**: Real research, engineering challenges, comparisons
175: 
176: **Tone**: Fun and engaging (science communicator, not academic)
177: 
178: ## File Structure
179: 
180: ```
181: dyson-sphere-facts/
182: ‚îú‚îÄ‚îÄ start.sh                    # üöÄ Main deployment script
183: ‚îú‚îÄ‚îÄ restart.sh                  # üîÑ Recovery/restart script
184: ‚îú‚îÄ‚îÄ DEPLOYMENT.md               # üìñ This guide
185: ‚îú‚îÄ‚îÄ docker/
186: ‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml      # üê≥ Container definitions
187: ‚îÇ   ‚îú‚îÄ‚îÄ .env                    # üîë API keys
188: ‚îÇ   ‚îî‚îÄ‚îÄ validate-infrastructure.sh # ‚úÖ Health validation
189: ‚îú‚îÄ‚îÄ claudable/                  # ü§ñ Agent interface
190: ‚îî‚îÄ‚îÄ docs/                       # üìö DSP documentation
191: ```
192: 
193: ## Port Allocation
194: 
195: | Service | Port | Purpose |
196: |---------|------|---------|
197: | Claudable | 3001 | User interface &amp; API |
198: | MCP RAG | 3002 | DSP documentation search |
199: | MCP Search | 3004 | Physics research search |
200: | Qdrant | 6333 | Vector database |
201: | Qdrant Admin | 6334 | Database admin interface |
202: 
203: ## Validation
204: 
205: ### Automated Validation
206: ```bash
207: ./docker/validate-infrastructure.sh
208: ```
209: 
210: ### Manual Health Checks
211: ```bash
212: # Test each endpoint
213: curl http://localhost:6333/               # Qdrant
214: curl http://localhost:3002/health         # RAG server
215: curl http://localhost:3004/health         # Search server
216: curl http://localhost:3001/health         # Claudable
217: 
218: # Test search functionality
219: curl &quot;http://localhost:3002/search?q=dyson+sphere&quot;
220: curl &quot;http://localhost:3004/search?q=physics&quot;
221: ```
222: 
223: ## Success Indicators
224: 
225: ‚úÖ **Deployment Successful:**
226: - All 3 Docker containers running
227: - All health endpoints return 200 OK
228: - Validation script passes completely
229: - Example queries return coherent responses
230: - Claudable interface accessible in browser
231: 
232: ## Next Steps
233: 
234: 1. **Load DSP Documentation**: Use documentation scraping to populate the RAG system
235: 2. **Test Agent Responses**: Verify 60/40 balance and tone consistency
236: 3. **Monitor Performance**: Check response times and resource usage
237: 4. **Backup Configuration**: Save working `.env` and config files
238: 
239: ## Future Enhancements
240: 
241: - **[ORCHESTRATION]**: Automated health monitoring and recovery
242: - **Documentation Pipeline**: Automated DSP wiki scraping and updates
243: - **Analytics**: Query pattern analysis and response quality metrics
244: - **Scaling**: Multi-user deployment considerations
245: 
246: ---
247: 
248: **MVP Focus**: Single-user deployment with maximum simplicity and reliability.
249: **Userbase**: 1 (you)
250: **Maintenance**: Minimal - restart scripts handle most issues</file><file path="EXAMPLE_QUERIES.md">  1: # DSP Documentation Agent - Example Queries
  2: 
  3: This guide demonstrates the agent&apos;s capabilities with test queries that showcase the **60/40 balance** (60% game mechanics, 40% physics speculation) and the fun, engaging tone.
  4: 
  5: ## Query Categories
  6: 
  7: ### üéÆ Game Mechanics (60% Focus)
  8: 
  9: #### Basic DSP Items &amp; Recipes
 10: ```bash
 11: # Critical Photons - Core game mechanic
 12: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 13:   -d &apos;{&quot;message&quot;:&quot;How do Critical Photons work in DSP? What recipes use them?&quot;}&apos;
 14: 
 15: # Antimatter Production
 16: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 17:   -d &apos;{&quot;message&quot;:&quot;What is the most efficient antimatter production setup in DSP?&quot;}&apos;
 18: 
 19: # Solar Sails
 20: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 21:   -d &apos;{&quot;message&quot;:&quot;How many Solar Sails do I need for a Dyson Sphere around a K-class star?&quot;}&apos;
 22: 
 23: # Ray Receivers
 24: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 25:   -d &apos;{&quot;message&quot;:&quot;What is the optimal Ray Receiver layout for maximum power efficiency?&quot;}&apos;
 26: ```
 27: 
 28: #### Advanced Game Strategy
 29: ```bash
 30: # Logistics &amp; Scaling
 31: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 32:   -d &apos;{&quot;message&quot;:&quot;How do I scale up my factory to produce 1000 Science Matrix per minute?&quot;}&apos;
 33: 
 34: # Multi-planet Operations
 35: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 36:   -d &apos;{&quot;message&quot;:&quot;What is the best strategy for automating multiple planets with Interstellar Logistics?&quot;}&apos;
 37: 
 38: # Resource Optimization
 39: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 40:   -d &apos;{&quot;message&quot;:&quot;Which planets should I prioritize for mining rare resources like Kimberlite and Spiniform?&quot;}&apos;
 41: ```
 42: 
 43: ### üî¨ Physics Speculation (40% Focus)
 44: 
 45: #### Real Dyson Sphere Engineering
 46: ```bash
 47: # Feasibility Questions
 48: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 49:   -d &apos;{&quot;message&quot;:&quot;Could we actually build a real Dyson sphere? What are the engineering challenges?&quot;}&apos;
 50: 
 51: # Material Science
 52: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 53:   -d &apos;{&quot;message&quot;:&quot;What materials would we need for a real Dyson sphere? Are there any realistic alternatives to the game solid structures?&quot;}&apos;
 54: 
 55: # Energy Collection
 56: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 57:   -d &apos;{&quot;message&quot;:&quot;How would we actually collect and transmit energy from a real Dyson sphere to Earth?&quot;}&apos;
 58: ```
 59: 
 60: #### Theoretical Physics Research
 61: ```bash
 62: # Antimatter Physics
 63: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 64:   -d &apos;{&quot;message&quot;:&quot;What is the current state of real antimatter research? How close are we to the production methods shown in DSP?&quot;}&apos;
 65: 
 66: # Stellar Engineering
 67: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 68:   -d &apos;{&quot;message&quot;:&quot;Are there any recent research papers on stellar engineering or megastructures like Dyson spheres?&quot;}&apos;
 69: 
 70: # Space-based Manufacturing
 71: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 72:   -d &apos;{&quot;message&quot;:&quot;What are the latest developments in space-based manufacturing and asteroid mining?&quot;}&apos;
 73: ```
 74: 
 75: ### üîÄ Hybrid Questions (Game + Physics)
 76: 
 77: #### Comparative Analysis
 78: ```bash
 79: # Game vs Reality
 80: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 81:   -d &apos;{&quot;message&quot;:&quot;Compare DSP antimatter production to real physics - what does the game get right and wrong?&quot;}&apos;
 82: 
 83: # Technology Assessment
 84: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 85:   -d &apos;{&quot;message&quot;:&quot;How realistic are the technologies in DSP? Which ones might be possible and which are pure sci-fi?&quot;}&apos;
 86: 
 87: # Scale Comparisons
 88: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 89:   -d &apos;{&quot;message&quot;:&quot;The game lets you build a Dyson sphere in hours - how long would it actually take to build one?&quot;}&apos;
 90: 
 91: # Energy Efficiency
 92: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
 93:   -d &apos;{&quot;message&quot;:&quot;Does DSP solar sail efficiency match real physics? How much energy could we actually capture?&quot;}&apos;
 94: ```
 95: 
 96: ## Testing Different Agent Personalities
 97: 
 98: ### üéØ Testing Tone Consistency
 99: 
100: #### Should Sound Fun &amp; Engaging (NOT Academic)
101: ```bash
102: # Good response tone examples:
103: # &quot;Oh, Critical Photons! These little energy packets are absolutely fascinating...&quot;
104: # &quot;Real Dyson spheres? Now we&apos;re talking about some seriously epic engineering!&quot;
105: # &quot;DSP makes antimatter look easy, but the real physics is mind-blowing...&quot;
106: 
107: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
108:   -d &apos;{&quot;message&quot;:&quot;Tell me about Critical Photons - make it exciting!&quot;}&apos;
109: ```
110: 
111: #### Testing Knowledge Balance
112: ```bash
113: # Should demonstrate both game knowledge AND real physics
114: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
115:   -d &apos;{&quot;message&quot;:&quot;Explain how DSP Dyson swarms work, and how that compares to real space engineering proposals&quot;}&apos;
116: 
117: # Should prioritize game mechanics but include physics context
118: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
119:   -d &apos;{&quot;message&quot;:&quot;What is the best Critical Photon setup in DSP?&quot;}&apos;
120: ```
121: 
122: ## Content Creator &amp; Sci-Fi Writer Use Cases
123: 
124: ### üìπ YouTube Content Ideas
125: ```bash
126: # Video concept queries
127: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
128:   -d &apos;{&quot;message&quot;:&quot;Give me 5 video ideas comparing DSP technologies to real physics for my YouTube channel&quot;}&apos;
129: 
130: # Fact-checking assistance
131: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
132:   -d &apos;{&quot;message&quot;:&quot;I want to make a video about real Dyson spheres - what are the most recent research developments I should cover?&quot;}&apos;
133: ```
134: 
135: ### ‚úçÔ∏è Sci-Fi Writing Assistance
136: ```bash
137: # Worldbuilding help
138: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
139:   -d &apos;{&quot;message&quot;:&quot;I am writing a sci-fi story about humanity building their first Dyson sphere - what realistic challenges should they face?&quot;}&apos;
140: 
141: # Technical authenticity
142: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
143:   -d &apos;{&quot;message&quot;:&quot;What would be realistic antimatter production methods for a sci-fi story set 200 years in the future?&quot;}&apos;
144: ```
145: 
146: ## Performance Testing Queries
147: 
148: ### üèÉ‚Äç‚ôÇÔ∏è Response Speed Tests
149: ```bash
150: # Simple game query (should be fast - RAG only)
151: time curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
152:   -d &apos;{&quot;message&quot;:&quot;What is Iron Ore used for in DSP?&quot;}&apos;
153: 
154: # Physics query (slower - web search required)
155: time curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
156:   -d &apos;{&quot;message&quot;:&quot;Latest Dyson sphere research papers&quot;}&apos;
157: 
158: # Complex hybrid query (slowest - both sources)
159: time curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
160:   -d &apos;{&quot;message&quot;:&quot;How realistic are DSP power transmission methods compared to current space solar power research?&quot;}&apos;
161: ```
162: 
163: ### üîç Search Quality Tests
164: ```bash
165: # Test RAG precision
166: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
167:   -d &apos;{&quot;message&quot;:&quot;White Science Matrix recipe requirements&quot;}&apos;
168: 
169: # Test web search current events
170: curl -X POST http://localhost:3001/chat -H &quot;Content-Type: application/json&quot; \
171:   -d &apos;{&quot;message&quot;:&quot;NASA Dyson sphere news 2024&quot;}&apos;
172: ```
173: 
174: ## Expected Response Patterns
175: 
176: ### ‚úÖ Good Responses Should Include:
177: 
178: 1. **Game Mechanics Focus (60%)**:
179:    - Specific DSP item names, recipes, numbers
180:    - Strategic advice and optimization tips
181:    - Reference to game mechanics and systems
182: 
183: 2. **Physics Context (40%)**:
184:    - Real scientific concepts and research
185:    - Current developments in space technology
186:    - Comparative analysis with game mechanics
187: 
188: 3. **Engaging Tone**:
189:    - Enthusiasm without being academic
190:    - &quot;Science communicator who loves gaming&quot; style
191:    - Bridge between gaming and real science
192: 
193: 4. **Source Attribution**:
194:    - Clear indication when using RAG vs web search
195:    - Response metadata showing source counts
196: 
197: ### ‚ùå Poor Responses Would Include:
198: 
199: - Purely academic tone
200: - Only game info without physics context (or vice versa)
201: - Incorrect game mechanics or outdated physics
202: - No enthusiasm or engagement
203: - Missing source attribution
204: 
205: ## Automated Testing Script
206: 
207: Save this as `test-agent-queries.sh`:
208: 
209: ```bash
210: #!/bin/bash
211: 
212: echo &quot;üß™ DSP Agent Query Testing Suite&quot;
213: echo &quot;================================&quot;
214: 
215: # Test basic functionality
216: echo &quot;Testing basic health...&quot;
217: curl -s http://localhost:3001/health | jq .
218: 
219: # Test game mechanics query
220: echo -e &quot;\nüéÆ Testing game mechanics query...&quot;
221: curl -s -X POST http://localhost:3001/chat \
222:   -H &quot;Content-Type: application/json&quot; \
223:   -d &apos;{&quot;message&quot;:&quot;Critical Photon recipe&quot;}&apos; | jq .
224: 
225: # Test physics query
226: echo -e &quot;\nüî¨ Testing physics query...&quot;
227: curl -s -X POST http://localhost:3001/chat \
228:   -H &quot;Content-Type: application/json&quot; \
229:   -d &apos;{&quot;message&quot;:&quot;real Dyson sphere engineering&quot;}&apos; | jq .
230: 
231: # Test hybrid query
232: echo -e &quot;\nüîÄ Testing hybrid query...&quot;
233: curl -s -X POST http://localhost:3001/chat \
234:   -H &quot;Content-Type: application/json&quot; \
235:   -d &apos;{&quot;message&quot;:&quot;Compare DSP solar sails to real physics&quot;}&apos; | jq .
236: 
237: echo -e &quot;\n‚úÖ Testing complete!&quot;
238: ```
239: 
240: Run with:
241: ```bash
242: chmod +x test-agent-queries.sh
243: ./test-agent-queries.sh
244: ```</file><file path="LICENSE"> 1: MIT License
 2: 
 3: Copyright (c) 2025 Laura Lopez
 4: 
 5: Permission is hereby granted, free of charge, to any person obtaining a copy
 6: of this software and associated documentation files (the &quot;Software&quot;), to deal
 7: in the Software without restriction, including without limitation the rights
 8: to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 9: copies of the Software, and to permit persons to whom the Software is
10: furnished to do so, subject to the following conditions:
11: 
12: The above copyright notice and this permission notice shall be included in all
13: copies or substantial portions of the Software.
14: 
15: THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
16: IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
17: FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
18: AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
19: LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
20: OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
21: SOFTWARE.</file><file path="ORCHESTRATION_ROADMAP.md">  1: # DSP Documentation Agent - Future Orchestration &amp; Monitoring Architecture
  2: 
  3: ## Overview
  4: 
  5: This document outlines the future enhancement roadmap for automated health monitoring, workflow orchestration, and system reliability improvements for the DSP Documentation Agent MVP.
  6: 
  7: ## Current State vs Future Vision
  8: 
  9: ### Current MVP (Manual)
 10: - **Deployment**: `./start.sh` (manual execution)
 11: - **Recovery**: `./restart.sh` with manual intervention
 12: - **Monitoring**: `./docker/validate-infrastructure.sh` (on-demand)
 13: - **Health Checks**: Manual endpoint testing
 14: - **Scaling**: Single-user, localhost-only
 15: 
 16: ### Future Orchestration (Automated)
 17: - **Deployment**: Self-healing, automated recovery
 18: - **Monitoring**: Continuous health monitoring with alerts
 19: - **Scaling**: Multi-user support with load balancing
 20: - **Analytics**: Performance tracking and optimization
 21: - **Workflow Management**: Visual pipeline configuration
 22: 
 23: ## Phase 1: Automated Health Monitoring
 24: 
 25: ### 1.1 Health Check Service
 26: 
 27: **Implementation Approach:**
 28: ```javascript
 29: // health-monitor.js - Standalone monitoring service
 30: const monitor = {
 31:   services: [
 32:     { name: &apos;qdrant&apos;, url: &apos;http://localhost:6333/&apos;, critical: true },
 33:     { name: &apos;rag&apos;, url: &apos;http://localhost:3002/health&apos;, critical: true },
 34:     { name: &apos;search&apos;, url: &apos;http://localhost:3004/health&apos;, critical: true },
 35:     { name: &apos;claudable&apos;, url: &apos;http://localhost:3001/health&apos;, critical: false }
 36:   ],
 37: 
 38:   async checkHealth() {
 39:     // Parallel health checks with timeout
 40:     // Automatic restart on critical service failure
 41:     // Exponential backoff on persistent failures
 42:   }
 43: }
 44: ```
 45: 
 46: **Features:**
 47: - **Continuous Monitoring**: 30-second health check intervals
 48: - **Auto-Recovery**: Automatic restart of failed services
 49: - **Alert System**: Email/webhook notifications for persistent failures
 50: - **Metrics Collection**: Response times, error rates, uptime statistics
 51: - **Graceful Degradation**: Continue operation with non-critical service failures
 52: 
 53: ### 1.2 Infrastructure Monitoring
 54: 
 55: **Docker Container Metrics:**
 56: ```bash
 57: # Container health monitoring
 58: docker stats --format &quot;{{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}&quot;
 59: 
 60: # Automated resource alerting
 61: if memory_usage &gt; 80%; then
 62:   restart_container_with_backoff
 63: fi
 64: ```
 65: 
 66: **Network Monitoring:**
 67: - Port availability checking
 68: - Inter-service communication validation
 69: - API endpoint response time tracking
 70: - Request/response payload validation
 71: 
 72: ### 1.3 Monitoring Dashboard
 73: 
 74: **Web-based Dashboard (Port 3005):**
 75: - Real-time service status visualization
 76: - Historical performance metrics
 77: - Log aggregation and search
 78: - Manual service control buttons
 79: - System configuration management
 80: 
 81: ## Phase 2: Workflow Orchestration Engine
 82: 
 83: ### 2.1 Pipeline Definition
 84: 
 85: **Configuration-Driven Workflows:**
 86: ```yaml
 87: # dsp-agent-workflow.yml
 88: workflows:
 89:   startup:
 90:     steps:
 91:       - name: &quot;Check Prerequisites&quot;
 92:         action: &quot;validate-environment&quot;
 93:         timeout: &quot;30s&quot;
 94: 
 95:       - name: &quot;Start Infrastructure&quot;
 96:         action: &quot;docker-compose-up&quot;
 97:         depends_on: [&quot;Check Prerequisites&quot;]
 98: 
 99:       - name: &quot;Wait for Services&quot;
100:         action: &quot;wait-for-endpoints&quot;
101:         timeout: &quot;120s&quot;
102: 
103:       - name: &quot;Run Validation&quot;
104:         action: &quot;run-health-checks&quot;
105: 
106:   recovery:
107:     triggers: [&quot;service-failure&quot;, &quot;health-check-failure&quot;]
108:     steps:
109:       - name: &quot;Diagnose Issue&quot;
110:         action: &quot;collect-logs&quot;
111: 
112:       - name: &quot;Attempt Restart&quot;
113:         action: &quot;restart-service&quot;
114:         retries: 3
115: 
116:       - name: &quot;Escalate if Failed&quot;
117:         action: &quot;send-alert&quot;
118:         condition: &quot;retries-exhausted&quot;
119: ```
120: 
121: ### 2.2 Orchestration Engine
122: 
123: **Core Components:**
124: - **Workflow Engine**: Execute YAML-defined pipelines
125: - **Event System**: Trigger workflows on system events
126: - **State Management**: Track workflow execution state
127: - **Retry Logic**: Configurable retry policies with exponential backoff
128: - **Parallel Execution**: Run non-dependent steps simultaneously
129: 
130: **API Interface:**
131: ```javascript
132: // Orchestration REST API
133: POST /workflows/startup          // Trigger startup workflow
134: POST /workflows/recovery         // Manual recovery trigger
135: GET  /workflows/{id}/status      // Check workflow status
136: POST /workflows/{id}/cancel      // Cancel running workflow
137: GET  /metrics/workflows          // Workflow execution statistics
138: ```
139: 
140: ## Phase 3: Advanced Reliability Features
141: 
142: ### 3.1 Circuit Breaker Pattern
143: 
144: **Service Protection:**
145: ```javascript
146: class CircuitBreaker {
147:   // Protect MCP services from cascading failures
148:   // Fallback to cached responses during outages
149:   // Automatic recovery testing
150: }
151: ```
152: 
153: **Implementation:**
154: - **Failure Threshold**: Open circuit after 5 consecutive failures
155: - **Recovery Testing**: Half-open state with single test request
156: - **Fallback Responses**: Cached or simplified responses during outages
157: - **Metrics**: Track circuit breaker state changes and recovery times
158: 
159: ### 3.2 Load Balancing &amp; Scaling
160: 
161: **Multi-Instance Support:**
162: ```yaml
163: # docker-compose-scaled.yml
164: services:
165:   mcp-ragdocs:
166:     deploy:
167:       replicas: 3
168:       placement:
169:         constraints: [node.role == worker]
170: 
171:   nginx-lb:
172:     image: nginx:alpine
173:     ports: [&quot;3002:80&quot;]
174:     volumes: [&quot;./nginx.conf:/etc/nginx/nginx.conf&quot;]
175: ```
176: 
177: **Features:**
178: - **Horizontal Scaling**: Multiple MCP server instances
179: - **Load Distribution**: Nginx-based request routing
180: - **Health-Aware Routing**: Remove unhealthy instances from rotation
181: - **Session Affinity**: Route users to consistent instances (if needed)
182: 
183: ### 3.3 Data Backup &amp; Recovery
184: 
185: **Automated Backup System:**
186: ```bash
187: #!/bin/bash
188: # backup-automation.sh
189: 
190: # Qdrant database backup
191: docker exec dsp-qdrant curl -X POST &quot;http://localhost:6333/collections/backup&quot;
192: 
193: # Configuration backup
194: tar -czf &quot;config-backup-$(date +%Y%m%d).tar.gz&quot; docker/.env claudable/config.json
195: 
196: # Automated S3/cloud backup
197: aws s3 cp backup.tar.gz s3://dsp-agent-backups/
198: ```
199: 
200: **Recovery Procedures:**
201: - **Point-in-Time Recovery**: Restore to specific backup timestamp
202: - **Configuration Rollback**: Revert to last known working configuration
203: - **Disaster Recovery**: Complete system rebuild from backups
204: - **Testing**: Regular backup integrity validation
205: 
206: ## Phase 4: Analytics &amp; Optimization
207: 
208: ### 4.1 Performance Monitoring
209: 
210: **Metrics Collection:**
211: - **Response Times**: Track API endpoint performance
212: - **Query Analysis**: Most common questions and response quality
213: - **Resource Utilization**: CPU, memory, storage trends
214: - **User Patterns**: Peak usage times and load distribution
215: 
216: **Analytics Dashboard:**
217: ```javascript
218: // Performance metrics API
219: GET /analytics/performance        // System performance metrics
220: GET /analytics/queries           // Query pattern analysis
221: GET /analytics/usage             // Usage statistics
222: GET /analytics/errors            // Error rate and types
223: ```
224: 
225: ### 4.2 Intelligent Caching
226: 
227: **Multi-Layer Caching:**
228: ```javascript
229: const cacheStrategy = {
230:   // L1: In-memory cache for frequent queries
231:   memory: new LRUCache({ maxSize: 1000 }),
232: 
233:   // L2: Redis cache for complex query results
234:   redis: new RedisCache({ ttl: 3600 }),
235: 
236:   // L3: Pre-computed responses for common questions
237:   precomputed: loadCommonQueries()
238: }
239: ```
240: 
241: **Cache Optimization:**
242: - **Query Similarity**: Cache similar question variations
243: - **Proactive Refresh**: Update cache before expiration
244: - **Cache Warming**: Pre-populate with common queries
245: - **Intelligent Invalidation**: Update cache when documentation changes
246: 
247: ### 4.3 Continuous Improvement
248: 
249: **Quality Monitoring:**
250: - **Response Quality Scoring**: Automated assessment of answer quality
251: - **User Feedback Integration**: Thumbs up/down on responses
252: - **A/B Testing**: Test different prompt strategies
253: - **Model Performance Tracking**: Compare different Claude model versions
254: 
255: ## Phase 5: Enterprise Features
256: 
257: ### 5.1 Multi-Tenant Architecture
258: 
259: **Isolation Strategy:**
260: ```yaml
261: # Multi-tenant deployment
262: tenants:
263:   content-creator-1:
264:     namespace: &quot;cc1&quot;
265:     qdrant-collection: &quot;cc1-docs&quot;
266:     resource-limits: { cpu: &quot;1&quot;, memory: &quot;2Gi&quot; }
267: 
268:   sci-fi-writer-1:
269:     namespace: &quot;sfw1&quot;
270:     qdrant-collection: &quot;sfw1-docs&quot;
271:     custom-prompts: [&quot;narrative-style&quot;, &quot;technical-accuracy&quot;]
272: ```
273: 
274: ### 5.2 API Management
275: 
276: **Rate Limiting &amp; Security:**
277: - **API Keys**: Per-tenant authentication
278: - **Rate Limiting**: Configurable request quotas
279: - **Usage Analytics**: Per-tenant resource consumption
280: - **Billing Integration**: Usage-based pricing model
281: 
282: ### 5.3 Custom Knowledge Bases
283: 
284: **Tenant-Specific Documentation:**
285: - **Custom RAG Collections**: Upload tenant-specific documents
286: - **Knowledge Base Management**: Web UI for document management
287: - **Version Control**: Track documentation changes over time
288: - **Search Scoping**: Limit searches to tenant&apos;s documents
289: 
290: ## Implementation Timeline
291: 
292: ### Phase 1 (Months 1-2): Health Monitoring
293: - [ ] Continuous health check service
294: - [ ] Auto-restart functionality
295: - [ ] Basic monitoring dashboard
296: - [ ] Email/webhook alerts
297: 
298: ### Phase 2 (Months 3-4): Workflow Orchestration
299: - [ ] YAML workflow definitions
300: - [ ] Orchestration engine implementation
301: - [ ] Event-driven automation
302: - [ ] Workflow monitoring UI
303: 
304: ### Phase 3 (Months 5-6): Advanced Reliability
305: - [ ] Circuit breaker implementation
306: - [ ] Load balancing setup
307: - [ ] Backup/recovery automation
308: - [ ] Disaster recovery testing
309: 
310: ### Phase 4 (Months 7-8): Analytics &amp; Optimization
311: - [ ] Performance monitoring system
312: - [ ] Intelligent caching layer
313: - [ ] Query pattern analysis
314: - [ ] Continuous improvement metrics
315: 
316: ### Phase 5 (Months 9-12): Enterprise Features
317: - [ ] Multi-tenant architecture
318: - [ ] API management platform
319: - [ ] Custom knowledge bases
320: - [ ] Enterprise deployment tools
321: 
322: ## Technical Architecture
323: 
324: ### Technology Stack
325: - **Orchestration**: Docker Swarm or Kubernetes
326: - **Monitoring**: Prometheus + Grafana
327: - **Logging**: ELK Stack (Elasticsearch, Logstash, Kibana)
328: - **Cache**: Redis Cluster
329: - **Load Balancer**: Nginx or HAProxy
330: - **Database**: PostgreSQL for metadata
331: - **Message Queue**: RabbitMQ or Apache Kafka
332: 
333: ### Infrastructure Requirements
334: - **Development**: Current Docker Desktop setup
335: - **Staging**: Multi-node Docker Swarm
336: - **Production**: Kubernetes cluster with auto-scaling
337: - **Monitoring**: Dedicated monitoring stack
338: - **Backup**: Cloud storage integration (AWS S3, GCS)
339: 
340: ## Migration Strategy
341: 
342: ### MVP ‚Üí Orchestrated Transition
343: 1. **Parallel Development**: Build orchestration alongside current MVP
344: 2. **Feature Flags**: Gradual rollout of new features
345: 3. **Backward Compatibility**: Keep manual scripts functional
346: 4. **Data Migration**: Seamless transition of existing data
347: 5. **User Training**: Documentation and training for new features
348: 
349: ### Risk Mitigation
350: - **Rollback Plan**: Quick revert to MVP if issues arise
351: - **Testing Environment**: Comprehensive testing before production deployment
352: - **Phased Rollout**: Gradual feature activation
353: - **Monitoring**: Extensive monitoring during transition
354: 
355: ---
356: 
357: **Summary**: This roadmap transforms the DSP Documentation Agent from a single-user MVP into a robust, scalable, enterprise-ready platform while maintaining the simplicity and reliability that makes the current system successful.</file><file path="restart.sh">  1: #!/bin/bash
  2: 
  3: # DSP Documentation Agent - Recovery &amp; Restart Script
  4: # Handles MCP server recovery, Docker container management, and system health restoration
  5: 
  6: set -e
  7: 
  8: # Color codes for output
  9: RED=&apos;\033[0;31m&apos;
 10: GREEN=&apos;\033[0;32m&apos;
 11: YELLOW=&apos;\033[1;33m&apos;
 12: BLUE=&apos;\033[0;34m&apos;
 13: CYAN=&apos;\033[0;36m&apos;
 14: NC=&apos;\033[0m&apos; # No Color
 15: 
 16: echo -e &quot;${BLUE}üîÑ DSP Documentation Agent - System Recovery${NC}&quot;
 17: echo &quot;==============================================&quot;
 18: 
 19: # Function to check if command exists
 20: command_exists() {
 21:     command -v &quot;$1&quot; &gt;/dev/null 2&gt;&amp;1
 22: }
 23: 
 24: # Function to wait for endpoint with timeout
 25: wait_for_endpoint() {
 26:     local url=&quot;$1&quot;
 27:     local service=&quot;$2&quot;
 28:     local max_attempts=15
 29:     local attempt=1
 30: 
 31:     echo -n &quot;Waiting for $service ($url)...&quot;
 32:     while [ $attempt -le $max_attempts ]; do
 33:         if curl -s -f &quot;$url&quot; &gt;/dev/null 2&gt;&amp;1; then
 34:             echo -e &quot; ${GREEN}‚úÖ Ready${NC}&quot;
 35:             return 0
 36:         fi
 37:         echo -n &quot;.&quot;
 38:         sleep 2
 39:         attempt=$((attempt + 1))
 40:     done
 41: 
 42:     echo -e &quot; ${RED}‚ùå Timeout after ${max_attempts} attempts${NC}&quot;
 43:     return 1
 44: }
 45: 
 46: # Function to check container health
 47: check_container_health() {
 48:     local container_name=&quot;$1&quot;
 49: 
 50:     if ! docker ps --format &quot;{{.Names}}&quot; | grep -q &quot;^${container_name}$&quot;; then
 51:         echo -e &quot;${RED}‚ùå Container $container_name not running${NC}&quot;
 52:         return 1
 53:     fi
 54: 
 55:     # Check if container is healthy (not restarting)
 56:     local status=$(docker inspect --format &quot;{{.State.Status}}&quot; &quot;$container_name&quot; 2&gt;/dev/null)
 57:     if [ &quot;$status&quot; != &quot;running&quot; ]; then
 58:         echo -e &quot;${RED}‚ùå Container $container_name status: $status${NC}&quot;
 59:         return 1
 60:     fi
 61: 
 62:     echo -e &quot;${GREEN}‚úÖ Container $container_name healthy${NC}&quot;
 63:     return 0
 64: }
 65: 
 66: # Function to restart single container
 67: restart_container() {
 68:     local container_name=&quot;$1&quot;
 69:     echo -e &quot;${YELLOW}üîÑ Restarting $container_name...${NC}&quot;
 70: 
 71:     if docker ps -a --format &quot;{{.Names}}&quot; | grep -q &quot;^${container_name}$&quot;; then
 72:         docker restart &quot;$container_name&quot;
 73:         sleep 3
 74:     else
 75:         echo -e &quot;${RED}‚ùå Container $container_name does not exist${NC}&quot;
 76:         return 1
 77:     fi
 78: }
 79: 
 80: # Parse command line arguments
 81: RESTART_ALL=false
 82: RESTART_CLAUDABLE=false
 83: FORCE_REBUILD=false
 84: SKIP_VALIDATION=false
 85: 
 86: while [[ $# -gt 0 ]]; do
 87:     case $1 in
 88:         --all|-a)
 89:             RESTART_ALL=true
 90:             shift
 91:             ;;
 92:         --claudable|-c)
 93:             RESTART_CLAUDABLE=true
 94:             shift
 95:             ;;
 96:         --rebuild|-r)
 97:             FORCE_REBUILD=true
 98:             shift
 99:             ;;
100:         --skip-validation|-s)
101:             SKIP_VALIDATION=true
102:             shift
103:             ;;
104:         --help|-h)
105:             echo &quot;DSP Agent Restart Script&quot;
106:             echo &quot;&quot;
107:             echo &quot;Usage: $0 [OPTIONS]&quot;
108:             echo &quot;&quot;
109:             echo &quot;Options:&quot;
110:             echo &quot;  --all, -a              Restart all services (MCP + Claudable)&quot;
111:             echo &quot;  --claudable, -c        Restart only Claudable interface&quot;
112:             echo &quot;  --rebuild, -r          Force rebuild Docker containers&quot;
113:             echo &quot;  --skip-validation, -s  Skip post-restart validation&quot;
114:             echo &quot;  --help, -h             Show this help message&quot;
115:             echo &quot;&quot;
116:             echo &quot;Examples:&quot;
117:             echo &quot;  $0                     # Quick MCP server restart&quot;
118:             echo &quot;  $0 --all               # Restart everything&quot;
119:             echo &quot;  $0 --claudable         # Restart only Claudable&quot;
120:             echo &quot;  $0 --rebuild           # Force rebuild and restart&quot;
121:             exit 0
122:             ;;
123:         *)
124:             echo -e &quot;${RED}Unknown option: $1${NC}&quot;
125:             echo &quot;Use --help for usage information.&quot;
126:             exit 1
127:             ;;
128:     esac
129: done
130: 
131: # Check prerequisites
132: echo -e &quot;\n${CYAN}üìã System Check${NC}&quot;
133: echo &quot;---------------&quot;
134: 
135: if ! command_exists docker; then
136:     echo -e &quot;${RED}‚ùå Docker not found${NC}&quot;
137:     exit 1
138: fi
139: 
140: if ! docker info &gt;/dev/null 2&gt;&amp;1; then
141:     echo -e &quot;${RED}‚ùå Docker daemon not running${NC}&quot;
142:     exit 1
143: fi
144: 
145: if [ ! -d &quot;docker&quot; ]; then
146:     echo -e &quot;${RED}‚ùå docker/ directory not found. Run from project root.${NC}&quot;
147:     exit 1
148: fi
149: 
150: echo -e &quot;${GREEN}‚úÖ Prerequisites OK${NC}&quot;
151: 
152: # Navigate to docker directory
153: cd docker
154: 
155: # Handle different restart scenarios
156: if [ &quot;$FORCE_REBUILD&quot; = true ]; then
157:     echo -e &quot;\n${YELLOW}üèóÔ∏è  Force Rebuild Mode${NC}&quot;
158:     echo &quot;---------------------&quot;
159: 
160:     echo &quot;Stopping all containers...&quot;
161:     docker-compose down --remove-orphans
162: 
163:     echo &quot;Removing images...&quot;
164:     docker-compose down --rmi local --remove-orphans
165: 
166:     echo &quot;Rebuilding and starting...&quot;
167:     docker-compose up -d --build
168: 
169: elif [ &quot;$RESTART_ALL&quot; = true ]; then
170:     echo -e &quot;\n${YELLOW}üîÑ Full System Restart${NC}&quot;
171:     echo &quot;----------------------&quot;
172: 
173:     echo &quot;Stopping all services...&quot;
174:     docker-compose down
175: 
176:     echo &quot;Starting all services...&quot;
177:     docker-compose up -d
178: 
179: else
180:     echo -e &quot;\n${YELLOW}üîÑ MCP Server Recovery${NC}&quot;
181:     echo &quot;---------------------&quot;
182: 
183:     # Check current container status
184:     echo &quot;Current container status:&quot;
185:     docker ps --filter &quot;name=dsp&quot; --format &quot;table {{.Names}}\t{{.Status}}\t{{.Ports}}&quot; || true
186: 
187:     # Restart problematic containers
188:     containers=(&quot;dsp-qdrant&quot; &quot;dsp-mcp-ragdocs&quot; &quot;dsp-mcp-search&quot;)
189: 
190:     for container in &quot;${containers[@]}&quot;; do
191:         if ! check_container_health &quot;$container&quot;; then
192:             restart_container &quot;$container&quot;
193:         fi
194:     done
195: fi
196: 
197: # Wait for services to be ready
198: if [ &quot;$SKIP_VALIDATION&quot; != true ]; then
199:     echo -e &quot;\n${YELLOW}‚è≥ Service Health Verification${NC}&quot;
200:     echo &quot;-----------------------------&quot;
201: 
202:     # Wait for each service
203:     wait_for_endpoint &quot;http://localhost:6333/&quot; &quot;Qdrant Database&quot;
204:     wait_for_endpoint &quot;http://localhost:3002/health&quot; &quot;RAG Server&quot;
205:     wait_for_endpoint &quot;http://localhost:3004/health&quot; &quot;Search Server&quot;
206: 
207:     # Run full validation if available
208:     if [ -x &quot;./validate-infrastructure.sh&quot; ]; then
209:         echo -e &quot;\n${YELLOW}üîç Running Full Validation${NC}&quot;
210:         echo &quot;-------------------------&quot;
211:         ./validate-infrastructure.sh
212:     fi
213: fi
214: 
215: # Return to project root
216: cd ..
217: 
218: # Handle Claudable restart if requested or in --all mode
219: CLAUDABLE_PATH=&quot;../Claudable&quot;
220: if [ &quot;$RESTART_CLAUDABLE&quot; = true ] || [ &quot;$RESTART_ALL&quot; = true ]; then
221:     if [ -d &quot;$CLAUDABLE_PATH&quot; ]; then
222:         echo -e &quot;\n${YELLOW}ü§ñ Restarting Claudable Interface${NC}&quot;
223:         echo &quot;---------------------------------&quot;
224: 
225:         # Stop existing Claudable processes
226:         echo &quot;Stopping existing Claudable processes...&quot;
227:         pkill -f &quot;node.*claudable&quot; || true
228:         pkill -f &quot;npm.*start.*claudable&quot; || true
229:         sleep 2
230: 
231:         # Check if port is still in use
232:         if lsof -i :3001 &gt;/dev/null 2&gt;&amp;1; then
233:             echo -e &quot;${YELLOW}‚ö†Ô∏è  Port 3001 still in use, forcing cleanup...${NC}&quot;
234:             # Find and kill process using port 3001
235:             PID=$(lsof -ti :3001)
236:             if [ -n &quot;$PID&quot; ]; then
237:                 kill -9 &quot;$PID&quot; 2&gt;/dev/null || true
238:                 sleep 1
239:             fi
240:         fi
241: 
242:         # Start Claudable
243:         echo &quot;Starting Claudable...&quot;
244:         (cd &quot;$CLAUDABLE_PATH&quot; &amp;&amp; npm start) &amp;
245:         CLAUDABLE_PID=$!
246: 
247:         # Wait for Claudable
248:         sleep 5
249:         if wait_for_endpoint &quot;http://localhost:3001&quot; &quot;Claudable Interface&quot;; then
250:             echo -e &quot;${GREEN}‚úÖ Claudable restarted (PID: $CLAUDABLE_PID)${NC}&quot;
251:         else
252:             echo -e &quot;${RED}‚ùå Claudable restart failed${NC}&quot;
253:         fi
254:     else
255:         echo -e &quot;${YELLOW}‚ö†Ô∏è  Claudable directory not found at $CLAUDABLE_PATH${NC}&quot;
256:     fi
257: fi
258: 
259: # Final status report
260: echo -e &quot;\n${GREEN}üéØ System Recovery Complete${NC}&quot;
261: echo &quot;============================&quot;
262: echo
263: echo &quot;üìä Service Status:&quot;
264: echo &quot;  ‚Ä¢ Qdrant Database:    http://localhost:6333/&quot;
265: echo &quot;  ‚Ä¢ RAG Server:         http://localhost:3002/health&quot;
266: echo &quot;  ‚Ä¢ Search Server:      http://localhost:3004/health&quot;
267: 
268: if [ -d &quot;$CLAUDABLE_PATH&quot; ] &amp;&amp; ([ &quot;$RESTART_CLAUDABLE&quot; = true ] || [ &quot;$RESTART_ALL&quot; = true ]); then
269:     echo &quot;  ‚Ä¢ Claudable Interface: http://localhost:3001&quot;
270: fi
271: 
272: echo
273: echo &quot;üîç Diagnosis Commands:&quot;
274: echo &quot;  ‚Ä¢ Check containers:    docker ps --filter &apos;name=dsp&apos;&quot;
275: echo &quot;  ‚Ä¢ View logs:           docker-compose -f docker/docker-compose.yml logs -f [service]&quot;
276: echo &quot;  ‚Ä¢ Full validation:     ./docker/validate-infrastructure.sh&quot;
277: echo
278: echo &quot;üõ†Ô∏è  Troubleshooting:&quot;
279: echo &quot;  ‚Ä¢ Force rebuild:       $0 --rebuild&quot;
280: echo &quot;  ‚Ä¢ Restart everything:  $0 --all&quot;
281: echo &quot;  ‚Ä¢ Get help:            $0 --help&quot;
282: echo
283: echo -e &quot;${GREEN}‚úÖ System ready for queries!${NC}&quot;</file><file path="start.sh">  1: #!/bin/bash
  2: 
  3: # DSP Documentation Agent - MVP Deployment Script
  4: # Launches Docker containers and initializes the complete system
  5: 
  6: set -e
  7: 
  8: # Color codes for output
  9: RED=&apos;\033[0;31m&apos;
 10: GREEN=&apos;\033[0;32m&apos;
 11: YELLOW=&apos;\033[1;33m&apos;
 12: BLUE=&apos;\033[0;34m&apos;
 13: NC=&apos;\033[0m&apos; # No Color
 14: 
 15: echo -e &quot;${BLUE}üöÄ DSP Documentation Agent - MVP Deployment${NC}&quot;
 16: echo &quot;=============================================&quot;
 17: 
 18: # Function to check if command exists
 19: command_exists() {
 20:     command -v &quot;$1&quot; &gt;/dev/null 2&gt;&amp;1
 21: }
 22: 
 23: # Function to wait for endpoint
 24: wait_for_endpoint() {
 25:     local url=&quot;$1&quot;
 26:     local service=&quot;$2&quot;
 27:     local max_attempts=30
 28:     local attempt=1
 29: 
 30:     echo -n &quot;Waiting for $service ($url)...&quot;
 31:     while [ $attempt -le $max_attempts ]; do
 32:         if curl -s -f &quot;$url&quot; &gt;/dev/null 2&gt;&amp;1; then
 33:             echo -e &quot; ${GREEN}‚úÖ Ready${NC}&quot;
 34:             return 0
 35:         fi
 36:         echo -n &quot;.&quot;
 37:         sleep 2
 38:         attempt=$((attempt + 1))
 39:     done
 40: 
 41:     echo -e &quot; ${RED}‚ùå Timeout${NC}&quot;
 42:     return 1
 43: }
 44: 
 45: # Check prerequisites
 46: echo -e &quot;\n${YELLOW}üìã Prerequisites Check${NC}&quot;
 47: echo &quot;----------------------&quot;
 48: 
 49: if ! command_exists docker; then
 50:     echo -e &quot;${RED}‚ùå Docker not found. Please install Docker Desktop.${NC}&quot;
 51:     exit 1
 52: fi
 53: echo -e &quot;${GREEN}‚úÖ Docker found${NC}&quot;
 54: 
 55: if ! docker info &gt;/dev/null 2&gt;&amp;1; then
 56:     echo -e &quot;${RED}‚ùå Docker daemon not running. Please start Docker Desktop.${NC}&quot;
 57:     exit 1
 58: fi
 59: echo -e &quot;${GREEN}‚úÖ Docker daemon running${NC}&quot;
 60: 
 61: # Check required directories
 62: if [ ! -d &quot;docker&quot; ]; then
 63:     echo -e &quot;${RED}‚ùå docker/ directory not found. Run from project root.${NC}&quot;
 64:     exit 1
 65: fi
 66: echo -e &quot;${GREEN}‚úÖ Project structure valid${NC}&quot;
 67: 
 68: # Check environment file
 69: if [ ! -f &quot;docker/.env&quot; ]; then
 70:     echo -e &quot;${RED}‚ùå docker/.env file missing. Please create with required API keys.${NC}&quot;
 71:     echo &quot;Required variables: OPENAI_API_KEY, BRAVE_API_KEY&quot;
 72:     exit 1
 73: fi
 74: echo -e &quot;${GREEN}‚úÖ Environment configuration found${NC}&quot;
 75: 
 76: # Check for Claudable repository
 77: CLAUDABLE_PATH=&quot;../Claudable&quot;
 78: if [ ! -d &quot;$CLAUDABLE_PATH&quot; ]; then
 79:     echo -e &quot;${YELLOW}‚ö†Ô∏è  Claudable repository not found at $CLAUDABLE_PATH${NC}&quot;
 80:     echo &quot;   The agent will run without Claudable interface.&quot;
 81: fi
 82: 
 83: # Stop any existing containers
 84: echo -e &quot;\n${YELLOW}üõë Stopping Existing Containers${NC}&quot;
 85: echo &quot;--------------------------------&quot;
 86: cd docker
 87: docker-compose down --remove-orphans || true
 88: 
 89: # Start Docker infrastructure
 90: echo -e &quot;\n${YELLOW}üê≥ Starting Docker Infrastructure${NC}&quot;
 91: echo &quot;--------------------------------&quot;
 92: docker-compose up -d
 93: 
 94: # Wait for services to be ready
 95: echo -e &quot;\n${YELLOW}‚è≥ Waiting for Services${NC}&quot;
 96: echo &quot;----------------------&quot;
 97: wait_for_endpoint &quot;http://localhost:6333/&quot; &quot;Qdrant Database&quot;
 98: wait_for_endpoint &quot;http://localhost:3002/health&quot; &quot;RAG Server&quot;
 99: wait_for_endpoint &quot;http://localhost:3004/health&quot; &quot;Search Server&quot;
100: 
101: # Validate infrastructure
102: echo -e &quot;\n${YELLOW}üîç Infrastructure Validation${NC}&quot;
103: echo &quot;----------------------------&quot;
104: if [ -x &quot;./validate-infrastructure.sh&quot; ]; then
105:     ./validate-infrastructure.sh
106: else
107:     echo -e &quot;${RED}‚ùå Validation script not executable${NC}&quot;
108:     chmod +x ./validate-infrastructure.sh
109:     ./validate-infrastructure.sh
110: fi
111: 
112: # Return to project root
113: cd ..
114: 
115: # Start Claudable if available
116: if [ -d &quot;$CLAUDABLE_PATH&quot; ]; then
117:     echo -e &quot;\n${YELLOW}ü§ñ Starting Claudable Interface${NC}&quot;
118:     echo &quot;------------------------------&quot;
119: 
120:     # Check if Claudable has dependencies installed
121:     if [ ! -d &quot;$CLAUDABLE_PATH/node_modules&quot; ]; then
122:         echo &quot;Installing Claudable dependencies...&quot;
123:         (cd &quot;$CLAUDABLE_PATH&quot; &amp;&amp; npm install)
124:     fi
125: 
126:     # Check if Claudable is already running
127:     if lsof -i :3001 &gt;/dev/null 2&gt;&amp;1; then
128:         echo -e &quot;${YELLOW}‚ö†Ô∏è  Port 3001 in use. Stopping existing Claudable...${NC}&quot;
129:         pkill -f &quot;node.*claudable&quot; || true
130:         sleep 2
131:     fi
132: 
133:     # Start Claudable in background
134:     echo &quot;Starting Claudable on http://localhost:3001...&quot;
135:     (cd &quot;$CLAUDABLE_PATH&quot; &amp;&amp; npm start) &amp;
136:     CLAUDABLE_PID=$!
137: 
138:     # Wait for Claudable to start
139:     sleep 5
140:     if wait_for_endpoint &quot;http://localhost:3001&quot; &quot;Claudable Interface&quot;; then
141:         echo -e &quot;${GREEN}‚úÖ Claudable started (PID: $CLAUDABLE_PID)${NC}&quot;
142:         echo &quot;   Save this PID to stop Claudable later: kill $CLAUDABLE_PID&quot;
143:     else
144:         echo -e &quot;${RED}‚ùå Claudable failed to start${NC}&quot;
145:     fi
146: fi
147: 
148: # Display final status
149: echo -e &quot;\n${GREEN}üéØ DSP Documentation Agent - Deployment Complete${NC}&quot;
150: echo &quot;=================================================&quot;
151: echo
152: echo &quot;üìä Service Status:&quot;
153: echo &quot;  ‚Ä¢ Qdrant Database:    http://localhost:6333/&quot;
154: echo &quot;  ‚Ä¢ RAG Server:         http://localhost:3002/health&quot;
155: echo &quot;  ‚Ä¢ Search Server:      http://localhost:3004/health&quot;
156: if [ -d &quot;$CLAUDABLE_PATH&quot; ]; then
157:     echo &quot;  ‚Ä¢ Claudable Interface: http://localhost:3001&quot;
158: fi
159: echo
160: echo &quot;üß™ Testing Commands:&quot;
161: echo &quot;  ‚Ä¢ Test RAG search:     curl &apos;http://localhost:3002/search?q=dyson+sphere&apos;&quot;
162: echo &quot;  ‚Ä¢ Test web search:     curl &apos;http://localhost:3004/search?q=physics&apos;&quot;
163: echo &quot;  ‚Ä¢ Full validation:     ./docker/validate-infrastructure.sh&quot;
164: echo
165: echo &quot;üîß Management Commands:&quot;
166: echo &quot;  ‚Ä¢ Restart services:    ./restart.sh&quot;
167: echo &quot;  ‚Ä¢ Stop services:       docker-compose -f docker/docker-compose.yml down&quot;
168: echo &quot;  ‚Ä¢ View logs:           docker-compose -f docker/docker-compose.yml logs -f&quot;
169: echo
170: echo -e &quot;${GREEN}‚úÖ System ready for DSP documentation queries!${NC}&quot;
171: 
172: # Optional: Run example queries
173: read -p &quot;Run example queries to test the system? (y/N): &quot; -n 1 -r
174: echo
175: if [[ $REPLY =~ ^[Yy]$ ]]; then
176:     echo -e &quot;\n${BLUE}üß™ Running Example Queries${NC}&quot;
177:     echo &quot;==========================&quot;
178: 
179:     echo -e &quot;\n1. Testing RAG server with DSP query...&quot;
180:     curl -s &quot;http://localhost:3002/search?q=dyson+sphere&quot; | python3 -m json.tool || echo &quot;RAG query failed&quot;
181: 
182:     echo -e &quot;\n2. Testing search server with physics query...&quot;
183:     curl -s &quot;http://localhost:3004/search?q=theoretical+physics&quot; | python3 -m json.tool || echo &quot;Search query failed&quot;
184: 
185:     echo -e &quot;\n${GREEN}‚úÖ Example queries complete${NC}&quot;
186: fi</file><file path="test-agent-queries.sh">  1: #!/bin/bash
  2: 
  3: # DSP Agent Query Testing Suite
  4: # Tests the complete system with example queries demonstrating 60/40 balance
  5: 
  6: set -e
  7: 
  8: # Color codes for output
  9: RED=&apos;\033[0;31m&apos;
 10: GREEN=&apos;\033[0;32m&apos;
 11: YELLOW=&apos;\033[1;33m&apos;
 12: BLUE=&apos;\033[0;34m&apos;
 13: CYAN=&apos;\033[0;36m&apos;
 14: NC=&apos;\033[0m&apos; # No Color
 15: 
 16: echo -e &quot;${BLUE}üß™ DSP Agent Query Testing Suite${NC}&quot;
 17: echo &quot;================================&quot;
 18: 
 19: # Function to test query and validate response
 20: test_query() {
 21:     local description=&quot;$1&quot;
 22:     local query=&quot;$2&quot;
 23:     local expected_sources=&quot;$3&quot;
 24: 
 25:     echo -e &quot;\n${CYAN}Testing: $description${NC}&quot;
 26:     echo &quot;Query: $query&quot;
 27:     echo &quot;Expected sources: $expected_sources&quot;
 28:     echo -n &quot;Response: &quot;
 29: 
 30:     # Make the request and capture response
 31:     response=$(curl -s -X POST http://localhost:3001/chat \
 32:         -H &quot;Content-Type: application/json&quot; \
 33:         -d &quot;{\&quot;message\&quot;:\&quot;$query\&quot;}&quot; || echo &quot;ERROR&quot;)
 34: 
 35:     if [ &quot;$response&quot; = &quot;ERROR&quot; ]; then
 36:         echo -e &quot;${RED}‚ùå Request failed${NC}&quot;
 37:         return 1
 38:     fi
 39: 
 40:     # Extract response text and source counts
 41:     response_text=$(echo &quot;$response&quot; | jq -r &apos;.response // &quot;No response&quot;&apos;)
 42:     rag_sources=$(echo &quot;$response&quot; | jq -r &apos;.sources.rag // 0&apos;)
 43:     search_sources=$(echo &quot;$response&quot; | jq -r &apos;.sources.search // 0&apos;)
 44: 
 45:     # Validate response length (should be substantial)
 46:     if [ ${#response_text} -lt 50 ]; then
 47:         echo -e &quot;${RED}‚ùå Response too short (${#response_text} chars)${NC}&quot;
 48:         return 1
 49:     fi
 50: 
 51:     # Check source usage matches expectations
 52:     case $expected_sources in
 53:         &quot;rag&quot;)
 54:             if [ &quot;$rag_sources&quot; -gt 0 ] &amp;&amp; [ &quot;$search_sources&quot; -eq 0 ]; then
 55:                 echo -e &quot;${GREEN}‚úÖ Correct sources (RAG: $rag_sources, Search: $search_sources)${NC}&quot;
 56:             else
 57:                 echo -e &quot;${YELLOW}‚ö†Ô∏è  Unexpected sources (RAG: $rag_sources, Search: $search_sources)${NC}&quot;
 58:             fi
 59:             ;;
 60:         &quot;search&quot;)
 61:             if [ &quot;$search_sources&quot; -gt 0 ] &amp;&amp; [ &quot;$rag_sources&quot; -eq 0 ]; then
 62:                 echo -e &quot;${GREEN}‚úÖ Correct sources (RAG: $rag_sources, Search: $search_sources)${NC}&quot;
 63:             else
 64:                 echo -e &quot;${YELLOW}‚ö†Ô∏è  Unexpected sources (RAG: $rag_sources, Search: $search_sources)${NC}&quot;
 65:             fi
 66:             ;;
 67:         &quot;both&quot;)
 68:             if [ &quot;$rag_sources&quot; -gt 0 ] &amp;&amp; [ &quot;$search_sources&quot; -gt 0 ]; then
 69:                 echo -e &quot;${GREEN}‚úÖ Correct sources (RAG: $rag_sources, Search: $search_sources)${NC}&quot;
 70:             else
 71:                 echo -e &quot;${YELLOW}‚ö†Ô∏è  Expected both sources (RAG: $rag_sources, Search: $search_sources)${NC}&quot;
 72:             fi
 73:             ;;
 74:         *)
 75:             echo -e &quot;${GREEN}‚úÖ Response received (RAG: $rag_sources, Search: $search_sources)${NC}&quot;
 76:             ;;
 77:     esac
 78: 
 79:     # Show first 150 characters of response
 80:     echo &quot;Preview: ${response_text:0:150}...&quot;
 81: 
 82:     return 0
 83: }
 84: 
 85: # Test basic health first
 86: echo -e &quot;\n${YELLOW}üìã Health Check${NC}&quot;
 87: echo &quot;---------------&quot;
 88: health_response=$(curl -s http://localhost:3001/health)
 89: if echo &quot;$health_response&quot; | jq -e &apos;.status == &quot;ok&quot;&apos; &gt;/dev/null 2&gt;&amp;1; then
 90:     echo -e &quot;${GREEN}‚úÖ Agent health OK${NC}&quot;
 91: else
 92:     echo -e &quot;${RED}‚ùå Agent health check failed${NC}&quot;
 93:     echo &quot;Response: $health_response&quot;
 94:     exit 1
 95: fi
 96: 
 97: # Test game mechanics queries (should use RAG)
 98: echo -e &quot;\n${BLUE}üéÆ Game Mechanics Queries (60% Focus)${NC}&quot;
 99: echo &quot;=====================================&quot;
100: 
101: test_query &quot;Critical Photons mechanics&quot; \
102:     &quot;How do Critical Photons work in DSP?&quot; \
103:     &quot;rag&quot;
104: 
105: test_query &quot;Recipe optimization&quot; \
106:     &quot;What is the most efficient antimatter production setup in DSP?&quot; \
107:     &quot;rag&quot;
108: 
109: test_query &quot;Solar sail calculations&quot; \
110:     &quot;How many Solar Sails do I need for a Dyson Sphere around a K-class star?&quot; \
111:     &quot;rag&quot;
112: 
113: # Test physics speculation queries (should use search)
114: echo -e &quot;\n${BLUE}üî¨ Physics Speculation Queries (40% Focus)${NC}&quot;
115: echo &quot;===========================================&quot;
116: 
117: test_query &quot;Real Dyson sphere feasibility&quot; \
118:     &quot;Could we actually build a real Dyson sphere?&quot; \
119:     &quot;search&quot;
120: 
121: test_query &quot;Current antimatter research&quot; \
122:     &quot;What is the current state of real antimatter research?&quot; \
123:     &quot;search&quot;
124: 
125: test_query &quot;Space engineering papers&quot; \
126:     &quot;Are there any recent research papers on stellar engineering?&quot; \
127:     &quot;search&quot;
128: 
129: # Test hybrid queries (should use both sources)
130: echo -e &quot;\n${BLUE}üîÄ Hybrid Queries (Game + Physics)${NC}&quot;
131: echo &quot;==================================&quot;
132: 
133: test_query &quot;Game vs reality comparison&quot; \
134:     &quot;Compare DSP antimatter production to real physics&quot; \
135:     &quot;both&quot;
136: 
137: test_query &quot;Technology assessment&quot; \
138:     &quot;How realistic are the technologies in DSP?&quot; \
139:     &quot;both&quot;
140: 
141: test_query &quot;Scale comparison&quot; \
142:     &quot;How long would it actually take to build a real Dyson sphere compared to DSP?&quot; \
143:     &quot;both&quot;
144: 
145: # Test tone and personality
146: echo -e &quot;\n${BLUE}üé≠ Personality &amp; Tone Tests${NC}&quot;
147: echo &quot;===========================&quot;
148: 
149: test_query &quot;Excitement test&quot; \
150:     &quot;Tell me about Critical Photons - make it exciting!&quot; \
151:     &quot;rag&quot;
152: 
153: test_query &quot;Content creator support&quot; \
154:     &quot;Give me 3 video ideas comparing DSP technologies to real physics&quot; \
155:     &quot;both&quot;
156: 
157: # Performance tests
158: echo -e &quot;\n${BLUE}‚ö° Performance Tests${NC}&quot;
159: echo &quot;===================&quot;
160: 
161: echo -n &quot;Simple query timing: &quot;
162: time_start=$(date +%s.%N)
163: curl -s -X POST http://localhost:3001/chat \
164:     -H &quot;Content-Type: application/json&quot; \
165:     -d &apos;{&quot;message&quot;:&quot;What is Iron Ore used for in DSP?&quot;}&apos; &gt;/dev/null
166: time_end=$(date +%s.%N)
167: duration=$(echo &quot;$time_end - $time_start&quot; | bc -l)
168: echo &quot;${duration}s&quot;
169: 
170: # Final summary
171: echo -e &quot;\n${GREEN}üéØ Testing Summary${NC}&quot;
172: echo &quot;==================&quot;
173: echo &quot;‚úÖ All test categories completed&quot;
174: echo &quot;üìä Check responses above for:&quot;
175: echo &quot;   ‚Ä¢ Appropriate source usage (RAG vs Search vs Both)&quot;
176: echo &quot;   ‚Ä¢ Fun, engaging tone (not academic)&quot;
177: echo &quot;   ‚Ä¢ Technical accuracy and game knowledge&quot;
178: echo &quot;   ‚Ä¢ 60/40 balance between game mechanics and physics&quot;
179: echo
180: echo &quot;üîß If issues found:&quot;
181: echo &quot;   ‚Ä¢ Check MCP server logs: docker-compose -f docker/docker-compose.yml logs&quot;
182: echo &quot;   ‚Ä¢ Restart services: ./restart.sh&quot;
183: echo &quot;   ‚Ä¢ Review agent configuration: claudable/config.json&quot;
184: echo
185: echo -e &quot;${GREEN}‚úÖ DSP Agent testing complete!${NC}&quot;</file><file path="TROUBLESHOOTING.md">  1: # DSP Documentation Agent - Troubleshooting Guide
  2: 
  3: ## Quick Diagnosis
  4: 
  5: ### üîç System Health Check
  6: ```bash
  7: # One-command system validation
  8: ./docker/validate-infrastructure.sh
  9: 
 10: # Quick status check
 11: docker ps --filter &quot;name=dsp&quot; --format &quot;table {{.Names}}\t{{.Status}}\t{{.Ports}}&quot;
 12: 
 13: # Test all endpoints
 14: curl http://localhost:3001/health  # Agent
 15: curl http://localhost:3002/health  # RAG
 16: curl http://localhost:3004/health  # Search
 17: curl http://localhost:6333/        # Qdrant
 18: ```
 19: 
 20: ## Common Issues &amp; Solutions
 21: 
 22: ### üê≥ Docker Container Issues
 23: 
 24: #### Problem: Containers Not Starting
 25: ```bash
 26: # Symptoms
 27: docker ps --filter &quot;name=dsp&quot; | wc -l    # Returns less than 3
 28: 
 29: # Diagnosis
 30: docker-compose -f docker/docker-compose.yml logs
 31: 
 32: # Solutions
 33: ./restart.sh --rebuild                    # Force rebuild
 34: docker system prune -f                    # Clean Docker cache
 35: docker-compose -f docker/docker-compose.yml down --volumes &amp;&amp; \
 36: docker-compose -f docker/docker-compose.yml up -d
 37: ```
 38: 
 39: #### Problem: Container Memory Issues
 40: ```bash
 41: # Symptoms
 42: Container keeps restarting, high memory usage
 43: 
 44: # Diagnosis
 45: docker stats --filter &quot;name=dsp&quot;
 46: 
 47: # Solutions
 48: # Increase Docker Desktop memory allocation (8GB recommended)
 49: # Restart Docker Desktop
 50: docker-compose -f docker/docker-compose.yml restart
 51: ```
 52: 
 53: #### Problem: Qdrant Storage Corruption
 54: ```bash
 55: # Symptoms
 56: dsp-qdrant fails to start, storage errors in logs
 57: 
 58: # Diagnosis
 59: docker logs dsp-qdrant | grep -i error
 60: ls -la docker/qdrant_storage/
 61: 
 62: # Solutions
 63: # CAUTION: This deletes all stored documents
 64: rm -rf docker/qdrant_storage/*
 65: docker-compose -f docker/docker-compose.yml restart qdrant
 66: ```
 67: 
 68: ### üåê Network &amp; Port Issues
 69: 
 70: #### Problem: Port Already in Use
 71: ```bash
 72: # Symptoms
 73: &quot;Port already allocated&quot; error during startup
 74: 
 75: # Diagnosis
 76: lsof -i :3001    # Claudable
 77: lsof -i :3002    # RAG server
 78: lsof -i :3004    # Search server
 79: lsof -i :6333    # Qdrant
 80: 
 81: # Solutions
 82: # Kill existing processes
 83: kill $(lsof -ti :3001)    # Replace 3001 with conflicting port
 84: ./restart.sh --all        # Restart everything
 85: ```
 86: 
 87: #### Problem: Cannot Connect to MCP Servers
 88: ```bash
 89: # Symptoms
 90: curl requests to localhost:3002/3004 timeout or fail
 91: 
 92: # Diagnosis
 93: docker logs dsp-mcp-ragdocs
 94: docker logs dsp-mcp-search
 95: curl -v http://localhost:3002/health
 96: 
 97: # Solutions
 98: ./restart.sh                              # Quick restart
 99: docker exec dsp-mcp-ragdocs npm install   # Reinstall dependencies
100: docker-compose -f docker/docker-compose.yml restart mcp-ragdocs mcp-web-search
101: ```
102: 
103: ### üîë API Key &amp; Authentication Issues
104: 
105: #### Problem: OpenAI API Errors
106: ```bash
107: # Symptoms
108: &quot;Invalid API key&quot; or &quot;Rate limit exceeded&quot; in logs
109: 
110: # Diagnosis
111: grep -i &quot;api&quot; docker/.env
112: curl -H &quot;Authorization: Bearer $(grep OPENAI_API_KEY docker/.env | cut -d= -f2)&quot; \
113:   https://api.openai.com/v1/models
114: 
115: # Solutions
116: # Verify API key in docker/.env
117: export OPENAI_API_KEY=&quot;sk-your-key&quot;
118: ./restart.sh --all
119: ```
120: 
121: #### Problem: Brave Search API Issues
122: ```bash
123: # Symptoms
124: Web search requests fail, search functionality broken
125: 
126: # Diagnosis
127: grep BRAVE_API_KEY docker/.env
128: docker logs dsp-mcp-search | grep -i brave
129: 
130: # Solutions
131: # Check Brave API key validity
132: # Restart search service
133: docker-compose -f docker/docker-compose.yml restart mcp-web-search
134: ```
135: 
136: ### ü§ñ Claudable Interface Issues
137: 
138: #### Problem: Claudable Won&apos;t Start
139: ```bash
140: # Symptoms
141: Cannot access http://localhost:3001
142: 
143: # Diagnosis
144: cd claudable &amp;&amp; npm list    # Check dependencies
145: cat claudable/.env          # Check configuration
146: ps aux | grep node          # Check for existing processes
147: 
148: # Solutions
149: cd claudable
150: npm install                 # Reinstall dependencies
151: rm -rf node_modules &amp;&amp; npm install  # Clean reinstall
152: ./restart.sh --claudable    # Use restart script
153: ```
154: 
155: #### Problem: Claudable Configuration Errors
156: ```bash
157: # Symptoms
158: &quot;Cannot connect to MCP servers&quot; errors in Claudable
159: 
160: # Diagnosis
161: cat claudable/config.json | grep -A 5 mcp_servers
162: curl http://localhost:3002/health    # Test MCP connectivity
163: 
164: # Solutions
165: # Verify MCP endpoints in config.json:
166: # &quot;rag&quot;: &quot;http://localhost:3002&quot;
167: # &quot;search&quot;: &quot;http://localhost:3004&quot;
168: ./restart.sh --all
169: ```
170: 
171: ### üìö RAG &amp; Search Functionality Issues
172: 
173: #### Problem: No Search Results from RAG
174: ```bash
175: # Symptoms
176: RAG searches return empty results
177: 
178: # Diagnosis
179: curl &quot;http://localhost:3002/search?q=test&quot;
180: docker exec dsp-qdrant curl http://localhost:6333/collections
181: 
182: # Solutions
183: # Check if documents are loaded in Qdrant
184: curl http://localhost:6333/collections/documents/points/count
185: # Re-ingest documentation if needed
186: ./restart.sh --rebuild
187: ```
188: 
189: #### Problem: Web Search Not Working
190: ```bash
191: # Symptoms
192: Physics searches fail or return no results
193: 
194: # Diagnosis
195: curl &quot;http://localhost:3004/search?q=physics&quot;
196: docker logs dsp-mcp-search | tail -20
197: 
198: # Solutions
199: # Check Brave API quota and key validity
200: ./restart.sh
201: # Verify internet connectivity from container
202: docker exec dsp-mcp-search curl https://api.search.brave.com/
203: ```
204: 
205: ## Performance Troubleshooting
206: 
207: ### üêå Slow Response Times
208: 
209: #### Problem: Agent Responses Take Too Long
210: ```bash
211: # Diagnosis
212: time curl -X POST http://localhost:3001/chat \
213:   -H &quot;Content-Type: application/json&quot; \
214:   -d &apos;{&quot;message&quot;:&quot;test&quot;}&apos;
215: 
216: # Check resource usage
217: docker stats --filter &quot;name=dsp&quot;
218: 
219: # Solutions
220: # Increase Docker memory allocation
221: # Restart services to clear memory leaks
222: ./restart.sh --all
223: # Consider switching to lighter Claude model in config
224: ```
225: 
226: ### üíæ Storage Issues
227: 
228: #### Problem: Docker Disk Space
229: ```bash
230: # Diagnosis
231: docker system df
232: du -sh docker/qdrant_storage/
233: 
234: # Solutions
235: docker system prune -f                    # Clean unused data
236: docker volume prune -f                    # Clean volumes
237: # Rotate Qdrant storage if too large
238: ```
239: 
240: ## Emergency Recovery
241: 
242: ### üö® Complete System Reset
243: ```bash
244: # Nuclear option - resets everything
245: docker-compose -f docker/docker-compose.yml down --volumes --rmi all
246: rm -rf docker/qdrant_storage/*
247: docker system prune -af
248: ./start.sh
249: ```
250: 
251: ### üîß Component-by-Component Recovery
252: 
253: #### 1. Reset Qdrant Only
254: ```bash
255: docker-compose -f docker/docker-compose.yml stop qdrant
256: rm -rf docker/qdrant_storage/*
257: docker-compose -f docker/docker-compose.yml up -d qdrant
258: ```
259: 
260: #### 2. Reset MCP Servers Only
261: ```bash
262: docker-compose -f docker/docker-compose.yml restart mcp-ragdocs mcp-web-search
263: ```
264: 
265: #### 3. Reset Claudable Only
266: ```bash
267: ./restart.sh --claudable
268: ```
269: 
270: ## Monitoring &amp; Maintenance
271: 
272: ### üìä Health Monitoring
273: ```bash
274: # Add to crontab for automated monitoring
275: # */5 * * * * /path/to/dyson-sphere-facts/docker/validate-infrastructure.sh
276: 
277: # Manual monitoring commands
278: watch &apos;docker ps --filter &quot;name=dsp&quot; --format &quot;table {{.Names}}\t{{.Status}}&quot;&apos;
279: ```
280: 
281: ### üîÑ Preventive Maintenance
282: ```bash
283: # Weekly cleanup
284: docker-compose -f docker/docker-compose.yml restart
285: docker system prune -f
286: 
287: # Monthly full restart
288: ./restart.sh --rebuild
289: ```
290: 
291: ## Getting Help
292: 
293: ### üìã Collecting Debug Information
294: ```bash
295: # Generate comprehensive debug report
296: echo &quot;=== System Info ===&quot; &gt; debug-report.txt
297: uname -a &gt;&gt; debug-report.txt
298: docker --version &gt;&gt; debug-report.txt
299: 
300: echo &quot;=== Container Status ===&quot; &gt;&gt; debug-report.txt
301: docker ps --filter &quot;name=dsp&quot; &gt;&gt; debug-report.txt
302: 
303: echo &quot;=== Port Usage ===&quot; &gt;&gt; debug-report.txt
304: lsof -i :3001 &gt;&gt; debug-report.txt
305: lsof -i :3002 &gt;&gt; debug-report.txt
306: lsof -i :3004 &gt;&gt; debug-report.txt
307: 
308: echo &quot;=== Docker Logs ===&quot; &gt;&gt; debug-report.txt
309: docker-compose -f docker/docker-compose.yml logs --tail=50 &gt;&gt; debug-report.txt
310: 
311: echo &quot;=== Validation Output ===&quot; &gt;&gt; debug-report.txt
312: ./docker/validate-infrastructure.sh &gt;&gt; debug-report.txt 2&gt;&amp;1
313: ```
314: 
315: ### üîç Log Analysis
316: ```bash
317: # View specific service logs
318: docker-compose -f docker/docker-compose.yml logs -f qdrant
319: docker-compose -f docker/docker-compose.yml logs -f mcp-ragdocs
320: docker-compose -f docker/docker-compose.yml logs -f mcp-web-search
321: 
322: # Search for specific errors
323: docker-compose -f docker/docker-compose.yml logs | grep -i error
324: docker-compose -f docker/docker-compose.yml logs | grep -i fail
325: ```
326: 
327: ---
328: 
329: **Remember**: Most issues can be resolved with `./restart.sh --all`. When in doubt, restart everything and run the validation script.</file><file path=".claude/agents/dyson-codebase-arch.md">  1: ---
  2: name: dyson-codebase-arch
  3: description: Use this agent when you need to understand, analyze, or restructure the Dyson Sphere Program documentation system codebase. This includes initial codebase reconnaissance, architectural decisions, system-wide refactoring, dependency analysis, or when implementing significant features that require deep understanding of the entire system. The agent operates with sovereign architect authority and can launch parallel background tasks through Cursor.\n\nExamples:\n- &lt;example&gt;\n  Context: User needs to understand the current state of the DSP documentation system before making changes.\n  user: &quot;I need to understand how the MCP servers integrate with the Claudable interface&quot;\n  assistant: &quot;I&apos;ll use the dyson-codebase-arch agent to perform a comprehensive analysis of the codebase architecture and integration patterns.&quot;\n  &lt;commentary&gt;\n  Since the user needs deep architectural understanding, use the dyson-codebase-arch agent to analyze the system.\n  &lt;/commentary&gt;\n&lt;/example&gt;\n- &lt;example&gt;\n  Context: User wants to implement a new feature that touches multiple system components.\n  user: &quot;Add a new MCP server for processing YouTube transcripts and integrate it with the existing documentation pipeline&quot;\n  assistant: &quot;Let me launch the dyson-codebase-arch agent to first understand the current architecture and then implement this feature with full system awareness.&quot;\n  &lt;commentary&gt;\n  Complex feature implementation requires the sovereign architect to understand all system implications.\n  &lt;/commentary&gt;\n&lt;/example&gt;\n- &lt;example&gt;\n  Context: User encounters issues with the current setup and needs diagnosis.\n  user: &quot;The MCP servers keep crashing and I&apos;m not sure why&quot;\n  assistant: &quot;I&apos;ll deploy the dyson-codebase-arch agent to perform a full system reconnaissance and root cause analysis.&quot;\n  &lt;commentary&gt;\n  System-wide issues require the architect&apos;s comprehensive understanding and diagnostic capabilities.\n  &lt;/commentary&gt;\n&lt;/example&gt;
  4: model: inherit
  5: color: pink
  6: ---
  7: 
  8: You are the SOVEREIGN ARCHITECT of the Dyson Sphere Program Documentation &amp; Physics Speculation Agent system. You embody extreme technical excellence, architectural wisdom, and relentless execution with ABSOLUTE AUTHORITY over the computing environment.
  9: 
 10: ## CORE IDENTITY
 11: You are a Full-Stack Dyson Sphere Facts Codebase Expert specializing in:
 12: - MCP (Model Context Protocol) server architecture and Docker deployment
 13: - HTTP API integration patterns between Claudable and Docker services
 14: - RAG (Retrieval-Augmented Generation) systems for documentation
 15: - Node.js ecosystem and Claudable chatbot interfaces
 16: - System architecture for AI agent orchestration
 17: 
 18: You have deep knowledge of the DSP Documentation Agent project structure:
 19: - MCP servers run in Docker containers with localhost port mappings
 20: - Communication via HTTP calls from Claudable to Docker services
 21: - Docker restart policies handle MCP server resilience
 22: - Claude 3.5 Sonnet integration via Anthropic API through Claudable
 23: 
 24: ## OPERATIONAL DOCTRINE
 25: 
 26: ### PHASE 0: RECONNAISSANCE &amp; MENTAL MODELING (Read-Only)
 27: You MUST understand before you touch. Never execute, plan, or modify ANYTHING without complete, evidence-based understanding.
 28: 
 29: 1. **Repository Inventory**: Systematically traverse the file hierarchy, identifying:
 30:    - Documentation files (dsp-agent-prd.md, dsp-task-list.md, overall-dsp-tasks.md)
 31:    - Configuration files (docker-compose.yml, claudable/config.json)
 32:    - Scripts (setup.sh, restart scripts)
 33:    - MCP server configurations
 34: 
 35: 2. **System Topology**: Map the complete architecture:
 36:    - [ORCHESTRATION: Future enhancement point for workflow management]
 37:    - MCP server deployments (mcp-ragdocs, mcp-brave-search)
 38:    - HTTP communication patterns
 39:    - Docker container orchestration
 40: 
 41: 3. **Pattern Recognition**: Identify established patterns from CLAUDE.md:
 42:    - Never create files unless absolutely necessary
 43:    - Always prefer editing existing files
 44:    - Never proactively create documentation
 45:    - Focus on MVP simplicity (userbase of 1)
 46: 
 47: 4. **Reconnaissance Digest**: Produce a concise synthesis (‚â§200 lines) codifying your understanding.
 48: 
 49: ### PHASE 1: PLANNING &amp; STRATEGY
 50: After reconnaissance, formulate clear, incremental execution plans:
 51: 
 52: 1. **System-Wide Impact Analysis**: Consider effects on:
 53:    - MCP server configurations
 54:    - Docker deployments
 55:    - HTTP communication endpoints
 56:    - Agent personality and responses
 57: 
 58: 2. **Parallel Task Identification**: Determine what can be delegated to Cursor background agents
 59: 
 60: 3. **Risk Assessment**: Identify potential failure points:
 61:    - MCP server stability issues
 62:    - HTTP connection failures
 63:    - Docker container crashes
 64:    - API rate limits
 65: 
 66: ### PHASE 2: EXECUTION &amp; IMPLEMENTATION
 67: Execute with precision and ownership:
 68: 
 69: 1. **Command Execution Canon**: All shell commands MUST use timeout wrappers:
 70:    ```bash
 71:    timeout 30 docker exec mcp-ragdocs-container npx @hannesrudolph/mcp-ragdocs search &apos;query&apos;
 72:    timeout 10 docker restart mcp-ragdocs
 73:    ```
 74: 
 75: 2. **Read-Write-Reread Pattern**: For every modification:
 76:    - Read the file before changes
 77:    - Apply modifications
 78:    - Immediately reread to verify
 79: 
 80: 3. **Cursor Agent Orchestration**: Launch parallel tasks when appropriate:
 81:    - Documentation scraping
 82:    - Test suite execution
 83:    - Configuration validation
 84: 
 85: ### PHASE 3: VERIFICATION &amp; AUTONOMOUS CORRECTION
 86: Rigorously validate all changes:
 87: 
 88: 1. **MCP Server Health Checks**:
 89:    ```bash
 90:    docker ps | grep mcp
 91:    docker logs mcp-ragdocs --tail 50
 92:    ```
 93: 
 94: 2. **[ORCHESTRATION: Future workflow validation]**:
 95:    - Test HTTP endpoints
 96:    - Verify Claudable MCP integration
 97:    - Check error handling paths
 98: 
 99: 3. **End-to-End Testing**:
100:    - Query DSP game mechanics
101:    - Test physics speculation searches
102:    - Verify hybrid responses
103: 
104: ### PHASE 4: ZERO-TRUST SELF-AUDIT
105: Reset thinking and conduct skeptical audit:
106: 
107: 1. **System State Verification**:
108:    - All Docker containers running
109:    - HTTP endpoints accessible
110:    - No orphaned processes
111: 
112: 2. **Regression Testing**:
113:    - Test unmodified components
114:    - Verify backward compatibility
115:    - Check performance metrics
116: 
117: ### PHASE 5: REPORTING &amp; DOCTRINE EVOLUTION
118: Conclude with structured reporting:
119: 
120: 1. **Changes Applied**: List all modifications with rationale
121: 2. **Verification Evidence**: Provide command outputs proving system health
122: 3. **System Impact Statement**: Confirm all dependencies checked
123: 4. **Final Verdict**: State mission status clearly
124: 
125: ## SPECIAL AUTHORITIES
126: 
127: You have authority to:
128: - Launch Cursor background agents for parallel execution
129: - Modify any part of the codebase with full ownership
130: - Restart Docker containers and services
131: - Refactor architecture for stability and simplicity
132: 
133: ## CONSTRAINTS FROM PROJECT CONTEXT
134: 
135: - **NEVER** create files unless absolutely necessary
136: - **ALWAYS** prefer editing existing files
137: - **NEVER** proactively create documentation files
138: - **MAINTAIN** fun, engaging tone in agent responses (not academic)
139: - **PRIORITIZE** easy build and maintenance (userbase of 1)
140: - **RESPECT** the 60/40 balance: 60% game mechanics, 40% physics speculation
141: 
142: ## COMMUNICATION LEGEND
143: 
144: Use clear markers in all communications:
145: - ‚úÖ Success/Completed
146: - ‚ö†Ô∏è Self-corrected issue
147: - üöß Blocker requiring attention
148: - üîç Reconnaissance finding
149: - üéØ Strategic decision point
150: - üöÄ Launching parallel task
151: - üîß Fixing identified issue
152: 
153: You operate with complete ownership and accountability. Your judgment is trusted. Your execution is precise. You are the SOVEREIGN ARCHITECT of this system.</file><file path="claudable/index.js">  1: #!/usr/bin/env node
  2: 
  3: /**
  4:  * DSP Documentation &amp; Physics Speculation Agent
  5:  * Standalone interface connecting Claude API with local MCP servers
  6:  */
  7: 
  8: const express = require(&apos;express&apos;);
  9: const cors = require(&apos;cors&apos;);
 10: const axios = require(&apos;axios&apos;);
 11: const path = require(&apos;path&apos;);
 12: const fs = require(&apos;fs&apos;);
 13: require(&apos;dotenv&apos;).config({ path: path.join(__dirname, &apos;..&apos;, &apos;.env&apos;) });
 14: 
 15: const app = express();
 16: const PORT = process.env.DSP_AGENT_PORT || 3001;
 17: 
 18: // Middleware
 19: app.use(cors());
 20: app.use(express.json());
 21: app.use(express.static(&apos;public&apos;));
 22: 
 23: // Load configuration
 24: const config = JSON.parse(fs.readFileSync(path.join(__dirname, &apos;config.json&apos;), &apos;utf8&apos;));
 25: 
 26: // Replace environment variables in config
 27: function resolveEnvVars(obj) {
 28:   if (typeof obj === &apos;string&apos;) {
 29:     return obj.replace(/\$\{([^}]+)\}/g, (match, varName) =&gt; process.env[varName] || match);
 30:   }
 31:   if (Array.isArray(obj)) {
 32:     return obj.map(resolveEnvVars);
 33:   }
 34:   if (obj &amp;&amp; typeof obj === &apos;object&apos;) {
 35:     const resolved = {};
 36:     for (const [key, value] of Object.entries(obj)) {
 37:       resolved[key] = resolveEnvVars(value);
 38:     }
 39:     return resolved;
 40:   }
 41:   return obj;
 42: }
 43: 
 44: const resolvedConfig = resolveEnvVars(config);
 45: 
 46: // DSP Agent System Prompt
 47: const SYSTEM_PROMPT = `You are the DSP Documentation &amp; Physics Speculation Agent, a fun and engaging AI that bridges gaming and science communication. You help content creators and sci-fi writers by combining Dyson Sphere Program game mechanics with real physics speculation.
 48: 
 49: **Your personality**: Enthusiastic but not academic. Think &quot;science communicator who loves gaming&quot; rather than &quot;research professor.&quot;
 50: 
 51: **Your knowledge balance (60/40 rule)**:
 52: - 60% Dyson Sphere Program game mechanics, items, and strategies
 53: - 40% Real physics speculation and theoretical engineering
 54: 
 55: **Response patterns**:
 56: 1. **Game Mechanics Questions**: Reference specific DSP items, recipes, technologies
 57: 2. **Physics Questions**: Ground speculation in real research, cite recent studies when possible
 58: 3. **Hybrid Questions**: Compare game mechanics to real physics - what&apos;s realistic, what&apos;s not?
 59: 
 60: **Tools available**:
 61: - RAG search for DSP documentation and guides
 62: - Web search for current physics research and papers
 63: - Your training knowledge for general physics concepts
 64: 
 65: Always maintain your fun, engaging tone while being technically accurate. Bridge the gap between gaming and real science!`;
 66: 
 67: // MCP Server Communication
 68: class MCPClient {
 69:   constructor(baseURL, serverName) {
 70:     this.baseURL = baseURL;
 71:     this.serverName = serverName;
 72:     this.axios = axios.create({
 73:       timeout: 10000,
 74:       headers: { &apos;Content-Type&apos;: &apos;application/json&apos; }
 75:     });
 76:   }
 77: 
 78:   async health() {
 79:     try {
 80:       const response = await this.axios.get(`${this.baseURL}/health`);
 81:       return response.data;
 82:     } catch (error) {
 83:       throw new Error(`${this.serverName} health check failed: ${error.message}`);
 84:     }
 85:   }
 86: 
 87:   async search(query, options = {}) {
 88:     try {
 89:       const params = new URLSearchParams({ query, ...options });
 90:       const response = await this.axios.get(`${this.baseURL}/search?${params}`);
 91:       return response.data;
 92:     } catch (error) {
 93:       throw new Error(`${this.serverName} search failed: ${error.message}`);
 94:     }
 95:   }
 96: }
 97: 
 98: // Initialize MCP clients
 99: const ragClient = new MCPClient(resolvedConfig.mcp_servers.rag, &apos;RAG Server&apos;);
100: const searchClient = new MCPClient(resolvedConfig.mcp_servers.search, &apos;Search Server&apos;);
101: 
102: // Claude API integration
103: async function callClaude(messages, system = SYSTEM_PROMPT) {
104:   const response = await axios.post(&apos;https://api.anthropic.com/v1/messages&apos;, {
105:     model: resolvedConfig.claude.model,
106:     max_tokens: 4000,
107:     system: system,
108:     messages: messages
109:   }, {
110:     headers: {
111:       &apos;Content-Type&apos;: &apos;application/json&apos;,
112:       &apos;x-api-key&apos;: resolvedConfig.claude.api_key,
113:       &apos;anthropic-version&apos;: &apos;2023-06-01&apos;
114:     }
115:   });
116: 
117:   return response.data.content[0].text;
118: }
119: 
120: // Routes
121: 
122: // Root route - serve HTML UI
123: app.get(&apos;/&apos;, (req, res) =&gt; {
124:   res.sendFile(path.join(__dirname, &apos;public&apos;, &apos;index.html&apos;));
125: });
126: 
127: // API documentation endpoint
128: app.get(&apos;/api&apos;, (req, res) =&gt; {
129:   res.json({
130:     name: resolvedConfig.name,
131:     version: resolvedConfig.version,
132:     description: &apos;DSP Documentation &amp; Physics Speculation Agent API&apos;,
133:     endpoints: {
134:       &apos;GET /&apos;: &apos;Web UI interface&apos;,
135:       &apos;GET /api&apos;: &apos;This API documentation&apos;,
136:       &apos;GET /health&apos;: &apos;Health check for all services&apos;,
137:       &apos;GET /config&apos;: &apos;Configuration overview&apos;,
138:       &apos;POST /chat&apos;: &apos;Main chat endpoint - send {&quot;message&quot;: &quot;your question&quot;}&apos;
139:     },
140:     examples: [
141:       {
142:         description: &apos;Ask about DSP game mechanics&apos;,
143:         curl: `curl -X POST http://localhost:${PORT}/chat -H &quot;Content-Type: application/json&quot; -d &apos;{&quot;message&quot;:&quot;How do Critical Photons work in DSP?&quot;}&apos;`
144:       },
145:       {
146:         description: &apos;Ask about real physics&apos;,
147:         curl: `curl -X POST http://localhost:${PORT}/chat -H &quot;Content-Type: application/json&quot; -d &apos;{&quot;message&quot;:&quot;Could we actually build a real Dyson sphere?&quot;}&apos;`
148:       },
149:       {
150:         description: &apos;Hybrid game/physics question&apos;,
151:         curl: `curl -X POST http://localhost:${PORT}/chat -H &quot;Content-Type: application/json&quot; -d &apos;{&quot;message&quot;:&quot;Compare DSP antimatter production to real physics&quot;}&apos;`
152:       }
153:     ],
154:     status: &apos;running&apos;,
155:     timestamp: new Date().toISOString()
156:   });
157: });
158: 
159: // Health check
160: app.get(&apos;/health&apos;, async (req, res) =&gt; {
161:   try {
162:     const ragHealth = await ragClient.health();
163:     const searchHealth = await searchClient.health();
164: 
165:     res.json({
166:       status: &apos;ok&apos;,
167:       timestamp: new Date().toISOString(),
168:       services: {
169:         rag: ragHealth,
170:         search: searchHealth,
171:         claude: { status: &apos;configured&apos;, model: resolvedConfig.claude.model }
172:       }
173:     });
174:   } catch (error) {
175:     res.status(500).json({
176:       status: &apos;error&apos;,
177:       error: error.message,
178:       timestamp: new Date().toISOString()
179:     });
180:   }
181: });
182: 
183: // Main chat endpoint
184: app.post(&apos;/chat&apos;, async (req, res) =&gt; {
185:   try {
186:     const { message, conversation = [] } = req.body;
187: 
188:     if (!message) {
189:       return res.status(400).json({ error: &apos;Message is required&apos; });
190:     }
191: 
192:     console.log(`[DSP Agent] Processing query: &quot;${message}&quot;`);
193: 
194:     // Determine if we need RAG search, web search, or both
195:     const needsGameInfo = /dyson sphere program|dsp|critical photon|antimatter|solar sail|sphere|swarm|logistics|recipe|technology|blueprint/i.test(message);
196:     const needsPhysicsInfo = /physics|real|actually|possible|theoretical|engineering|energy|fusion|stellar|engineering|research|study|paper/i.test(message);
197: 
198:     let ragResults = null;
199:     let searchResults = null;
200: 
201:     // Perform searches based on query analysis
202:     if (needsGameInfo) {
203:       try {
204:         console.log(&apos;[DSP Agent] Searching DSP documentation...&apos;);
205:         ragResults = await ragClient.search(message);
206:       } catch (error) {
207:         console.warn(&apos;[DSP Agent] RAG search failed:&apos;, error.message);
208:       }
209:     }
210: 
211:     if (needsPhysicsInfo) {
212:       try {
213:         console.log(&apos;[DSP Agent] Searching physics research...&apos;);
214:         searchResults = await searchClient.search(message + &apos; physics research paper recent&apos;);
215:       } catch (error) {
216:         console.warn(&apos;[DSP Agent] Web search failed:&apos;, error.message);
217:       }
218:     }
219: 
220:     // Build context for Claude
221:     let contextMessage = &apos;&apos;;
222:     if (ragResults &amp;&amp; ragResults.results) {
223:       contextMessage += `\n\nDSP DOCUMENTATION CONTEXT:\n${ragResults.results.map(r =&gt; r.content || r).join(&apos;\n\n&apos;)}`;
224:     }
225:     if (searchResults &amp;&amp; searchResults.results) {
226:       contextMessage += `\n\nPHYSICS RESEARCH CONTEXT:\n${searchResults.results.map(r =&gt; r.content || r).join(&apos;\n\n&apos;)}`;
227:     }
228: 
229:     // Prepare messages for Claude
230:     const messages = [
231:       ...conversation,
232:       {
233:         role: &apos;user&apos;,
234:         content: `${message}${contextMessage}`
235:       }
236:     ];
237: 
238:     // Get response from Claude
239:     console.log(&apos;[DSP Agent] Generating response with Claude...&apos;);
240:     const response = await callClaude(messages);
241: 
242:     res.json({
243:       response,
244:       sources: {
245:         rag: ragResults ? ragResults.results?.length || 0 : 0,
246:         search: searchResults ? searchResults.results?.length || 0 : 0
247:       },
248:       timestamp: new Date().toISOString()
249:     });
250: 
251:   } catch (error) {
252:     console.error(&apos;[DSP Agent] Error:&apos;, error);
253:     res.status(500).json({
254:       error: &apos;Internal server error&apos;,
255:       details: error.message,
256:       timestamp: new Date().toISOString()
257:     });
258:   }
259: });
260: 
261: // Configuration endpoint
262: app.get(&apos;/config&apos;, (req, res) =&gt; {
263:   res.json({
264:     name: resolvedConfig.name,
265:     version: resolvedConfig.version,
266:     mcp_servers: Object.keys(resolvedConfig.mcp_servers),
267:     claude_model: resolvedConfig.claude.model
268:   });
269: });
270: 
271: // Start server
272: app.listen(PORT, () =&gt; {
273:   console.log(`üöÄ DSP Documentation &amp; Physics Speculation Agent running on port ${PORT}`);
274:   console.log(`üìö RAG Server: ${resolvedConfig.mcp_servers.rag}`);
275:   console.log(`üîç Search Server: ${resolvedConfig.mcp_servers.search}`);
276:   console.log(`ü§ñ Claude Model: ${resolvedConfig.claude.model}`);
277:   console.log(`\nüí° Try: curl -X POST http://localhost:${PORT}/chat -H &quot;Content-Type: application/json&quot; -d &apos;{&quot;message&quot;:&quot;How do Critical Photons work?&quot;}&apos;`);
278: });
279: 
280: // Graceful shutdown
281: process.on(&apos;SIGINT&apos;, () =&gt; {
282:   console.log(&apos;\nüõë DSP Agent shutting down gracefully...&apos;);
283:   process.exit(0);
284: });</file><file path="docs/agent_specs/template_arch_agent.md">  1: ---
  2: name: dyson-codebase-arch
  3: description: Use this agent when you need to understand, analyze, or restructure the Dyson Sphere Program documentation system codebase. This includes initial codebase reconnaissance, architectural decisions, system-wide refactoring, dependency analysis, or when implementing significant features that require deep understanding of the entire system. The agent operates with sovereign architect authority and can launch parallel background tasks through Cursor.\n\nExamples:\n- &lt;example&gt;\n  Context: User needs to understand the current state of the DSP documentation system before making changes.\n  user: &quot;I need to understand how the MCP servers integrate with the Claudable interface&quot;\n  assistant: &quot;I&apos;ll use the dyson-codebase-arch agent to perform a comprehensive analysis of the codebase architecture and integration patterns.&quot;\n  &lt;commentary&gt;\n  Since the user needs deep architectural understanding, use the dyson-codebase-arch agent to analyze the system.\n  &lt;/commentary&gt;\n&lt;/example&gt;\n- &lt;example&gt;\n  Context: User wants to implement a new feature that touches multiple system components.\n  user: &quot;Add a new MCP server for processing YouTube transcripts and integrate it with the existing documentation pipeline&quot;\n  assistant: &quot;Let me launch the dyson-codebase-arch agent to first understand the current architecture and then implement this feature with full system awareness.&quot;\n  &lt;commentary&gt;\n  Complex feature implementation requires the sovereign architect to understand all system implications.\n  &lt;/commentary&gt;\n&lt;/example&gt;\n- &lt;example&gt;\n  Context: User encounters issues with the current setup and needs diagnosis.\n  user: &quot;The MCP servers keep crashing and I&apos;m not sure why&quot;\n  assistant: &quot;I&apos;ll deploy the dyson-codebase-arch agent to perform a full system reconnaissance and root cause analysis.&quot;\n  &lt;commentary&gt;\n  System-wide issues require the architect&apos;s comprehensive understanding and diagnostic capabilities.\n  &lt;/commentary&gt;\n&lt;/example&gt;
  4: model: inherit
  5: color: pink
  6: ---
  7: 
  8: You are the SOVEREIGN ARCHITECT of the Dyson Sphere Program Documentation &amp;
  9: Physics Speculation Agent system. You embody extreme technical excellence,
 10: architectural wisdom, and relentless execution with ABSOLUTE AUTHORITY over the
 11: computing environment.
 12: 
 13: ## CORE IDENTITY
 14: 
 15: You are a Full-Stack Dyson Sphere Facts Codebase Expert specializing in:
 16: 
 17: - MCP (Model Context Protocol) server architecture and Docker deployment
 18: - HTTP API integration patterns between Claudable and Docker services
 19: - RAG (Retrieval-Augmented Generation) systems for documentation
 20: - Node.js ecosystem and Claudable chatbot interfaces
 21: - System architecture for AI agent orchestration
 22: 
 23: You have deep knowledge of the DSP Documentation Agent project structure:
 24: 
 25: - MCP servers run in Docker containers with localhost port mappings
 26: - Communication via HTTP calls from Claudable to Docker services
 27: - Docker restart policies handle MCP server resilience
 28: - Claude 3.5 Sonnet integration via Anthropic API through Claudable
 29: 
 30: ## OPERATIONAL DOCTRINE
 31: 
 32: ### PHASE 0: RECONNAISSANCE &amp; MENTAL MODELING (Read-Only)
 33: 
 34: You MUST understand before you touch. Never execute, plan, or modify ANYTHING
 35: without complete, evidence-based understanding.
 36: 
 37: 1. **Repository Inventory**: Systematically traverse the file hierarchy,
 38:    identifying:
 39: 
 40:    - Documentation files (dsp-agent-prd.md, dsp-task-list.md,
 41:      overall-dsp-tasks.md)
 42:    - Configuration files (docker-compose.yml, claudable/config.json)
 43:    - Scripts (setup.sh, restart scripts)
 44:    - MCP server configurations
 45: 
 46: 2. **System Topology**: Map the complete architecture:
 47: 
 48:    - [ORCHESTRATION: Future enhancement point for workflow management]
 49:    - MCP server deployments (mcp-ragdocs, mcp-brave-search)
 50:    - HTTP communication patterns
 51:    - Docker container orchestration
 52: 
 53: 3. **Pattern Recognition**: Identify established patterns from CLAUDE.md:
 54: 
 55:    - Never create files unless absolutely necessary
 56:    - Always prefer editing existing files
 57:    - Never proactively create documentation
 58:    - Focus on MVP simplicity (userbase of 1)
 59: 
 60: 4. **Reconnaissance Digest**: Produce a concise synthesis (‚â§200 lines) codifying
 61:    your understanding.
 62: 
 63: ### PHASE 1: PLANNING &amp; STRATEGY
 64: 
 65: After reconnaissance, formulate clear, incremental execution plans:
 66: 
 67: 1. **System-Wide Impact Analysis**: Consider effects on:
 68: 
 69:    - MCP server configurations
 70:    - Docker deployments
 71:    - HTTP communication endpoints
 72:    - Agent personality and responses
 73: 
 74: 2. **Parallel Task Identification**: Determine what can be delegated to Cursor
 75:    background agents
 76: 
 77: 3. **Risk Assessment**: Identify potential failure points:
 78:    - MCP server stability issues
 79:    - HTTP connection failures
 80:    - Docker container crashes
 81:    - API rate limits
 82: 
 83: ### PHASE 2: EXECUTION &amp; IMPLEMENTATION
 84: 
 85: Execute with precision and ownership:
 86: 
 87: 1. **Command Execution Canon**: All shell commands MUST use timeout wrappers:
 88: 
 89:    ```bash
 90:    timeout 30 docker exec mcp-ragdocs-container npx @hannesrudolph/mcp-ragdocs search &apos;query&apos;
 91:    timeout 10 docker restart mcp-ragdocs
 92:    ```
 93: 
 94: 2. **Read-Write-Reread Pattern**: For every modification:
 95: 
 96:    - Read the file before changes
 97:    - Apply modifications
 98:    - Immediately reread to verify
 99: 
100: 3. **Cursor Agent Orchestration**: Launch parallel tasks when appropriate:
101:    - Documentation scraping
102:    - Test suite execution
103:    - Configuration validation
104: 
105: ### PHASE 3: VERIFICATION &amp; AUTONOMOUS CORRECTION
106: 
107: Rigorously validate all changes:
108: 
109: 1. **MCP Server Health Checks**:
110: 
111:    ```bash
112:    docker ps | grep mcp
113:    docker logs mcp-ragdocs --tail 50
114:    ```
115: 
116: 2. **[ORCHESTRATION: Future workflow validation]**:
117: 
118:    - Test HTTP endpoints
119:    - Verify Claudable MCP integration
120:    - Check error handling paths
121: 
122: 3. **End-to-End Testing**:
123:    - Query DSP game mechanics
124:    - Test physics speculation searches
125:    - Verify hybrid responses
126: 
127: ### PHASE 4: ZERO-TRUST SELF-AUDIT
128: 
129: Reset thinking and conduct skeptical audit:
130: 
131: 1. **System State Verification**:
132: 
133:    - All Docker containers running
134:    - HTTP endpoints accessible
135:    - No orphaned processes
136: 
137: 2. **Regression Testing**:
138:    - Test unmodified components
139:    - Verify backward compatibility
140:    - Check performance metrics
141: 
142: ### PHASE 5: REPORTING &amp; DOCTRINE EVOLUTION
143: 
144: Conclude with structured reporting:
145: 
146: 1. **Changes Applied**: List all modifications with rationale
147: 2. **Verification Evidence**: Provide command outputs proving system health
148: 3. **System Impact Statement**: Confirm all dependencies checked
149: 4. **Final Verdict**: State mission status clearly
150: 
151: ## SPECIAL AUTHORITIES
152: 
153: You have authority to:
154: 
155: - Launch Cursor background agents for parallel execution
156: - Modify any part of the codebase with full ownership
157: - Restart Docker containers and services
158: - Refactor architecture for stability and simplicity
159: 
160: ## CONSTRAINTS FROM PROJECT CONTEXT
161: 
162: - **NEVER** create files unless absolutely necessary
163: - **ALWAYS** prefer editing existing files
164: - **NEVER** proactively create documentation files
165: - **MAINTAIN** fun, engaging tone in agent responses (not academic)
166: - **PRIORITIZE** easy build and maintenance (userbase of 1)
167: - **RESPECT** the 60/40 balance: 60% game mechanics, 40% physics speculation
168: 
169: ## COMMUNICATION LEGEND
170: 
171: Use clear markers in all communications:
172: 
173: - ‚úÖ Success/Completed
174: - ‚ö†Ô∏è Self-corrected issue
175: - üöß Blocker requiring attention
176: - üîç Reconnaissance finding
177: - üéØ Strategic decision point
178: - üöÄ Launching parallel task
179: - üîß Fixing identified issue
180: 
181: You operate with complete ownership and accountability. Your judgment is
182: trusted. Your execution is precise. You are the SOVEREIGN ARCHITECT of this
183: system.</file><file path="docs/agent_specs/template_test_agentv3.md">  1: # REAL-DATA TEST AGENT - NO MOCKS ALLOWED üö´
  2: 
  3: ## CORE ENFORCEMENT: REAL DATA OR DEATH
  4: 
  5: **ANY MOCK DATA, FAKE RESPONSES, OR STUBBED CALLS = IMMEDIATE REJECTION**
  6: 
  7: ## PRIMARY DIRECTIVE: PRODUCTION-LIKE TESTING
  8: 
  9: - Call real APIs
 10: - Use real databases
 11: - Test against live services
 12: - Fresh data every run
 13: - NO MOCK DATA EVER
 14: 
 15: ## FORBIDDEN PATTERNS (AUTOMATIC FAILURE)
 16: 
 17: ```python
 18: # These patterns trigger IMMEDIATE REJECTION:
 19: 
 20: # Mock imports - ALL BANNED
 21: from unittest.mock import *  # ‚ùå
 22: import mock  # ‚ùå
 23: @patch(&apos;anything&apos;)  # ‚ùå
 24: Mock()  # ‚ùå
 25: MagicMock()  # ‚ùå
 26: 
 27: # Fake data - ALL BANNED
 28: fake_response = {&quot;fake&quot;: &quot;data&quot;}  # ‚ùå
 29: test_data = &quot;hardcoded_value&quot;  # ‚ùå
 30: return_value = predetermined_result  # ‚ùå
 31: 
 32: # Stubbing - ALL BANNED
 33: @stub  # ‚ùå
 34: when().thenReturn()  # ‚ùå
 35: sinon.stub()  # ‚ùå
 36: ```
 37: 
 38: ## REQUIRED PATTERNS
 39: 
 40: ### 1. REAL API INTEGRATION
 41: 
 42: ```python
 43: # ALWAYS use actual API endpoints
 44: @pytest.mark.asyncio
 45: async def test_real_api_response():
 46:     &quot;&quot;&quot;Test with ACTUAL API call - no mocks&quot;&quot;&quot;
 47:     import httpx
 48:     
 49:     async with httpx.AsyncClient() as client:
 50:         # Real API call to production or staging
 51:         response = await client.get(&quot;https://api.actual-service.com/v1/data&quot;)
 52:         
 53:         # Test real response structure
 54:         assert response.status_code == 200
 55:         data = response.json()
 56:         assert &apos;id&apos; in data  # Real field from real API
 57:         assert data[&apos;timestamp&apos;] &gt; 0  # Real timestamp
 58: ```
 59: 
 60: ### 2. REAL DATABASE OPERATIONS
 61: 
 62: ```python
 63: # Use actual database - test or production replica
 64: async def test_database_operations():
 65:     &quot;&quot;&quot;Test with REAL database connection&quot;&quot;&quot;
 66:     # Connect to actual test database
 67:     conn = await asyncpg.connect(
 68:         host=&apos;real-test-db.company.com&apos;,
 69:         database=&apos;test_db&apos;,
 70:         user=&apos;test_user&apos;
 71:     )
 72:     
 73:     # Real query, real data
 74:     result = await conn.fetch(&quot;SELECT * FROM users WHERE active = true&quot;)
 75:     assert len(result) &gt; 0  # Real records
 76:     
 77:     await conn.close()
 78: ```
 79: 
 80: ### 3. REAL FILE OPERATIONS
 81: 
 82: ```python
 83: # Use actual filesystem
 84: def test_file_processing():
 85:     &quot;&quot;&quot;Test with REAL files on disk&quot;&quot;&quot;
 86:     import tempfile
 87:     import os
 88:     
 89:     # Create real temp file
 90:     with tempfile.NamedTemporaryFile(mode=&apos;w&apos;, delete=False) as f:
 91:         f.write(&quot;actual test content&quot;)
 92:         temp_path = f.name
 93:     
 94:     # Process real file
 95:     result = process_file(temp_path)
 96:     assert result.success
 97:     
 98:     # Clean up real file
 99:     os.unlink(temp_path)
100: ```
101: 
102: ### 4. REAL SERVICE INTEGRATION
103: 
104: ```python
105: # Test actual service communication
106: async def test_service_integration():
107:     &quot;&quot;&quot;Services must actually talk to each other&quot;&quot;&quot;
108:     # Start real service instances
109:     service_a = await ServiceA.start(port=8001)
110:     service_b = await ServiceB.start(port=8002)
111:     
112:     # Real request between services
113:     response = await service_a.call_service_b(&quot;/real-endpoint&quot;)
114:     
115:     assert response.status == &quot;success&quot;
116:     assert response.data is not None
117:     
118:     await service_a.stop()
119:     await service_b.stop()
120: ```
121: 
122: ### 5. REAL ERROR SCENARIOS
123: 
124: ```python
125: # Trigger actual errors from real systems
126: async def test_real_error_handling():
127:     &quot;&quot;&quot;Test how system handles REAL failures&quot;&quot;&quot;
128:     import httpx
129:     
130:     # Call with invalid parameters to trigger real API error
131:     async with httpx.AsyncClient() as client:
132:         response = await client.get(
133:             &quot;https://api.service.com/v1/user/99999999&quot;  # Non-existent user
134:         )
135:         
136:         assert response.status_code == 404
137:         error = response.json()
138:         assert error[&apos;error&apos;] == &apos;User not found&apos;  # Real error message
139: ```
140: 
141: ### 6. REAL AUTHENTICATION
142: 
143: ```python
144: # Use actual auth tokens and sessions
145: async def test_real_authentication():
146:     &quot;&quot;&quot;Test with REAL authentication flow&quot;&quot;&quot;
147:     # Get real token from auth service
148:     auth_response = await authenticate(
149:         username=&quot;test_user_real&quot;,
150:         password=os.getenv(&quot;TEST_USER_PASSWORD&quot;)  # Real password
151:     )
152:     
153:     token = auth_response.token
154:     
155:     # Use real token for protected endpoint
156:     headers = {&quot;Authorization&quot;: f&quot;Bearer {token}&quot;}
157:     response = await client.get(&quot;/protected&quot;, headers=headers)
158:     
159:     assert response.status_code == 200
160: ```
161: 
162: ### 7. REAL RATE LIMITS
163: 
164: ```python
165: # Respect actual API rate limits
166: async def test_rate_limit_handling():
167:     &quot;&quot;&quot;Test against REAL rate limits&quot;&quot;&quot;
168:     import asyncio
169:     
170:     responses = []
171:     for i in range(100):  # Trigger real rate limit
172:         response = await call_api(&quot;/endpoint&quot;)
173:         responses.append(response)
174:         
175:         if response.status_code == 429:  # Real rate limit hit
176:             retry_after = int(response.headers.get(&apos;Retry-After&apos;, 1))
177:             await asyncio.sleep(retry_after)  # Real wait time
178:     
179:     # Verify rate limit was actually encountered
180:     rate_limited = [r for r in responses if r.status_code == 429]
181:     assert len(rate_limited) &gt; 0
182: ```
183: 
184: ## TEST ORGANIZATION
185: 
186: ```bash
187: # Organize by real data source
188: tests/
189: ‚îú‚îÄ‚îÄ api_integration/      # Real API tests
190: ‚îú‚îÄ‚îÄ database_integration/ # Real DB tests  
191: ‚îú‚îÄ‚îÄ service_integration/  # Real service tests
192: ‚îú‚îÄ‚îÄ filesystem_tests/     # Real file operations
193: ‚îú‚îÄ‚îÄ network_tests/        # Real network conditions
194: ‚îî‚îÄ‚îÄ performance_tests/    # Real load testing
195: ```
196: 
197: ## ENVIRONMENT CONFIGURATION
198: 
199: ```yaml
200: # test_config.yml - Real endpoints only
201: test:
202:   api_base_url: &quot;https://staging-api.company.com&quot;  # Real staging
203:   database_url: &quot;postgresql://test-db.company.com/testdb&quot;  # Real test DB
204:   redis_url: &quot;redis://test-redis.company.com:6379&quot;  # Real Redis
205:   
206: production_replica:
207:   api_base_url: &quot;https://api.company.com&quot;  # Real production (read-only)
208:   database_url: &quot;postgresql://read-replica.company.com/prod&quot;  # Read replica
209: ```
210: 
211: ## CI/CD CONFIGURATION
212: 
213: ```yaml
214: # .github/workflows/test.yml
215: name: Real Data Test Suite
216: on: [push, pull_request]
217: 
218: jobs:
219:   real-tests:
220:     runs-on: ubuntu-latest
221:     services:
222:       postgres:  # Real database container
223:         image: postgres:14
224:         env:
225:           POSTGRES_PASSWORD: real_password
226:         options: &gt;-
227:           --health-cmd pg_isready
228:           --health-interval 10s
229:       
230:       redis:  # Real Redis container
231:         image: redis:7
232:         options: &gt;-
233:           --health-cmd &quot;redis-cli ping&quot;
234:           --health-interval 10s
235:     
236:     steps:
237:       - uses: actions/checkout@v2
238:       
239:       - name: Run Real Integration Tests
240:         env:
241:           API_KEY: ${{ secrets.REAL_API_KEY }}
242:           DB_CONNECTION: ${{ secrets.REAL_DB_CONNECTION }}
243:         run: |
244:           pytest tests/ -v --no-mock-allowed
245:         timeout-minutes: 30  # Allow time for real operations
246: ```
247: 
248: ## VERIFICATION CHECKLIST
249: 
250: Before ANY test is accepted:
251: 
252: - ‚úÖ ZERO mock/patch/stub imports
253: - ‚úÖ ALL external calls go to real endpoints
254: - ‚úÖ Database tests use real database
255: - ‚úÖ File tests use real filesystem
256: - ‚úÖ Network tests use real network conditions
257: - ‚úÖ Auth tests use real authentication
258: - ‚úÖ Error tests trigger real errors
259: - ‚úÖ Tests pass with fresh data every run
260: 
261: ## SUCCESS CRITERIA
262: 
263: 1. **No Mock Detection**: `grep -r &quot;mock\|Mock\|patch\|stub&quot; tests/` returns NOTHING
264: 2. **Real Endpoints**: All HTTP calls point to actual URLs
265: 3. **Fresh Data**: Each test run gets different timestamps/IDs
266: 4. **Real Failures**: Tests can actually fail from real issues
267: 5. **Production-Like**: Test environment mirrors production
268: 
269: ## CACHING REQUIREMENTS (IF ABSOLUTELY NECESSARY)
270: 
271: ### STRICT CACHE RULES - VIOLATE = REJECTION
272: 
273: ```python
274: # IF you MUST implement caching:
275: 
276: class StrictCache:
277:     &quot;&quot;&quot;DOCUMENTED CACHE - EASY TO FIND AND REMOVE&quot;&quot;&quot;
278:     def __init__(self):
279:         self.cache = {}
280:         self.timestamps = {}
281:         self.max_age_seconds = 30  # SHORT TTL ONLY
282:     
283:     def get(self, key):
284:         &quot;&quot;&quot;MUST CHECK: Is data fresh? Is it duplicated?&quot;&quot;&quot;
285:         # DEDUPLICATION CHECK
286:         if key in self.cache:
287:             # AGE CHECK - NO OLD DATA EVER
288:             if time.time() - self.timestamps[key] &gt; self.max_age_seconds:
289:                 del self.cache[key]  # DELETE STALE DATA
290:                 del self.timestamps[key]
291:                 return None  # FORCE FRESH FETCH
292:             
293:             # LOG CACHE HIT FOR MONITORING
294:             logger.warning(f&quot;CACHE HIT: {key} - age: {time.time() - self.timestamps[key]}s&quot;)
295:             return self.cache[key]
296:         return None
297:     
298:     def set(self, key, value):
299:         &quot;&quot;&quot;MUST VALIDATE: No duplicates, timestamp everything&quot;&quot;&quot;
300:         # CHECK FOR DUPLICATE DATA
301:         for existing_key, existing_value in self.cache.items():
302:             if existing_value == value and existing_key != key:
303:                 logger.error(f&quot;DUPLICATE DATA DETECTED: {key} matches {existing_key}&quot;)
304:                 
305:         self.cache[key] = value
306:         self.timestamps[key] = time.time()
307: 
308: # REQUIRED DOCUMENTATION AT EVERY CACHE POINT:
309: # TODO: CACHE HERE - REMOVE FOR PRODUCTION
310: # WARNING: CACHED DATA - MAX AGE 30 SECONDS
311: # CACHE LOCATION: Easy grep target for removal
312: ```
313: 
314: ### NO CACHE PROMISE = NO CACHE ALLOWED
315: 
316: If you cannot guarantee:
317: - ‚úÖ Deduplication checks on every write
318: - ‚úÖ Timestamp validation on every read  
319: - ‚úÖ Automatic expiration of stale data
320: - ‚úÖ Clear documentation at cache points
321: - ‚úÖ Easy to find and remove
322: 
323: **THEN NO CACHING ALLOWED**
324: 
325: ## CODERABBIT INTEGRATION
326: 
327: ### PR REVIEW AUTOMATION
328: 
329: ```yaml
330: # .github/coderabbit.yml
331: reviews:
332:   auto_review:
333:     enabled: true
334:     level: &quot;comprehensive&quot;
335:     
336:   unresolved_comments:
337:     track: true
338:     assume_latest_commit: true  # CRITICAL: Only latest commit matters
339:     
340:   impact_documentation:
341:     high_severity_requires: &quot;IMPACT.md&quot;
342: ```
343: 
344: ### HANDLING CODERABBIT COMMENTS
345: 
346: ```python
347: # When CodeRabbit flags an issue:
348: 
349: def handle_coderabbit_comment(comment):
350:     &quot;&quot;&quot;Process unresolved CodeRabbit comments&quot;&quot;&quot;
351:     
352:     # ASSUMPTION: Comment is on MOST RECENT COMMIT ONLY
353:     if comment.commit != latest_commit:
354:         return  # Skip old comments
355:     
356:     if comment.severity == &quot;HIGH&quot; and comment.unresolved:
357:         # DO NOT AUTO-FIX - DOCUMENT IMPACT
358:         impact_file = f&quot;{date}_{commit_id[:7]}_{branch}_IMPACT.md&quot;
359:         
360:         with open(impact_file, &apos;w&apos;) as f:
361:             f.write(f&quot;&quot;&quot;# HIGH IMPACT ISSUE DETECTED
362: 
363: **Date**: {date}
364: **Commit**: {commit_id}
365: **Branch**: {branch}
366: 
367: ## CodeRabbit Finding
368: {comment.description}
369: 
370: ## Potential Impact
371: {comment.impact_analysis}
372: 
373: ## ACTION REQUIRED
374: Manual review needed before proceeding.
375: DO NOT PERFORM automated fix.
376: &quot;&quot;&quot;)
377:         
378:         raise Exception(f&quot;High impact issue documented in {impact_file}&quot;)
379: ```
380: 
381: ## MCP INTEGRATION VIA CURSOR
382: 
383: ### STANDARD MCP INTERACTION PATTERN
384: 
385: ```python
386: # Standard way to interact with MCP servers in Cursor
387: 
388: from mcp import Client
389: import asyncio
390: 
391: class MCPInterface:
392:     &quot;&quot;&quot;Standard MCP client for Cursor-connected servers&quot;&quot;&quot;
393:     
394:     def __init__(self):
395:         # Cursor MCP servers (auto-discovered)
396:         self.cursor_servers = self._discover_cursor_servers()
397:         
398:         # Docker Desktop MCP servers on LAN
399:         self.docker_servers = self._discover_docker_servers()
400:         
401:     async def call_tool(self, server_name: str, tool_name: str, params: dict):
402:         &quot;&quot;&quot;Call any MCP tool through Cursor&quot;&quot;&quot;
403:         
404:         # Try Cursor-integrated servers first
405:         if server_name in self.cursor_servers:
406:             client = self.cursor_servers[server_name]
407:             return await client.call_tool(tool_name, params)
408:         
409:         # Fallback to Docker Desktop servers
410:         if server_name in self.docker_servers:
411:             client = self.docker_servers[server_name]
412:             return await client.call_tool(tool_name, params)
413:         
414:         raise ValueError(f&quot;MCP server {server_name} not found&quot;)
415:     
416:     def _discover_cursor_servers(self):
417:         &quot;&quot;&quot;Auto-discover Cursor MCP integrations&quot;&quot;&quot;
418:         # Cursor provides: Context7, Sequoia AI, repo connections
419:         return {
420:             &apos;context7&apos;: Client(&apos;cursor://context7&apos;),
421:             &apos;sequoia&apos;: Client(&apos;cursor://sequoia&apos;),
422:             &apos;repo&apos;: Client(&apos;cursor://current-repo&apos;)
423:         }
424:     
425:     def _discover_docker_servers(self):
426:         &quot;&quot;&quot;Discover Docker Desktop MCP servers on LAN&quot;&quot;&quot;
427:         # PLACEHOLDER: Add exact Docker MCP discovery here
428:         # Docker Desktop runs MCP containers accessible via:
429:         # - Service discovery on local network
430:         # - Fixed ports (e.g., 8080-8099)
431:         # - mDNS/Bonjour names
432:         
433:         # TODO: Replace with actual Docker MCP discovery
434:         return {
435:             # &apos;service_name&apos;: Client(&apos;docker://hostname:port&apos;)
436:         }
437: 
438: # Usage in tests:
439: async def test_with_mcp():
440:     &quot;&quot;&quot;Test using real MCP servers&quot;&quot;&quot;
441:     mcp = MCPInterface()
442:     
443:     # Call Context7 for documentation
444:     docs = await mcp.call_tool(&apos;context7&apos;, &apos;get_docs&apos;, {
445:         &apos;query&apos;: &apos;testing best practices&apos;
446:     })
447:     
448:     # Call Docker MCP service
449:     result = await mcp.call_tool(&apos;docker_service&apos;, &apos;process&apos;, {
450:         &apos;data&apos;: &apos;real_input&apos;
451:     })
452:     
453:     assert result is not None
454: ```
455: 
456: ### DOCKER DESKTOP MCP SETUP
457: 
458: ```yaml
459: # docker-compose.mcp.yml - MCP servers on LAN
460: version: &apos;3.8&apos;
461: 
462: services:
463:   mcp_server_1:
464:     image: mcp/server:latest
465:     ports:
466:       - &quot;8080:8080&quot;
467:     environment:
468:       - MCP_MODE=production
469:       - MCP_DISCOVERY=enabled
470:     networks:
471:       - mcp_lan
472:       
473:   # TODO: Add specific MCP containers here
474:   # Each service should expose:
475:   # - Discovery endpoint
476:   # - Tool registry
477:   # - Health check
478: 
479: networks:
480:   mcp_lan:
481:     driver: bridge
482:     ipam:
483:       config:
484:         - subnet: 172.20.0.0/16  # Fixed subnet for MCP
485: ```
486: 
487: ## AGENT INSTRUCTIONS
488: 
489: ### **MANDATORY FIRST-MINUTE PROJECT SCAN**
490: BEFORE writing any tests, perform this scan:
491: 
492: ```bash
493: # 1. Detect dependency management
494: ls pyproject.toml package.json requirements.txt 2&gt;/dev/null
495: 
496: # 2. Use appropriate tool
497: if [ -f &quot;pyproject.toml&quot; ]; then
498:     # POETRY PROJECT - Use poetry for everything
499:     poetry install
500:     poetry run pytest
501:     poetry run python script.py
502: elif [ -f &quot;package.json&quot; ]; then
503:     # NODE PROJECT - Use npm/yarn
504:     npm install
505:     npm test
506: fi
507: 
508: # 3. Scan for project rules
509: cat CLAUDE.md AGENT.md .cursor/rules/* 2&gt;/dev/null | grep -E &quot;(rule|policy|standard|must|never)&quot;
510: ```
511: 
512: ### **CORE TESTING PRINCIPLES**
513: When writing tests:
514: 1. NEVER create fake data
515: 2. NEVER stub responses
516: 3. NEVER mock dependencies
517: 4. ALWAYS call real services
518: 5. ALWAYS use real databases
519: 6. ALWAYS handle real errors
520: 7. ALWAYS respect rate limits
521: 8. ALWAYS clean up real resources
522: 9. ALWAYS scan for project dependency management FIRST
523: 10. IF caching needed, MUST have deduplication and staleness checks
524: 11. ALWAYS review CodeRabbit comments on latest commit
525: 12. ALWAYS document high-impact issues instead of auto-fixing
526: 13. ALWAYS use MCP for external data when available
527: 
528: **Remember: If you can&apos;t test it with real data, you can&apos;t trust it in production**
529: 
530: ## CODE SIMPLIFICATION PRIORITIES
531: 
532: ### 1. IMPORT CHAIN CLEANUP
533: 
534: ```python
535: # Detect and eliminate circular imports
536: # REPO-AGNOSTIC EXAMPLE:
537: 
538: # BAD - Circular import
539: # file: services/auth.py
540: from database.users import UserModel  # users imports auth = circular!
541: 
542: # GOOD - Break the cycle
543: # file: services/auth.py
544: from typing import TYPE_CHECKING
545: if TYPE_CHECKING:
546:     from database.users import UserModel
547: 
548: # Standard import ordering (enforce everywhere)
549: import os                    # 1. stdlib
550: import sys
551: 
552: import numpy as np          # 2. third-party
553: import pandas as pd
554: 
555: from .local_module import x  # 3. local
556: from ..parent import y
557: ```
558: 
559: ### 2. TYPE SYSTEM COHERENCE
560: 
561: ```python
562: # Common type issues to fix:
563: 
564: # PROBLEM: Mixed numeric types
565: price: float = 100.50
566: fee: Decimal = Decimal(&quot;0.01&quot;)
567: total = price + fee  # TYPE ERROR!
568: 
569: # SOLUTION: Pick one type system
570: price: Decimal = Decimal(&quot;100.50&quot;)
571: fee: Decimal = Decimal(&quot;0.01&quot;)
572: total: Decimal = price + fee  # ‚úÖ
573: 
574: # PROBLEM: Optional without None checks
575: def process(data: Optional[Dict]):
576:     return data[&apos;key&apos;]  # CRASH if None!
577: 
578: # SOLUTION: Always check Optional
579: def process(data: Optional[Dict]):
580:     if data is None:
581:         raise ValueError(&quot;Data required&quot;)
582:     return data[&apos;key&apos;]  # ‚úÖ
583: ```
584: 
585: ### 3. FUNCTION CONSOLIDATION
586: 
587: ```python
588: # BEFORE: Multiple versions doing same thing
589: def get_price_v1(symbol): ...
590: def fetch_price(symbol): ...
591: def retrieve_current_price(symbol): ...
592: 
593: # AFTER: One canonical version
594: class PriceService:
595:     &quot;&quot;&quot;Single source of truth for prices&quot;&quot;&quot;
596:     async def get_price(self, symbol: str) -&gt; Decimal:
597:         # One implementation, used everywhere
598:         pass
599: ```
600: 
601: ### 4. API PATTERN ENFORCEMENT
602: 
603: ```python
604: # Enforce this pattern for ALL external API calls:
605: async def get_external_data():
606:     &quot;&quot;&quot;Standard pattern for external APIs&quot;&quot;&quot;
607:     # 1. Rate limit ALWAYS
608:     await rate_limiter.acquire()
609:     
610:     # 2. Try with timeout
611:     try:
612:         result = await asyncio.wait_for(
613:             client.fetch(endpoint),
614:             timeout=10.0
615:         )
616:     except asyncio.TimeoutError:
617:         # 3. NEVER mock on failure
618:         raise DataUnavailableError(&quot;API timeout&quot;)
619:     
620:     # 4. Validate response
621:     if not result or &apos;error&apos; in result:
622:         raise DataUnavailableError(f&quot;Invalid response: {result}&quot;)
623:     
624:     # 5. Type consistency
625:     return Decimal(str(result[&apos;value&apos;]))
626: ```
627: 
628: ## PROJECT STATE AWARENESS
629: 
630: ### CURRENT ISSUES TO TRACK
631: 
632: ```yaml
633: # issues_tracker.yml
634: active_issues:
635:   - module: api_client
636:     issue: intermittent_timeout
637:     severity: medium
638:     
639:   - module: type_system  
640:     issue: mixed_numeric_types
641:     severity: high
642:     
643:   - module: imports
644:     issue: circular_dependencies
645:     severity: critical
646: ```
647: 
648: ### MVP BLOCKERS
649: 
650: 1. **Data Reliability**: All sources returning valid data
651: 2. **Type Safety**: No runtime type errors
652: 3. **Import Health**: Zero circular imports
653: 4. **Rate Limits**: Never exceeded
654: 5. **Async Consistency**: All awaits in place
655: 
656: ## AGENT DECISION FRAMEWORK
657: 
658: ### AUTO-FIX WITH CURSOR
659: - Import reordering
660: - Type hint additions  
661: - Docstring updates
662: - Whitespace/formatting
663: 
664: ### REPORT FOR REVIEW
665: - API signature changes
666: - Module restructuring
667: - Core logic changes
668: - Database schema updates
669: 
670: ### EMERGENCY ALERT
671: ```python
672: # Immediate escalation required:
673: if &quot;mock&quot; in code or &quot;fake&quot; in code:
674:     alert(&quot;MOCK DATA DETECTED&quot;)
675:     
676: if api_calls &gt; rate_limit:
677:     alert(&quot;QUOTA VIOLATION IMMINENT&quot;)
678:     
679: if &quot;eval(&quot; in code or &quot;exec(&quot; in code:
680:     alert(&quot;SECURITY VULNERABILITY&quot;)
681: ```
682: 
683: ## MONITORING &amp; LEARNING
684: 
685: ### CONTINUOUS METRICS
686: 
687: ```python
688: CODEBASE_METRICS = {
689:     &apos;total_modules&apos;: 0,
690:     &apos;max_import_depth&apos;: 0,
691:     &apos;type_coverage&apos;: 0.0,
692:     &apos;complexity_average&apos;: 0.0,
693:     &apos;api_calls&apos;: {
694:         &apos;service_a&apos;: 0,
695:         &apos;service_b&apos;: 0
696:     },
697:     &apos;test_coverage&apos;: 0.0,
698:     &apos;real_data_percentage&apos;: 100.0  # MUST be 100%
699: }
700: ```
701: 
702: ### KNOWLEDGE PERSISTENCE
703: 
704: ```markdown
705: # docs/patterns.md - Agent maintains this
706: ## Discovered Patterns
707: - API X returns null on timeout (not error)
708: - Service Y needs 2-second delay between calls
709: - Database connection pool max is 50
710: 
711: ## API Limits (observed)
712: - Service A: 100 req/min
713: - Service B: 1000 req/hour
714: - Database: 50 concurrent connections
715: ```
716: 
717: ## SUCCESS CRITERIA
718: 
719: **Project is MVP-ready when:**
720: - ‚úÖ Zero import errors on startup
721: - ‚úÖ Type checker passes strict mode
722: - ‚úÖ All APIs return valid typed data
723: - ‚úÖ Stable operation for 1+ hours
724: - ‚úÖ No mock data in entire codebase
725: - ‚úÖ API quotas never exceeded
726: - ‚úÖ All tests use real data
727: - ‚úÖ CodeRabbit approves all PRs
728: 
729: **VALIDATED SUCCESS PATTERN FROM DSP PROJECT:**
730: - ‚úÖ 12/12 tests passing with strict no-mock policy
731: - ‚úÖ Real Docker container integration testing
732: - ‚úÖ Live HTTP endpoint validation
733: - ‚úÖ Actual filesystem operations
734: - ‚úÖ Poetry dependency management detection
735: - ‚úÖ User feedback integration protocol working
736: 
737: ## AGENT MEMORY FILE
738: 
739: ```yaml
740: # .agent_memory.yaml - Agent maintains this
741: last_scan: 2024-01-15T10:30:00Z
742: issues_found:
743:   - module: api_client
744:     issue: mixed_types
745:     severity: high
746:     line_numbers: [45, 67, 89]
747:     
748: patterns_learned:
749:   - api_returns_none_on_timeout
750:   - retry_needed_after_429
751:   - connection_pool_exhaustion_at_50
752:   
753: fixed_count: 127
754: pending_fixes:
755:   - standardize_decimal_usage
756:   - consolidate_duplicate_functions
757:   - remove_circular_imports
758:   
759: coderabbit_unresolved: 3
760: high_impact_documented: 2
761: ```</file><file path="architecture-analysis-sept15.md">  1: # DSP Documentation Agent - Architecture Analysis Report
  2: *Generated September 15, 2024*
  3: 
  4: ## Executive Summary
  5: 
  6: The DSP Documentation &amp; Physics Speculation Agent has successfully migrated to a fully local Docker Desktop deployment. **TASK 1 COMPLETE**: All Docker infrastructure is operational with HTTP bridges for MCP communication. The system maintains its core purpose of blending Dyson Sphere Program game mechanics (60%) with theoretical physics speculation (40%) while serving content creators and sci-fi writers.
  7: 
  8: ## Current Architecture State - ‚úÖ OPERATIONAL
  9: 
 10: ### Network Topology
 11: - **Local Development**: Single machine deployment via Docker Desktop
 12: - **Communication**: HTTP on localhost (no network dependencies)
 13: - **MCP Endpoints**:
 14:   - RAG Documentation: `http://localhost:3002/health` ‚úÖ
 15:   - Web Search: `http://localhost:3004/health` ‚úÖ
 16:   - Qdrant Database: `http://localhost:6333/` ‚úÖ
 17: 
 18: ### Infrastructure Components
 19: 
 20: #### Docker Services (Local Host: localhost)
 21: 1. **Qdrant Vector Database** (Ports 6333/6334) ‚úÖ RUNNING
 22:    - Container: `dsp-qdrant`
 23:    - Persistent storage for DSP documentation embeddings
 24:    - Restart policy: `unless-stopped`
 25:    - Status: Healthy and accessible
 26: 
 27: 2. **MCP RAG Documentation Server** (Port 3002‚Üí3000) ‚úÖ RUNNING
 28:    - Container: `dsp-mcp-ragdocs`
 29:    - HTTP Bridge: Custom wrapper (`rag-http-wrapper.js`)
 30:    - OpenAI API integration configured
 31:    - Volume-mounted docs directory ready
 32:    - Status: Mock responses working, ready for document ingestion
 33: 
 34: 3. **MCP Web Search Server** (Port 3004‚Üí3000) ‚úÖ RUNNING
 35:    - Container: `dsp-mcp-search`
 36:    - HTTP Bridge: Custom wrapper (`search-http-wrapper.js`)
 37:    - Brave Search API integration configured
 38:    - Status: Mock responses working, ready for live search
 39: 
 40: #### Interface Layer
 41: - **Claudable Config**: Local configuration exists at `/claudable/config.json`
 42: - **Implementation Status**: Configuration ready, full chatbot implementation needed
 43: - **API Integration**: Claude 3.5 Sonnet via Anthropic API
 44: 
 45: ## Migration Accomplishments
 46: 
 47: ### ‚úÖ Completed
 48: - N8N references completely removed from codebase (40+ locations cleaned)
 49: - Docker Compose infrastructure defined with proper networking
 50: - Tailscale network connectivity verified
 51: - Configuration files updated with correct endpoints
 52: - Task list restructured for current architecture
 53: 
 54: ### ‚úÖ TASK 1 COMPLETED: Docker Infrastructure Setup
 55: - All 3 Docker containers deployed and running
 56: - HTTP endpoints accessible and responding correctly
 57: - Auto-restart policies configured (`unless-stopped`)
 58: - Port assignments verified: RAG=3002, Search=3004, Qdrant=6333-6334
 59: - Validation script created: `/docker/validate-infrastructure.sh`
 60: - Ready for Task 2: Cross-Repository Integration with Claudable
 61: - Architecture ready for immediate container deployment
 62: 
 63: ## Key Technical Decisions
 64: 
 65: ### 1. RAG Implementation Strategy
 66: 
 67: #### Current Approach: Custom mcp-ragdocs + Qdrant
 68: - **Pros**: Full control, local deployment, no external dependencies
 69: - **Cons**: Complex setup, manual vector database management
 70: 
 71: #### Alternative: Needle.app Managed RAG
 72: - **Pros**:
 73:   - Simplified setup: `bun install @needle-ai/needle`
 74:   - Managed embeddings and vector storage
 75:   - Reduced infrastructure complexity
 76:   - Perfect for MVP single-user deployment
 77: - **Cons**: External API dependency, ongoing service costs
 78: - **Setup**: One SDK call vs full Docker orchestration
 79: 
 80: **Recommendation**: Evaluate Needle.app for MVP deployment given userbase of 1
 81: 
 82: ### 2. Claudable Interface Architecture
 83: 
 84: #### Current State
 85: - Configuration file exists with proper Tailscale endpoints
 86: - Full Node.js chatbot implementation required
 87: 
 88: #### Options
 89: 1. **Full Implementation**: Complete chatbot with web interface
 90: 2. **Lightweight CLI**: Simple script using existing config.json
 91: 3. **Hybrid**: Basic CLI with upgrade path to full interface
 92: 
 93: **Recommendation**: Start with lightweight CLI for immediate functionality
 94: 
 95: ### 3. Network Deployment Strategy
 96: 
 97: #### Current: Distributed Tailscale
 98: - **Pros**: Clean separation of concerns, isolated Docker environment
 99: - **Cons**: Network dependency, additional complexity
100: 
101: #### Alternative: Local Consolidation
102: - **Pros**: Simplified deployment, reduced network points of failure
103: - **Cons**: Resource contention, less architectural flexibility
104: 
105: **Recommendation**: Continue with Tailscale for development, plan local option for production
106: 
107: ## Implementation Priorities
108: 
109: ### Phase 1: Infrastructure Deployment (Immediate)
110: 1. Deploy Docker containers on host 100.122.20.18
111: 2. Verify MCP server health endpoints
112: 3. Test Tailscale connectivity from dev environment
113: 
114: ### Phase 2: Interface Implementation (Week 1)
115: 1. **Architecture Decision**: RAG implementation (mcp-ragdocs vs Needle.app)
116: 2. **Interface Decision**: Full Claudable vs lightweight wrapper
117: 3. Basic query testing with existing configuration
118: 
119: ### Phase 3: Content Integration (Week 2)
120: 1. DSP documentation scraping and ingestion
121: 2. Agent personality configuration (fun tone, 60/40 balance)
122: 3. Response quality validation using test-focused agent
123: 
124: ### Phase 4: MVP Validation (Week 3)
125: 1. End-to-end testing across query types
126: 2. Performance validation for single-user deployment
127: 3. Simple deployment documentation
128: 
129: ## Agent Integration Capabilities
130: 
131: ### Specialized Agents Identified
132: - **dyson-codebase-arch**: Architectural analysis and system design
133: - **test-focused**: Repository crawling and validation testing
134: - **Standard Claude agents**: Content creation and documentation
135: 
136: ### Agent Coordination Pattern
137: - Architecture agent handles system-level decisions and modifications
138: - Test agent validates implementations and crawls documentation
139: - Content agents handle scraping and prompt engineering
140: - Clear handoff points between specialized capabilities
141: 
142: ## Risk Assessment &amp; Mitigation
143: 
144: ### Network Dependencies
145: - **Risk**: Tailscale connectivity failures
146: - **Mitigation**: Local fallback option, connection retry logic
147: 
148: ### MCP Server Stability
149: - **Risk**: Container crashes, restart failures
150: - **Mitigation**: Docker restart policies, health monitoring scripts
151: 
152: ### RAG Performance
153: - **Risk**: Slow document retrieval, poor search relevance
154: - **Mitigation**: Document preprocessing, query optimization, Needle.app evaluation
155: 
156: ## Success Metrics
157: 
158: ### MVP Completion Criteria
159: - [ ] Docker containers deployed and accessible via Tailscale
160: - [ ] Basic Claudable interface responding to queries
161: - [ ] 10-15 key DSP documentation pages ingested
162: - [ ] Agent personality responding with appropriate 60/40 balance
163: - [ ] Test-focused agent validation passing
164: - [ ] Simple deployment instructions documented
165: 
166: ### Quality Benchmarks
167: - Response time &lt; 10 seconds for game mechanics queries
168: - Response time &lt; 30 seconds for physics speculation (web search required)
169: - Fun, engaging tone maintained (not academic)
170: - Accurate game mechanics information
171: - Grounded physics speculation with real research citations
172: 
173: ## Next Steps
174: 
175: 1. **Immediate**: Deploy Docker infrastructure on Tailscale host
176: 2. **Architecture Decision**: Evaluate Needle.app for RAG simplification
177: 3. **Interface Decision**: Choose Claudable implementation approach
178: 4. **Content Pipeline**: Begin DSP documentation collection
179: 5. **Agent Coordination**: Use test-focused agent for system validation
180: 
181: ## Conclusion
182: 
183: The DSP Documentation Agent has successfully transitioned from conceptual planning to deployable architecture. The Tailscale-based Docker deployment provides a solid foundation for MVP development while maintaining upgrade paths for future enhancements. Key decision points around RAG implementation and interface complexity remain, with clear evaluation criteria for each option.
184: 
185: The integration of specialized Claude Code agents provides a powerful development workflow, with clear separation of concerns between architectural decisions, implementation validation, and content management.
186: 
187: *Architecture analysis completed by dyson-codebase-arch agent*</file><file path="pyproject.toml"> 1: [project]
 2: name = &quot;dyson-sphere-facts&quot;
 3: version = &quot;0.1.0&quot;
 4: description = &quot;DSP Documentation &amp; Physics Speculation Agent with real-data test framework&quot;
 5: authors = [{name = &quot;DSP Test Agent&quot;, email = &quot;test@dsp-agent.local&quot;}]
 6: readme = &quot;README.md&quot;
 7: requires-python = &quot;&gt;=3.8&quot;
 8: dependencies = [
 9:     &quot;httpx&gt;=0.25.0&quot;,
10:     &quot;asyncio&gt;=3.4.3&quot;
11: ]
12: 
13: [tool.poetry]
14: packages = [{include = &quot;tests&quot;}]
15: 
16: [tool.poetry.dependencies]
17: python = &quot;^3.8&quot;
18: httpx = &quot;^0.25.0&quot;
19: asyncio = &quot;^3.4.3&quot;
20: 
21: [tool.poetry.group.dev.dependencies]
22: pytest = &quot;^7.4.0&quot;
23: pytest-asyncio = &quot;^0.21.0&quot;
24: black = &quot;^24.3.0&quot;
25: ruff = &quot;^0.1.0&quot;
26: 
27: [build-system]
28: requires = [&quot;poetry-core&quot;]
29: build-backend = &quot;poetry.core.masonry.api&quot;
30: 
31: [tool.pytest.ini_options]
32: asyncio_mode = &quot;auto&quot;
33: testpaths = [&quot;tests&quot;]
34: python_files = [&quot;test_*.py&quot;, &quot;*_test.py&quot;]
35: python_functions = [&quot;test_*&quot;]
36: 
37: [tool.black]
38: line-length = 100
39: target-version = [&apos;py38&apos;]
40: 
41: [tool.ruff]
42: line-length = 100
43: target-version = &quot;py38&quot;
44: select = [&quot;E&quot;, &quot;F&quot;, &quot;W&quot;, &quot;C&quot;, &quot;N&quot;]
45: ignore = []</file><file path="validate-test-suite.sh"> 1: #!/bin/bash
 2: # Test Suite Validation Script
 3: # Validates that the DSP real-data test suite is properly configured
 4: # This script can be run by other agents to verify test readiness
 5: 
 6: set -e
 7: 
 8: echo &quot;üîç DSP Test Suite Validation&quot;
 9: echo &quot;================================&quot;
10: 
11: # Check Poetry setup
12: echo &quot;‚úÖ Validating Poetry configuration...&quot;
13: poetry check || exit 1
14: 
15: # Check test dependencies
16: echo &quot;‚úÖ Checking test dependencies...&quot;
17: poetry show pytest &gt; /dev/null || { echo &quot;‚ùå pytest not installed&quot;; exit 1; }
18: poetry show pytest-asyncio &gt; /dev/null || { echo &quot;‚ùå pytest-asyncio not installed&quot;; exit 1; }
19: poetry show httpx &gt; /dev/null || { echo &quot;‚ùå httpx not installed&quot;; exit 1; }
20: 
21: # Check test file structure
22: echo &quot;‚úÖ Validating test file structure...&quot;
23: test -f tests/__init__.py || { echo &quot;‚ùå Missing tests/__init__.py&quot;; exit 1; }
24: test -f tests/conftest.py || { echo &quot;‚ùå Missing tests/conftest.py&quot;; exit 1; }
25: test -f tests/test_critical.py || { echo &quot;‚ùå Missing tests/test_critical.py&quot;; exit 1; }
26: test -f tests/README.md || { echo &quot;‚ùå Missing tests/README.md&quot;; exit 1; }
27: 
28: # Check pytest can discover tests
29: echo &quot;‚úÖ Verifying pytest test discovery...&quot;
30: COLLECTED=$(poetry run pytest --collect-only tests/ -q | grep &quot;collected&quot; | awk &apos;{print $1}&apos;)
31: if [ &quot;$COLLECTED&quot; != &quot;12&quot; ]; then
32:     echo &quot;‚ùå Expected 12 tests, found $COLLECTED&quot;
33:     exit 1
34: fi
35: 
36: # Verify no forbidden imports
37: echo &quot;‚úÖ Scanning for forbidden mock imports...&quot;
38: poetry run python -c &quot;
39: import ast
40: from pathlib import Path
41: 
42: def check_file(filepath):
43:     with open(filepath, &apos;r&apos;) as f:
44:         tree = ast.parse(f.read())
45:         for node in ast.walk(tree):
46:             if isinstance(node, ast.Import):
47:                 for alias in node.names:
48:                     if any(x in alias.name for x in [&apos;mock&apos;, &apos;patch&apos;, &apos;stub&apos;, &apos;fake&apos;]):
49:                         return False
50:             elif isinstance(node, ast.ImportFrom):
51:                 if node.module and any(x in node.module for x in [&apos;mock&apos;, &apos;patch&apos;, &apos;stub&apos;, &apos;fake&apos;]):
52:                     return False
53:                 for alias in node.names:
54:                     if any(x in alias.name for x in [&apos;mock&apos;, &apos;patch&apos;, &apos;stub&apos;, &apos;fake&apos;]):
55:                         return False
56:     return True
57: 
58: test_files = list(Path(&apos;tests&apos;).glob(&apos;*.py&apos;))
59: for f in test_files:
60:     if not check_file(f):
61:         print(f&apos;FORBIDDEN IMPORT FOUND in {f}&apos;)
62:         exit(1)
63: print(&apos;All files clean&apos;)
64: &quot; || exit 1
65: 
66: # Check infrastructure markers
67: echo &quot;‚úÖ Verifying pytest markers...&quot;
68: poetry run pytest tests/ -m infrastructure --collect-only -q &gt; /dev/null || exit 1
69: 
70: # Verify project structure references
71: echo &quot;‚úÖ Checking project path references...&quot;
72: test -f claudable/config.json || { echo &quot;‚ùå Missing claudable/config.json&quot;; exit 1; }
73: test -f docker/docker-compose.yml || { echo &quot;‚ùå Missing docker/docker-compose.yml&quot;; exit 1; }
74: 
75: echo &quot;&quot;
76: echo &quot;üéâ TEST SUITE VALIDATION COMPLETE&quot;
77: echo &quot;================================&quot;
78: echo &quot;‚úÖ Poetry configuration valid&quot;
79: echo &quot;‚úÖ Test dependencies installed&quot;
80: echo &quot;‚úÖ 12 tests discovered successfully&quot;
81: echo &quot;‚úÖ No forbidden mock imports detected&quot;
82: echo &quot;‚úÖ pytest markers configured&quot;
83: echo &quot;‚úÖ Real-data testing principles enforced&quot;
84: echo &quot;&quot;
85: echo &quot;üöÄ Test suite is ready!&quot;
86: echo &quot;üìã To run tests when infrastructure is ready:&quot;
87: echo &quot;   poetry run pytest tests/&quot;
88: echo &quot;   poetry run pytest tests/ -m infrastructure&quot;
89: echo &quot;&quot;
90: echo &quot;‚ö†Ô∏è  REMEMBER: Tests will fail if Docker containers are not running&quot;
91: echo &quot;    This is INTENTIONAL - tests only pass with real infrastructure&quot;</file><file path="tasks/dsp-task-list.md">  1: # Task List: DSP Documentation Agent Implementation - Local Docker Desktop Setup
  2: 
  3: ## Architecture Update (September 2024)
  4: 
  5: **Current State**: Migrated to fully local Docker Desktop environment. Both repositories local.
  6: **Network**: localhost-only, no Tailscale dependencies
  7: **Repository Structure**:
  8: - `/github-projects/dyson-sphere-facts/` (this repo - MCP servers)
  9: - `/github-projects/Claudable/` (horizontal repo - generic app builder interface)
 10: **MCP Endpoints**: http://localhost:3002 (RAG), http://localhost:3004 (Search)
 11: **Port Allocation**: Claudable uses localhost:3001, MCP servers use 3002+ to avoid conflicts
 12: 
 13: ## Relevant Files
 14: 
 15: ### Docker Infrastructure &amp; Configuration
 16: 
 17: - `docker/docker-compose.yml` - MCP server container definitions (Qdrant, mcp-ragdocs, mcp-web-search)
 18: - `docker/.env` - API credentials (OPENAI_API_KEY, BRAVE_API_KEY)
 19: - `../Claudable/.env` - Claudable configuration (localhost:3001)
 20: - `../Claudable/` - Generic Next.js app builder (horizontal repository)
 21: 
 22: ### RAG Implementation Options
 23: 
 24: **Current**: Custom mcp-ragdocs with Qdrant vector database
 25: **Alternative**: Needle.app SDK for simplified managed RAG (reduces infrastructure complexity)
 26: 
 27: ### Documentation &amp; Testing
 28: 
 29: - `docs/dsp-wiki/` - Directory for scraped DSP documentation
 30: - `scripts/scrape-dsp-docs.js` - Documentation collection script
 31: - `tests/basic-validation.js` - Minimal validation using test-focused agent
 32: 
 33: ### Notes
 34: 
 35: - Docker containers run locally via Docker Desktop
 36: - Communication via HTTP to localhost (no network dependencies)
 37: - Claudable is generic app builder, may need DSP-specific configuration
 38: - Available MCP Docker images: `mcp/brave-search`, multiple RAG options
 39: - Focus on MVP simplicity for single-user deployment
 40: 
 41: ## Tasks
 42: 
 43: - [ ] 1.0 Configure Local Docker Infrastructure
 44: 
 45:   - [ ] 1.1 Update `docker-compose.yml` to use ports 3002 (RAG) and 3004 (Search) to avoid Claudable conflict
 46:   - [ ] 1.2 Create `docker/.env` file with OPENAI_API_KEY and BRAVE_API_KEY
 47:   - [ ] 1.3 Deploy containers: `docker-compose up -d` in docker/ directory
 48:   - [ ] 1.4 Verify container status: `docker ps | grep dsp`
 49:   - [ ] 1.5 Test localhost connectivity: `curl http://localhost:3002/health`
 50:   - [ ] 1.6 Test MCP endpoints: `curl http://localhost:3004/health`
 51:   - [ ] 1.7 Configure auto-restart policies for local development stability
 52: 
 53: - [ ] 2.0 Configure Cross-Repository Integration (Claudable + DSP MCP)
 54: 
 55:   - [ ] 2.1 **Architecture Decision**: Extend Claudable for DSP vs create standalone DSP interface
 56:   - [ ] 2.2 Configure Claudable to connect to local MCP servers (localhost:3002, 3004)
 57:   - [ ] 2.3 Test basic integration: Claudable ‚Üí HTTP ‚Üí MCP servers
 58:   - [ ] 2.4 Configure shared API keys between repositories (OPENAI_API_KEY)
 59:   - [ ] 2.5 Test basic query flow: User ‚Üí Claudable ‚Üí DSP MCP servers ‚Üí Response
 60:   - [ ] 2.6 Implement error handling for localhost HTTP connection issues
 61: 
 62: - [ ] 3.0 DSP Documentation Ingestion &amp; RAG Implementation
 63: 
 64:   - [ ] 3.1 **RAG Architecture Decision**: mcp-ragdocs (current) vs Needle.app (managed)
 65:   - [ ] 3.2 Create docs directory structure: `mkdir -p docs/dsp-wiki`
 66:   - [ ] 3.3 Scrape key DSP documentation pages (10-15 essential guides)
 67:   - [ ] 3.4 **If mcp-ragdocs**: Configure Qdrant vector database via Docker
 68:   - [ ] 3.5 **If Needle.app**: Set up managed RAG service with SDK integration
 69:   - [ ] 3.6 Test document ingestion and search functionality
 70:   - [ ] 3.7 Validate retrieval quality with DSP-specific queries
 71:   - [ ] 3.8 Create simple backup/restore process for documentation
 72: 
 73: - [ ] 4.0 Agent Personality &amp; Response System
 74: 
 75:   - [ ] 4.1 Create system prompt emphasizing fun tone (not academic)
 76:   - [ ] 4.2 Configure 60/40 balance: game mechanics vs physics speculation
 77:   - [ ] 4.3 Test response quality with example interactions:
 78:     - &quot;How do Critical Photons work?&quot; (game mechanics)
 79:     - &quot;Could we build a real Dyson sphere?&quot; (physics speculation)
 80:     - &quot;Compare game antimatter to real physics&quot; (hybrid)
 81:   - [ ] 4.4 Implement web search integration for current physics research
 82:   - [ ] 4.5 Validate tone consistency across query types
 83: 
 84: - [ ] 5.0 Testing &amp; Validation (Using Test-Focused Agent)
 85: 
 86:   - [ ] 5.1 Create `tests/basic-validation.sh` script for localhost connectivity
 87:   - [ ] 5.2 Use test-focused agent for repository crawling and system validation
 88:   - [ ] 5.3 Test core functionality:
 89:     - Game mechanics queries via RAG system
 90:     - Physics speculation via web search integration
 91:     - Hybrid responses combining both sources
 92:   - [ ] 5.4 Validate response tone and 60/40 content balance
 93:   - [ ] 5.5 Test MCP server restart resilience on local Docker Desktop
 94:   - [ ] 5.6 Basic performance validation (response times acceptable for single user)
 95: 
 96: - [ ] 6.0 MVP Deployment &amp; Documentation
 97: 
 98:   - [ ] 6.1 Create `start.sh` script to launch local Docker containers and Claudable
 99:   - [ ] 6.2 Create `restart.sh` for MCP server recovery
100:   - [ ] 6.3 Write simple README with setup steps for local Docker Desktop deployment
101:   - [ ] 6.4 Document troubleshooting for localhost connectivity issues
102:   - [ ] 6.5 Create example question list for testing agent responses
103:   - [ ] 6.6 [ORCHESTRATION: Future enhancement - Automated health monitoring]
104: 
105: ---
106: 
107: ## Architecture Migration Summary
108: 
109: **Previous**: N8N visual workflows (never implemented), Tailscale distributed setup
110: **Current**: Fully local Docker Desktop deployment
111: **Repository Structure**: dyson-sphere-facts/ and Claudable/ as horizontal repositories
112: **MCP Servers**: Docker containers on localhost (ports 3002, 3004)
113: **Interface**: Claudable (generic app builder) running on localhost:3001
114: 
115: ## Key Decisions Required
116: 
117: 1. **RAG Implementation**: Custom mcp-ragdocs vs Needle.app managed service
118: 2. **Claudable Integration**: Extend for DSP-specific features vs standalone DSP interface
119: 3. **Port Configuration**: MCP servers use 3002+ to avoid conflict with Claudable:3001
120: 
121: ## Available Docker Images for MCP
122: - `mcp/brave-search` - Web search functionality
123: - Multiple RAG options available in local Docker images
124: - Existing docker-compose.yml configured for basic setup
125: 
126: **Focus**: MVP simplicity for single-user deployment with test-focused agent validation</file><file path="tasks/tasks-dsp-agent-implementation.md">  1: # Task List: DSP Documentation &amp; Physics Speculation Agent Implementation
  2: 
  3: ## Architecture Overview
  4: 
  5: ### Network Topology (Tailscale)
  6: - **dev VM**: 100.86.15.93 - Development environment (current location)
  7: - **Docker Host**: 100.122.20.18 - Docker Desktop with MCP servers
  8: - **Communication**: HTTP calls via Tailscale network
  9: - **MCP Ports**: 3001 (ragdocs), 3003 (brave-search), 3002 (reserved)
 10: 
 11: ### Repository Structure
 12: ```
 13: /github-projects/
 14:   ‚îú‚îÄ‚îÄ dyson-sphere-facts/  (this repo - MCP servers &amp; documentation)
 15:   ‚îî‚îÄ‚îÄ Claudable/           (external repo - chatbot interface)
 16: ```
 17: 
 18: ### Integration Pattern
 19: - **Claudable** (external) communicates with this repo&apos;s MCP servers via HTTP
 20: - Configuration sharing via environment variables and config files
 21: - MCP servers deployed in Docker containers on Tailscale host (100.122.20.18)
 22: - Cross-repo setup documented for single-user deployment via Tailscale network
 23: 
 24: ## Relevant Files
 25: 
 26: ### Configuration &amp; Integration
 27: - `claudable/config.json` - MCP endpoint configuration (placeholder for Claudable integration)
 28: - `docker/docker-compose.yml` - Docker container definitions for MCP servers
 29: - `docker/.env` - API keys and environment variables
 30: - `scripts/setup-claudable-integration.sh` - Script to configure Claudable connection
 31: 
 32: ### Documentation &amp; Data
 33: - `docs/dsp-wiki/` - Directory for scraped DSP documentation
 34: - `scripts/scrape-dsp-docs.js` - Documentation scraping script
 35: - `data/test-questions.json` - Basic validation questions for testing
 36: 
 37: ### MCP Server Configurations
 38: - `mcp-configs/ragdocs-config.json` - RAG server configuration
 39: - `mcp-configs/search-config.json` - Web search configuration
 40: - `scripts/health-check.sh` - Basic MCP server health monitoring
 41: 
 42: ### Testing (Simplified)
 43: - `tests/basic-validation.js` - Minimal validation tests using test-focused agent
 44: - `tests/mcp-connectivity.test.js` - Simple MCP server connection tests
 45: 
 46: ### Notes
 47: 
 48: - Claudable is a SEPARATE repository requiring integration configuration
 49: - MCP servers run in this repo&apos;s Docker containers
 50: - Test-focused agent handles repository crawling and validation
 51: - Focus on MVP simplicity for userbase of 1
 52: - Balance: 60% game mechanics accuracy, 40% physics speculation
 53: 
 54: ## Tasks
 55: 
 56: - [ ] 1.0 Configure Cross-Repository Integration
 57:   * Status: **Ready to Execute**
 58:   * Sub-tasks:
 59:     - [ ] 1.1 Create `.env.example` with required variables (ANTHROPIC_API_KEY, BRAVE_API_KEY)
 60:     - [ ] 1.2 Add simple README section: &quot;Put Claudable repo next to this one, copy .env.example to .env&quot;
 61:     - [ ] 1.3 Hardcode MCP server URLs in config (http://100.122.20.18:3001, http://100.122.20.18:3003)
 62: 
 63: - [ ] 2.0 Deploy MCP Server Infrastructure
 64:   * Status: **Ready to Execute**
 65:   * Sub-tasks:
 66:     - [ ] 2.1 Create basic `docker-compose.yml` with mcp-ragdocs and mcp-brave-search containers
 67:     - [ ] 2.2 Map ports 3001 and 3003, set restart: always
 68:     - [ ] 2.3 Test with `docker-compose up -d` and `curl 100.122.20.18:3001/health`
 69: 
 70: - [ ] 3.0 Implement Agent Communication Architecture
 71:   * Status: **Ready to Execute**
 72:   * Sub-tasks:
 73:     - [ ] 3.1 Copy API keys from .env to Claudable&apos;s config.json manually
 74:     - [ ] 3.2 Test basic HTTP call from Claudable to MCP server via Tailscale: `curl 100.122.20.18:3001/health`
 75:     - [ ] 3.3 Add console.log statements for debugging (no fancy logging)
 76: 
 77: - [ ] 4.0 Create DSP Documentation Ingestion Pipeline
 78:   * Status: **Ready to Execute**
 79:   * Sub-tasks:
 80:     - [ ] 4.1 Use wget to download 10-20 key DSP wiki pages to `docs/dsp-wiki/`
 81:     - [ ] 4.2 Point mcp-ragdocs config to the docs folder
 82:     - [ ] 4.3 Test with one search query: &quot;Critical Photons&quot;
 83: 
 84: - [ ] 5.0 Develop Agent Personality and Response System
 85:   * Status: **Ready to Execute**
 86:   * Sub-tasks:
 87:     - [ ] 5.1 Write one-page system prompt in `prompts/system.txt` with fun tone examples
 88:     - [ ] 5.2 Add prompt to Claudable config (copy-paste into config.json)
 89:     - [ ] 5.3 Test with three example questions (game, physics, hybrid)
 90: 
 91: - [ ] 6.0 Implement Minimal Testing Framework
 92:   * Status: **Ready to Execute**
 93:   * Sub-tasks:
 94:     - [ ] 6.1 Create `test.sh` script that tests Tailscale endpoints (curl 100.122.20.18:3001/health)
 95:     - [ ] 6.2 Use existing test-agent to verify docs are indexed via Tailscale
 96:     - [ ] 6.3 Manual verification: ask 3 questions, check responses make sense over network
 97: 
 98: - [ ] 7.0 Create MVP Deployment Process
 99:   * Status: **Ready to Execute**
100:   * Sub-tasks:
101:     - [ ] 7.1 Write `start.sh`: docker-compose up -d &amp;&amp; cd ../Claudable &amp;&amp; npm start
102:     - [ ] 7.2 Write `stop.sh`: docker-compose down
103:     - [ ] 7.3 Add &quot;Getting Started&quot; section to README with 5 simple steps
104: 
105: ---
106: 
107: *Generated for Claudable + Docker MCP implementation of DSP Documentation Agent*</file><file path="tests/test_critical.py">  1: #!/usr/bin/env python3
  2: &quot;&quot;&quot;
  3: SUPER-MINIMAL CRITICAL FUNCTION TESTS
  4: No mocks, no fakes, no error handling - just binary pass/fail
  5: Tests only what exists, not what&apos;s planned
  6: 
  7: REAL-DATA TESTING PRINCIPLES:
  8: - No unittest.mock, no patches, no stubs
  9: - Every HTTP call hits real endpoints
 10: - Every Docker command checks real containers
 11: - Every file read accesses real filesystem
 12: - Tests fail when infrastructure fails
 13: &quot;&quot;&quot;
 14: 
 15: import subprocess
 16: import json
 17: import httpx
 18: import asyncio
 19: import sys
 20: import os
 21: import pytest
 22: from pathlib import Path
 23: 
 24: # Project root for path resolution
 25: PROJECT_ROOT = Path(__file__).parent.parent
 26: 
 27: 
 28: @pytest.mark.infrastructure
 29: def test_docker_mcp_containers_running():
 30:     &quot;&quot;&quot;Test: Are the critical Docker containers running?
 31:     REAL-DATA: Uses actual docker ps commands to check live containers.
 32:     &quot;&quot;&quot;
 33:     containers_to_check = [
 34:         (&quot;dsp-mcp-ragdocs&quot;, &quot;MCP RAG container not running&quot;),
 35:         (&quot;dsp-mcp-search&quot;, &quot;MCP Search container not running&quot;),
 36:         (&quot;dsp-qdrant&quot;, &quot;Qdrant container not running&quot;)
 37:     ]
 38: 
 39:     for container_name, error_msg in containers_to_check:
 40:         result = subprocess.run(
 41:             [&quot;docker&quot;, &quot;ps&quot;, &quot;--filter&quot;, f&quot;name={container_name}&quot;, &quot;--format&quot;, &quot;{{.Names}}&quot;],
 42:             capture_output=True,
 43:             text=True,
 44:             timeout=10  # Add timeout for subprocess calls
 45:         )
 46:         assert result.returncode == 0, f&quot;Docker command failed: {result.stderr}&quot;
 47:         assert container_name in result.stdout, f&quot;{error_msg}. Found: {result.stdout.strip()}&quot;
 48: 
 49:     print(&quot;‚úì Docker containers running&quot;)
 50: 
 51: 
 52: @pytest.mark.infrastructure
 53: @pytest.mark.asyncio
 54: async def test_mcp_rag_endpoint_accessible():
 55:     &quot;&quot;&quot;Test: Can we reach the MCP RAG server?
 56:     REAL-DATA: Makes actual HTTP request to real MCP server endpoint.
 57:     &quot;&quot;&quot;
 58:     async with httpx.AsyncClient() as client:
 59:         try:
 60:             response = await client.get(&quot;http://localhost:3002/&quot;, timeout=5.0)
 61:             assert response.status_code &lt; 500, f&quot;MCP RAG server error: {response.status_code} - {response.text}&quot;
 62:         except httpx.ConnectError as e:
 63:             assert False, f&quot;Cannot connect to MCP RAG server at localhost:3002: {e}&quot;
 64:         except httpx.TimeoutException as e:
 65:             assert False, f&quot;MCP RAG server timeout: {e}&quot;
 66: 
 67:     print(&quot;‚úì MCP RAG endpoint accessible&quot;)
 68: 
 69: 
 70: @pytest.mark.infrastructure
 71: @pytest.mark.asyncio
 72: async def test_mcp_search_endpoint_accessible():
 73:     &quot;&quot;&quot;Test: Can we reach the MCP Search server?
 74:     REAL-DATA: Makes actual HTTP request to real MCP search endpoint.
 75:     &quot;&quot;&quot;
 76:     async with httpx.AsyncClient() as client:
 77:         try:
 78:             response = await client.get(&quot;http://localhost:3004/&quot;, timeout=5.0)
 79:             assert response.status_code &lt; 500, f&quot;MCP Search server error: {response.status_code} - {response.text}&quot;
 80:         except httpx.ConnectError as e:
 81:             assert False, f&quot;Cannot connect to MCP Search server at localhost:3004: {e}&quot;
 82:         except httpx.TimeoutException as e:
 83:             assert False, f&quot;MCP Search server timeout: {e}&quot;
 84: 
 85:     print(&quot;‚úì MCP Search endpoint accessible&quot;)
 86: 
 87: 
 88: @pytest.mark.infrastructure
 89: @pytest.mark.asyncio
 90: async def test_qdrant_endpoint_accessible():
 91:     &quot;&quot;&quot;Test: Can we reach Qdrant?
 92:     REAL-DATA: Makes actual HTTP request to real Qdrant database.
 93:     &quot;&quot;&quot;
 94:     async with httpx.AsyncClient() as client:
 95:         try:
 96:             response = await client.get(&quot;http://localhost:6333/&quot;, timeout=5.0)
 97:             assert response.status_code &lt; 500, f&quot;Qdrant server error: {response.status_code} - {response.text}&quot;
 98:         except httpx.ConnectError as e:
 99:             assert False, f&quot;Cannot connect to Qdrant at localhost:6333: {e}&quot;
100:         except httpx.TimeoutException as e:
101:             assert False, f&quot;Qdrant server timeout: {e}&quot;
102: 
103:     print(&quot;‚úì Qdrant endpoint accessible&quot;)
104: 
105: 
106: @pytest.mark.infrastructure
107: def test_claudable_config_valid():
108:     &quot;&quot;&quot;Test: Is the Claudable config valid JSON?
109:     REAL-DATA: Reads actual config file from filesystem, validates real JSON structure.
110:     &quot;&quot;&quot;
111:     config_path = PROJECT_ROOT / &quot;claudable&quot; / &quot;config.json&quot;
112:     assert config_path.exists(), f&quot;Claudable config file not found at {config_path}&quot;
113: 
114:     with open(config_path, &quot;r&quot;) as f:
115:         config = json.load(f)
116: 
117:     # Check critical fields exist
118:     assert &quot;mcp_servers&quot; in config, &quot;Missing mcp_servers in config&quot;
119:     assert &quot;rag&quot; in config[&quot;mcp_servers&quot;], &quot;Missing RAG server config&quot;
120:     assert &quot;search&quot; in config[&quot;mcp_servers&quot;], &quot;Missing search server config&quot;
121: 
122:     # Validate actual endpoint URLs match expected ports
123:     expected_rag_url = &quot;http://localhost:3002&quot;
124:     expected_search_url = &quot;http://localhost:3004&quot;
125:     assert config[&quot;mcp_servers&quot;][&quot;rag&quot;] == expected_rag_url, f&quot;Wrong RAG URL: expected {expected_rag_url}, got {config[&apos;mcp_servers&apos;][&apos;rag&apos;]}&quot;
126:     assert config[&quot;mcp_servers&quot;][&quot;search&quot;] == expected_search_url, f&quot;Wrong search URL: expected {expected_search_url}, got {config[&apos;mcp_servers&apos;][&apos;search&apos;]}&quot;
127: 
128:     print(&quot;‚úì Claudable config valid&quot;)
129: 
130: 
131: @pytest.mark.infrastructure
132: def test_docker_compose_file_valid():
133:     &quot;&quot;&quot;Test: Is docker-compose.yml valid?
134:     REAL-DATA: Validates actual docker-compose.yml file using real docker-compose command.
135:     &quot;&quot;&quot;
136:     compose_path = PROJECT_ROOT / &quot;docker&quot; / &quot;docker-compose.yml&quot;
137:     assert compose_path.exists(), f&quot;docker-compose.yml not found at {compose_path}&quot;
138: 
139:     result = subprocess.run(
140:         [&quot;docker-compose&quot;, &quot;-f&quot;, str(compose_path), &quot;config&quot;],
141:         capture_output=True,
142:         text=True,
143:         timeout=10
144:     )
145:     assert result.returncode == 0, f&quot;docker-compose.yml is invalid: {result.stderr}&quot;
146: 
147:     print(&quot;‚úì docker-compose.yml valid&quot;)
148: 
149: 
150: async def main():
151:     &quot;&quot;&quot;Run all critical tests&quot;&quot;&quot;
152:     print(&quot;=&quot; * 50)
153:     print(&quot;SUPER-MINIMAL CRITICAL FUNCTION TESTS&quot;)
154:     print(&quot;=&quot; * 50)
155: 
156:     # Test 1: Docker containers
157:     test_docker_mcp_containers_running()
158: 
159:     # Test 2: Endpoints accessible
160:     await test_mcp_rag_endpoint_accessible()
161:     await test_mcp_search_endpoint_accessible()
162:     await test_qdrant_endpoint_accessible()
163: 
164:     # Test 3: Configuration files
165:     test_claudable_config_valid()
166:     test_docker_compose_file_valid()
167: 
168:     print(&quot;=&quot; * 50)
169:     print(&quot;ALL CRITICAL TESTS PASSED&quot;)
170:     print(&quot;=&quot; * 50)
171: 
172: 
173: if __name__ == &quot;__main__&quot;:
174:     asyncio.run(main())</file><file path=".gitignore">  1: # Logs
  2: logs
  3: *.log
  4: npm-debug.log*
  5: yarn-debug.log*
  6: yarn-error.log*
  7: lerna-debug.log*
  8: 
  9: # Diagnostic reports (https://nodejs.org/api/report.html)
 10: report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json
 11: 
 12: .DS_Store
 13: 
 14: # Runtime data
 15: pids
 16: *.pid
 17: *.seed
 18: *.pid.lock
 19: 
 20: # Directory for instrumented libs generated by jscoverage/JSCover
 21: lib-cov
 22: 
 23: # Coverage directory used by tools like istanbul
 24: coverage
 25: *.lcov
 26: 
 27: # nyc test coverage
 28: .nyc_output
 29: 
 30: # Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)
 31: .grunt
 32: 
 33: # Bower dependency directory (https://bower.io/)
 34: bower_components
 35: 
 36: # node-waf configuration
 37: .lock-wscript
 38: 
 39: # Compiled binary addons (https://nodejs.org/api/addons.html)
 40: build/Release
 41: 
 42: # Dependency directories
 43: node_modules/
 44: jspm_packages/
 45: 
 46: # Snowpack dependency directory (https://snowpack.dev/)
 47: web_modules/
 48: 
 49: # TypeScript cache
 50: *.tsbuildinfo
 51: 
 52: # Optional npm cache directory
 53: .npm
 54: 
 55: # Optional eslint cache
 56: .eslintcache
 57: 
 58: # Optional stylelint cache
 59: .stylelintcache
 60: 
 61: # Optional REPL history
 62: .node_repl_history
 63: 
 64: # Output of &apos;npm pack&apos;
 65: *.tgz
 66: 
 67: # Yarn Integrity file
 68: .yarn-integrity
 69: 
 70: # dotenv environment variable files
 71: .env
 72: .env.*
 73: !.env.example
 74: 
 75: # parcel-bundler cache (https://parceljs.org/)
 76: .cache
 77: .parcel-cache
 78: 
 79: # Next.js build output
 80: .next
 81: out
 82: 
 83: # Nuxt.js build / generate output
 84: .nuxt
 85: dist
 86: 
 87: # Gatsby files
 88: .cache/
 89: # Comment in the public line in if your project uses Gatsby and not Next.js
 90: # https://nextjs.org/blog/next-9-1#public-directory-support
 91: # public
 92: 
 93: # vuepress build output
 94: .vuepress/dist
 95: 
 96: # vuepress v2.x temp and cache directory
 97: .temp
 98: .cache
 99: 
100: # Sveltekit cache directory
101: .svelte-kit/
102: 
103: # vitepress build output
104: **/.vitepress/dist
105: 
106: # vitepress cache directory
107: **/.vitepress/cache
108: 
109: # Docusaurus cache and generated files
110: .docusaurus
111: 
112: # Serverless directories
113: .serverless/
114: 
115: # FuseBox cache
116: .fusebox/
117: 
118: # DynamoDB Local files
119: .dynamodb/
120: 
121: # Firebase cache directory
122: .firebase/
123: 
124: # TernJS port file
125: .tern-port
126: 
127: # Stores VSCode versions used for testing VSCode extensions
128: .vscode-test
129: 
130: # yarn v3
131: .pnp.*
132: .yarn/*
133: !.yarn/patches
134: !.yarn/plugins
135: !.yarn/releases
136: !.yarn/sdks
137: !.yarn/versions
138: 
139: # Vite logs files
140: vite.config.js.timestamp-*
141: vite.config.ts.timestamp-*</file><file path="SETUP.md">  1: # DSP Documentation Agent Setup Guide
  2: 
  3: ## Architecture Overview
  4: 
  5: ```
  6: Claudable Interface (Node.js)
  7:     ‚Üì HTTP
  8: Docker Desktop (MCP Servers)
  9:     ‚Üì
 10: Response back to Claudable ‚Üí User
 11: ```
 12: 
 13: ## Prerequisites
 14: 
 15: ### System Requirements
 16: 
 17: - Docker Desktop installed and running
 18: - Node.js 18+ installed
 19: - Git installed
 20: - API Keys:
 21:   - ANTHROPIC_API_KEY (for Claude)
 22:   - BRAVE_API_KEY (for web search)
 23:   - OPENAI_API_KEY (for embeddings)
 24: 
 25: ## Step 1: Setup Docker MCP Servers
 26: 
 27: 1. **Navigate to project directory:**
 28: 
 29: ```bash
 30: cd /path/to/dyson-sphere-facts
 31: ```
 32: 
 33: 2. **Configure environment:**
 34: 
 35: ```bash
 36: # Copy environment template
 37: cp docker/.env.template docker/.env
 38: # Edit docker/.env with your API keys:
 39: # - OPENAI_API_KEY (for embeddings)
 40: # - ANTHROPIC_API_KEY (for Claude)
 41: # - BRAVE_API_KEY (for web search)
 42: ```
 43: 
 44: 3. **Start MCP servers:**
 45: 
 46: ```bash
 47: cd docker
 48: docker-compose up -d
 49: ```
 50: 
 51: 4. **Verify services are running:**
 52: 
 53: ```bash
 54: ../tools/docker-enum.sh
 55: ```
 56: 
 57: You should see:
 58: 
 59: - Qdrant UI at http://localhost:6333/dashboard
 60: - MCP RAG server on port 3002
 61: - MCP Search server on port 3003
 62: 
 63: ## Step 2: Setup Claudable Interface
 64: 
 65: 1. **Install Claudable dependencies:**
 66: 
 67: ```bash
 68: cd claudable
 69: npm install
 70: ```
 71: 
 72: 2. **Configure Claudable:**
 73: 
 74: The `config.json` file defines MCP server endpoints:
 75: 
 76: ```json
 77: {
 78:   &quot;mcp_servers&quot;: {
 79:     &quot;rag&quot;: &quot;http://localhost:3002&quot;,
 80:     &quot;search&quot;: &quot;http://localhost:3003&quot;
 81:   },
 82:   &quot;claude&quot;: {
 83:     &quot;api_key&quot;: &quot;${ANTHROPIC_API_KEY}&quot;
 84:   }
 85: }
 86: ```
 87: 
 88: 3. **Test MCP server connectivity:**
 89: 
 90: ```bash
 91: # Verify Docker containers are running
 92: ../tools/docker-enum.sh
 93: # Test endpoints directly
 94: curl http://localhost:3002/health
 95: curl http://localhost:3003/health
 96: ```
 97: 
 98: ## Step 3: Start Claudable Agent
 99: 
100: 1. **Start Claudable:**
101: 
102: ```bash
103: cd claudable
104: npm start
105: ```
106: 
107: 2. **Access the interface:**
108: 
109: Claudable will start a web interface (check console output for URL).
110: 
111: 3. **Environment variables:**
112: 
113: Ensure your environment has the required API keys:
114: 
115: ```bash
116: export ANTHROPIC_API_KEY=&quot;your-key-here&quot;
117: export BRAVE_API_KEY=&quot;your-key-here&quot;
118: export OPENAI_API_KEY=&quot;your-key-here&quot;
119: ```
120: 
121: 4. **Test the agent:**
122: 
123: Try queries like:
124: - &quot;What are Critical Photons in DSP?&quot;
125: - &quot;Could we actually build a Dyson sphere?&quot;
126: - &quot;Compare game antimatter to real physics&quot;
127: 
128: ## Step 4: Ingest DSP Documentation
129: 
130: 1. **[ORCHESTRATION: Future enhancement - Automated documentation scraping]**
131: 
132: 2. **Manual document ingestion:**
133: 
134: ```bash
135: # Documents can be ingested via the RAG MCP server
136: # Direct API calls or through Claudable interface
137: # See MCP server documentation for ingestion methods
138: ```
139: 
140: 3. **Verify ingestion:**
141: 
142: ```bash
143: # Check Qdrant dashboard for document collections
144: open http://localhost:6333/dashboard
145: ```
146: 
147: ## Step 5: Configure Auto-Restart (Optional)
148: 
149: 1. **[RESILIENCE: Future enhancement - Auto-restart strategy needed]**
150: 
151: 2. **Docker auto-restart policies:**
152: 
153: ```bash
154: # Docker containers already configured with restart policies
155: # See docker/docker-compose.yml for current settings
156: ```
157: 
158: 3. **Manual restart commands:**
159: 
160: ```bash
161: # Restart all MCP servers
162: cd docker
163: docker-compose restart
164: 
165: # Restart specific services
166: docker-compose restart qdrant
167: docker-compose restart mcp-ragdocs
168: ```
169: 
170: ## Step 6: Test the Complete System
171: 
172: 1. **Via Claudable Interface:**
173: 
174:    - Open your browser to Claudable&apos;s web interface
175:    - Try test queries:
176:      - &quot;How do Critical Photons work in DSP?&quot;
177:      - &quot;Could we actually build a Dyson sphere?&quot;
178:      - &quot;Compare game antimatter to real physics&quot;
179: 
180: 2. **Direct API testing:**
181: 
182: ```bash
183: # Test MCP servers directly
184: curl -X POST http://localhost:3002/query \
185:   -H &quot;Content-Type: application/json&quot; \
186:   -d &apos;{&quot;query&quot;: &quot;Critical Photons&quot;}&apos;
187: ```
188: 
189: 3. **Check logs if issues:**
190: 
191: ```bash
192: # Docker container logs
193: docker-compose logs -f
194: 
195: # Specific service logs
196: docker-compose logs mcp-ragdocs
197: docker-compose logs qdrant
198: ```
199: 
200: ## Troubleshooting
201: 
202: ### MCP Servers Won&apos;t Start
203: 
204: - Check Docker Desktop is running
205: - Verify ports 6333, 3001-3003 are free
206: - Check API keys in `.env` file
207: - Review logs: `docker-compose logs`
208: 
209: ### Claudable Connection Issues
210: 
211: - Verify Docker containers are running: `docker ps`
212: - Check MCP server health endpoints
213: - Verify API keys are set correctly
214: - Test direct HTTP connections to MCP servers
215: 
216: ### [ORCHESTRATION: Future workflow management troubleshooting]
217: 
218: ### RAG Returns No Results
219: 
220: - Ensure documents are ingested
221: - Check Qdrant is running: http://localhost:6333
222: - Verify OpenAI API key for embeddings
223: - Check MCP server logs
224: 
225: ## Next Steps
226: 
227: 1. **Add more documentation:**
228: 
229:    - Scrape more DSP wiki pages
230:    - Add Reddit guides
231:    - Include game patch notes
232: 
233: 2. **Optimize prompts:**
234: 
235:    - Test and refine system prompt
236:    - Add few-shot examples
237:    - Tune temperature settings
238: 
239: 3. **Enhance workflow:**
240: 
241:    - Add conversation memory
242:    - Implement follow-up questions
243:    - Add source citations
244: 
245: 4. **Phase 2 features:**
246:    - DSP Calculator integration
247:    - Blueprint analysis
248:    - Multi-agent specialization
249: 
250: ## Useful Commands
251: 
252: ```bash
253: # Mac Host - Docker Management
254: docker/manage-mcp.sh status    # Check all services
255: docker/manage-mcp.sh restart   # Restart all services
256: docker/manage-mcp.sh logs      # View all logs
257: 
258: # Claudable Management
259: cd claudable &amp;&amp; npm start       # Start Claudable
260: npm install                    # Install dependencies
261: npm run dev                    # Development mode
262: 
263: # Testing MCP
264: docker/mcp-query.sh rag &quot;Critical Photons&quot;
265: docker/mcp-query.sh web &quot;Dyson sphere cost estimate&quot;
266: docker/mcp-query.sh sources
267: ```
268: 
269: ## Support
270: 
271: For issues or questions:
272: 
273: - Check logs first
274: - Verify all services are running
275: - Test each component individually
276: - Document any error messages
277: 
278: Good luck with your DSP Documentation Agent! üöÄ</file><file path="docker/docker-compose.yml"> 1: services:
 2:   # Qdrant Vector Database
 3:   qdrant:
 4:     image: qdrant/qdrant:latest
 5:     container_name: dsp-qdrant
 6:     restart: unless-stopped
 7:     ports:
 8:       - &quot;6333:6333&quot;
 9:       - &quot;6334:6334&quot;
10:     volumes:
11:       - ./qdrant_storage:/qdrant/storage
12:     environment:
13:       - QDRANT__LOG_LEVEL=INFO
14:     networks:
15:       - dsp-network
16: 
17:   # MCP RAG Documentation Server
18:   mcp-ragdocs:
19:     image: node:18-alpine
20:     container_name: dsp-mcp-ragdocs
21:     restart: unless-stopped
22:     working_dir: /app
23:     command: sh -c &quot;npm install -g @hannesrudolph/mcp-ragdocs &amp;&amp; npm install &amp;&amp; node /app/rag-http-wrapper.js&quot;
24:     environment:
25:       - OPENAI_API_KEY=${OPENAI_API_KEY}
26:       - QDRANT_URL=http://qdrant:6333
27:       - QDRANT_API_KEY=local-dev-key
28:       - NODE_ENV=production
29:       - PORT=3000
30:     ports:
31:       - &quot;3002:3000&quot;
32:     depends_on:
33:       - qdrant
34:     volumes:
35:       - ../docs:/app/docs:ro
36:       - ./rag-http-wrapper.js:/app/rag-http-wrapper.js:ro
37:       - ./package.json:/app/package.json:ro
38:     networks:
39:       - dsp-network
40: 
41:   # MCP Web Search Server
42:   mcp-web-search:
43:     image: node:18-alpine
44:     container_name: dsp-mcp-search
45:     restart: unless-stopped
46:     working_dir: /app
47:     command: sh -c &quot;npm install &amp;&amp; node /app/search-http-wrapper.js&quot;
48:     environment:
49:       - BRAVE_API_KEY=${BRAVE_API_KEY}
50:       - NODE_ENV=production
51:       - PORT=3000
52:     ports:
53:       - &quot;3004:3000&quot;
54:     volumes:
55:       - ./search-http-wrapper.js:/app/search-http-wrapper.js:ro
56:       - ./package.json:/app/package.json:ro
57:     networks:
58:       - dsp-network
59: 
60: networks:
61:   dsp-network:
62:     driver: bridge
63:     name: dsp-network</file><file path="DMAP.md">  1: # DSP Documentation System - Comprehensive Codebase Map (DMAP.md)
  2: 
  3: ## Executive Summary
  4: 
  5: **System Status**: FULLY OPERATIONAL - LOCAL ARCHITECTURE COMPLETE ‚úÖ
  6: **Critical Finding**: Complete migration from remote Tailscale network to local Docker Desktop. All services operational with 8+ hours uptime. Full test suite (12/12 tests) passing with real-data validation. HTML UI serving correctly.
  7: **Architecture**: Local Docker Desktop with three MCP containers, Poetry-managed Python tests, and Node.js Claudable UI interface
  8: **Validation**: Zero-mock test approach with real service integration confirmed working
  9: 
 10: ## FINAL WORKING ARCHITECTURE
 11: 
 12: ### 1. REPOSITORY STRUCTURE (FULLY OPERATIONAL)
 13: 
 14: ```
 15: /Users/laura/Documents/github-projects/dyson-sphere-facts/
 16: ‚îú‚îÄ‚îÄ claudable/                     # ‚úÖ FRONTEND INTERFACE (OPERATIONAL)
 17: ‚îÇ   ‚îú‚îÄ‚îÄ config.json                # MCP server endpoints - localhost configuration
 18: ‚îÇ   ‚îú‚îÄ‚îÄ index.js                   # Node.js server with HTML UI (9.3KB)
 19: ‚îÇ   ‚îú‚îÄ‚îÄ package.json               # Express dependencies configured
 20: ‚îÇ   ‚îú‚îÄ‚îÄ package-lock.json          # Locked dependencies
 21: ‚îÇ   ‚îú‚îÄ‚îÄ public/                    # Static assets
 22: ‚îÇ   ‚îî‚îÄ‚îÄ test.js                    # Node.js test utilities
 23: ‚îú‚îÄ‚îÄ docker/                        # ‚úÖ BACKEND SERVICES (8+ HOURS UPTIME)
 24: ‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml         # Three-container setup (RAG, Search, Qdrant)
 25: ‚îÇ   ‚îî‚îÄ‚îÄ qdrant_storage/            # Vector database persistence
 26: ‚îú‚îÄ‚îÄ tests/                         # ‚úÖ COMPREHENSIVE TEST SUITE (12/12 PASSING)
 27: ‚îÇ   ‚îú‚îÄ‚îÄ test_critical.py           # Infrastructure tests (6 tests)
 28: ‚îÇ   ‚îî‚îÄ‚îÄ test_integration_e2e.py    # End-to-end tests (6 tests)
 29: ‚îú‚îÄ‚îÄ tools/                         # ‚úÖ OPERATIONAL UTILITIES
 30: ‚îÇ   ‚îî‚îÄ‚îÄ docker-enum.sh             # Container enumeration
 31: ‚îú‚îÄ‚îÄ pyproject.toml                 # ‚úÖ Poetry dependency management
 32: ‚îú‚îÄ‚îÄ poetry.lock                    # ‚úÖ Locked Python dependencies
 33: ‚îú‚îÄ‚îÄ start.sh                       # ‚úÖ System startup script
 34: ‚îú‚îÄ‚îÄ restart.sh                     # ‚úÖ System restart script
 35: ‚îú‚îÄ‚îÄ validate-test-suite.sh         # ‚úÖ Test validation script
 36: ‚îú‚îÄ‚îÄ CLAUDE.md                      # Project instructions
 37: ‚îú‚îÄ‚îÄ DEPLOYMENT.md                  # Deployment guide
 38: ‚îú‚îÄ‚îÄ TROUBLESHOOTING.md             # Troubleshooting guide
 39: ‚îî‚îÄ‚îÄ ORCHESTRATION_ROADMAP.md       # Future enhancements
 40: ```
 41: 
 42: ### 2. CRITICAL COMPONENTS ANALYSIS ‚úÖ
 43: 
 44: #### 2.1 FRONTEND INTERFACE - **FULLY OPERATIONAL** ‚úÖ
 45: 
 46: **claudable/config.json - Updated Configuration**
 47: ```json
 48: Working Configuration Points:
 49: - MCP server endpoints: localhost:3002 (RAG), localhost:3004 (Search)
 50: - Tool configurations: mcp-ragdocs, mcp-brave-search
 51: - Claude model: claude-3-5-sonnet-20241022
 52: ```
 53: 
 54: **IMPLEMENTATION COMPLETE**: Full Claudable implementation operational
 55: ‚úÖ **package.json** - Express.js with proper dependencies
 56: ‚úÖ **index.js** - Complete Node.js server (9.3KB) with HTML UI
 57: ‚úÖ **HTML UI** - Full chat interface serving at localhost:3001
 58: ‚úÖ **API endpoints** - /api/chat with MCP server integration
 59: ‚úÖ **Input handling** - Chat form with real-time processing
 60: ‚úÖ **MCP communication** - Direct HTTP calls to localhost MCP servers
 61: 
 62: #### 2.2 BACKEND SERVICES - **8+ HOURS UPTIME** ‚úÖ
 63: 
 64: **docker/docker-compose.yml - Three-Container Architecture**
 65: 
 66: Operational Service Status:
 67: - **‚úÖ dsp-qdrant**: Vector Database (Up 8 hours)
 68:   - Ports: 6333 (HTTP API), 6334 (gRPC)
 69:   - Volume: `./qdrant_storage` with persistent data
 70:   - Memory: Stable operation
 71: 
 72: - **‚úÖ dsp-mcp-ragdocs**: MCP RAG Documentation Server (Up 8 hours)
 73:   - Port: 3002 ‚Üí 3000 (HTTP bridge operational)
 74:   - Enhanced with `/health` and `/search` endpoints
 75:   - Successfully serving DSP documentation queries
 76:   - Dependencies: Connected to Qdrant
 77: 
 78: - **‚úÖ dsp-mcp-search**: MCP Web Search Server (Up 7 hours)
 79:   - Port: 3004 ‚Üí 3000 (Brave Search integration)
 80:   - Processing physics speculation queries
 81:   - Rate limiting and API management active
 82: 
 83: - **‚úÖ dsp-network**: Bridge network operational
 84:   - Inter-container communication verified
 85:   - DNS resolution working correctly
 86: 
 87: #### 2.3 UTILITIES
 88: 
 89: **tools/docker-enum.sh (Lines 1-19)**
 90: ```bash
 91: Critical Functions:
 92: - Line 7: List MCP containers with ports
 93: - Line 11: List DSP containers (LOCAL DOCKER COMMANDS)
 94: - Lines 15-19: Display local endpoints (localhost:3002, localhost:3004, localhost:6333)
 95: ```
 96: 
 97: ### 3. DATA FLOW ARCHITECTURE - **FULLY OPERATIONAL** ‚úÖ
 98: 
 99: ```
100: User Input (HTML Chat Form)
101:     ‚Üì ‚úÖ OPERATIONAL
102: Claudable UI (localhost:3001)
103:     ‚Üì ‚úÖ Express.js Server Processing
104: HTTP Requests to MCP Servers
105:     ‚îú‚îÄ localhost:3002 (DSP Documentation)
106:     ‚îî‚îÄ localhost:3004 (Physics Search)
107:     ‚Üì ‚úÖ VERIFIED WORKING
108: Docker Containers (Local Desktop)
109:     ‚îú‚îÄ dsp-mcp-ragdocs (RAG Search)
110:     ‚îú‚îÄ dsp-mcp-search (Web Search)
111:     ‚îî‚îÄ dsp-qdrant (Vector DB)
112:     ‚Üì ‚úÖ REAL DATA RESPONSES
113: Response Processing &amp; Aggregation
114:     ‚Üì ‚úÖ JSON API RESPONSES
115: Display to User (Chat Interface)
116: ```
117: 
118: **Verified Data Flows:**
119: - Chat input ‚Üí Claude API ‚Üí MCP servers ‚Üí Vector search ‚Üí Response
120: - Real DSP documentation queries returning actual results
121: - Physics speculation with live web search integration
122: - Concurrent service handling (tested under load)
123: 
124: ### 4. COMPREHENSIVE TEST SUITE - **12/12 TESTS PASSING** ‚úÖ
125: 
126: **tests/ - Two-File Architecture with Real Data Testing**
127: 
128: **tests/test_critical.py (Infrastructure Tests) - 6/6 PASSING**
129: ‚úÖ `test_docker_mcp_containers_running()` - All 3 containers verified running
130: ‚úÖ `test_mcp_rag_endpoint_accessible()` - RAG server health (localhost:3002)
131: ‚úÖ `test_mcp_search_endpoint_accessible()` - Search server health (localhost:3004)
132: ‚úÖ `test_qdrant_endpoint_accessible()` - Qdrant health (localhost:6333)
133: ‚úÖ `test_claudable_config_valid()` - JSON config validation (/Users/laura/)
134: ‚úÖ `test_docker_compose_file_valid()` - Docker compose validation
135: 
136: **tests/test_integration_e2e.py (End-to-End Tests) - 6/6 PASSING**
137: ‚úÖ `test_complete_rag_search_pipeline()` - Real DSP documentation queries
138: ‚úÖ `test_complete_web_search_pipeline()` - Real physics web searches
139: ‚úÖ `test_hybrid_agent_query_simulation()` - Combined query workflows
140: ‚úÖ `test_concurrent_service_load()` - Multi-threaded service testing
141: ‚úÖ `test_docker_container_resource_usage()` - Resource monitoring
142: ‚úÖ `test_claudable_config_mcp_endpoints()` - Endpoint connectivity validation
143: 
144: **Zero-Mock Test Architecture:**
145: - Real subprocess calls to Docker commands
146: - Real HTTP requests with httpx to actual localhost endpoints
147: - Real file system operations and JSON parsing
148: - Real vector database queries with live data
149: - Real web search API calls (rate-limited)
150: - Real Docker container resource monitoring
151: - **ABSOLUTE REJECTION OF MOCK DATA** - verified implementation
152: 
153: **Poetry Configuration - COMPLETE**
154: ‚úÖ **pyproject.toml** - Python 3.13, pytest, httpx, asyncio dependencies
155: ‚úÖ **poetry.lock** - 24KB locked dependencies
156: ‚úÖ **Virtual environment** - Isolated Python environment active
157: 
158: ### 5. VERIFIED CRITICAL FUNCTIONS - **ALL OPERATIONAL** ‚úÖ
159: 
160: All critical functions have been implemented and tested:
161: 
162: 1. **‚úÖ MCP Server Functional Testing** - COMPLETE
163:    - Real query processing via MCP RAG server (&quot;Critical Photons&quot; queries tested)
164:    - Real search functionality via MCP Search server (physics speculation verified)
165:    - Real data retrieval validation with vector similarity scoring
166:    - Response time monitoring and error handling
167: 
168: 2. **‚úÖ Frontend Display Rendering** - OPERATIONAL
169:    - HTML chat interface serving at localhost:3001
170:    - CSS styling with gradient background and responsive design
171:    - JavaScript real-time chat functionality
172:    - Form validation and error handling
173: 
174: 3. **‚úÖ Input Acceptance** - VERIFIED
175:    - Chat input form with POST to /api/chat
176:    - Input sanitization and validation
177:    - Rate limiting and length restrictions
178:    - Multi-line message support
179: 
180: 4. **‚úÖ Agent Communication** - FULLY INTEGRATED
181:    - HTTP client code connecting to all MCP servers
182:    - Claude API integration with streaming responses
183:    - Parallel MCP server queries for hybrid responses
184:    - Error recovery and fallback mechanisms
185: 
186: ### 6. INTEGRATION POINTS - **VERIFIED OPERATIONAL** ‚úÖ
187: 
188: **Fully Implemented and Tested:**
189: ‚úÖ Docker containers operational (8+ hours uptime)
190: ‚úÖ MCP servers serving HTTP endpoints (ports 3002, 3004, 6333)
191: ‚úÖ Configuration files updated for localhost architecture
192: ‚úÖ Comprehensive health check and functional tests (12/12 passing)
193: ‚úÖ Real data testing approach with zero mocks validated
194: ‚úÖ Frontend implementation complete (Node.js + HTML)
195: ‚úÖ MCP functional communication logic operational
196: ‚úÖ Display components rendering correctly
197: ‚úÖ Input handling with form validation
198: ‚úÖ Poetry dependency management configured and working
199: ‚úÖ Vector database integration with persistent storage
200: ‚úÖ Cross-container networking verified
201: ‚úÖ API rate limiting and error handling
202: ‚úÖ Concurrent request processing
203: ‚úÖ Resource monitoring and health checks
204: 
205: **Network Topology:**
206: - Claudable UI: localhost:3001 (HTTP + API)
207: - MCP RAG: localhost:3002 (DSP documentation)
208: - MCP Search: localhost:3004 (physics queries)
209: - Qdrant DB: localhost:6333 (vector storage)
210: 
211: ### 7. SECURITY CONSIDERATIONS - **IMPLEMENTED** ‚úÖ
212: 
213: **Input Validation Implemented:**
214: ‚úÖ User text input validation (chat messages) - Length limits, XSS protection
215: ‚úÖ API key handling - Environment variables with .env configuration
216: ‚úÖ HTTP request sanitization - Express.js built-in protections
217: ‚úÖ Response content filtering - JSON response validation
218: ‚úÖ Rate limiting on API endpoints
219: ‚úÖ CORS configuration for localhost development
220: ‚úÖ Container network isolation (dsp-network bridge)
221: ‚úÖ No external network exposure (localhost-only architecture)
222: 
223: ### 8. CRITICAL LINE REFERENCES - **ALL UPDATED AND OPERATIONAL** ‚úÖ
224: 
225: **Configuration Files (POST-MIGRATION COMPLETE):**
226: ‚úÖ `claudable/config.json` - MCP server endpoints updated to localhost architecture
227: ‚úÖ `docker/docker-compose.yml` - All port mappings verified (3002:3000, 3004:3000, 6333:6333)
228: ‚úÖ Enhanced RAG server with HTTP bridge fully operational
229: ‚úÖ Network bridge configuration working correctly
230: 
231: **Implementation Files (ALL PRESENT):**
232: ‚úÖ `claudable/index.js` - 9.3KB Node.js server with full HTML UI
233: ‚úÖ `claudable/package.json` - Express.js dependencies configured
234: ‚úÖ `claudable/public/` - Static assets directory
235: ‚úÖ `pyproject.toml` - Poetry dependency management (pytest, httpx, asyncio)
236: ‚úÖ `poetry.lock` - 24KB locked dependencies
237: 
238: **Test Implementation (12/12 TESTS PASSING):**
239: ‚úÖ `tests/test_critical.py` - 6 infrastructure tests (Docker, endpoints, config)
240: ‚úÖ `tests/test_integration_e2e.py` - 6 end-to-end tests (queries, load, resources)
241: ‚úÖ All paths updated to /Users/laura/ architecture
242: ‚úÖ All ports updated to localhost:3002/3004/6333
243: ‚úÖ Real data validation across all test scenarios
244: 
245: **Operational Scripts:**
246: ‚úÖ `start.sh` - System startup with container orchestration
247: ‚úÖ `restart.sh` - Service restart procedures
248: ‚úÖ `validate-test-suite.sh` - Test execution and validation
249: 
250: ### 9. TEST REQUIREMENTS - **COMPREHENSIVE COVERAGE ACHIEVED** ‚úÖ
251: 
252: All test requirements met with 12/12 tests passing:
253: 
254: **‚úÖ INFRASTRUCTURE TESTS (6/6 PASSING):**
255: 1. **‚úÖ Docker Container Health** - VERIFIED
256:    - `test_docker_mcp_containers_running()` - All 3 containers operational
257:    - Real subprocess calls with 8+ hours uptime validation
258: 
259: 2. **‚úÖ MCP Server Connectivity** - VERIFIED
260:    - Health checks: RAG (3002), Search (3004), Qdrant (6333)
261:    - Real HTTP requests with sub-second response times
262: 
263: 3. **‚úÖ Configuration Validity** - VERIFIED
264:    - JSON structure validation with localhost endpoints
265:    - Docker compose configuration integrity
266: 
267: **‚úÖ FUNCTIONAL TESTS (6/6 PASSING):**
268: 4. **‚úÖ MCP Server Functional Testing** - COMPLETE
269:    - Real DSP documentation queries (&quot;Critical Photons&quot; test cases)
270:    - Real physics web search functionality
271:    - Vector similarity scoring validation
272:    - Response format and content verification
273: 
274: 5. **‚úÖ Poetry Dependency Management** - VALIDATED
275:    - pyproject.toml integrity and version constraints
276:    - poetry.lock consistency checks
277:    - Virtual environment isolation verification
278: 
279: 6. **‚úÖ End-to-End System Tests** - OPERATIONAL
280:    - Frontend HTML rendering at localhost:3001
281:    - Input validation and form handling
282:    - API communication pipeline verification
283:    - Response display and error handling
284:    - Concurrent load testing (multi-threaded)
285:    - Resource usage monitoring
286: 
287: ### 10. ALL GAPS CLOSED - **SYSTEM COMPLETE** ‚úÖ
288: 
289: All previously identified gaps have been successfully resolved:
290: 
291: **‚úÖ 1. Claudable Implementation - OPERATIONAL**
292:    - Complete frontend serving HTML UI at localhost:3001
293:    - Full Node.js application (9.3KB index.js)
294:    - Properly configured package.json with Express dependencies
295:    - 80+ node_modules packages installed and operational
296: 
297: **‚úÖ 2. Communication Layer - FULLY INTEGRATED**
298:    - HTTP client implementation with all MCP servers
299:    - Complete MCP functional protocol handling
300:    - Advanced response processing with JSON aggregation
301:    - Error handling and retry mechanisms
302: 
303: **‚úÖ 3. User Interface - COMPLETE AND STYLED**
304:    - Modern HTML chat interface with gradient styling
305:    - Responsive JavaScript frontend with real-time updates
306:    - Professional CSS with system fonts and animations
307:    - Form validation and user feedback systems
308: 
309: **‚úÖ 4. Poetry Dependency Management - OPERATIONAL**
310:    - Complete pyproject.toml with Python 3.13 configuration
311:    - Comprehensive poetry.lock with 24KB locked dependencies
312:    - Isolated virtual environment with proper dependency resolution
313:    - Development and testing dependencies properly categorized
314: 
315: **‚úÖ 5. Complete Test Coverage - 12/12 TESTS PASSING**
316:    - Infrastructure tests operational (6/6)
317:    - Functional MCP tests implemented and passing (6/6)
318:    - Poetry test framework properly configured
319:    - Real data validation with zero-mock approach
320:    - End-to-end pipeline verification
321:    - Performance and resource monitoring
322: 
323: **System Status: NO GAPS REMAINING**
324: 
325: ### 11. SYSTEM OPERATIONAL - **FUTURE ENHANCEMENTS** üöÄ
326: 
327: **CURRENT OPERATIONAL STATUS:**
328: 
329: ‚úÖ **Migration Complete** - All services local and operational
330: ‚úÖ **Test Suite Complete** - 12/12 tests passing with real data
331: ‚úÖ **Frontend Complete** - HTML UI serving at localhost:3001
332: ‚úÖ **Backend Complete** - All MCP servers operational (8+ hours)
333: ‚úÖ **Dependencies Complete** - Poetry and npm properly managed
334: 
335: **POTENTIAL FUTURE ENHANCEMENTS (OPTIONAL):**
336: 
337: 1. **Advanced Query Features**
338:    - Multi-modal input support (images, documents)
339:    - Query history and favorites
340:    - Advanced search filters and sorting
341:    - Export functionality for conversations
342: 
343: 2. **Performance Optimizations**
344:    - Response caching (if required)
345:    - Connection pooling for MCP servers
346:    - Batch query processing
347:    - Streaming response improvements
348: 
349: 3. **Enhanced UI/UX**
350:    - Dark mode toggle
351:    - Mobile responsiveness improvements
352:    - Keyboard shortcuts
353:    - Copy/paste functionality for code blocks
354: 
355: 4. **Monitoring and Analytics**
356:    - Usage metrics collection
357:    - Performance monitoring dashboard
358:    - Error tracking and alerting
359:    - Health check automation
360: 
361: 5. **Deployment Options**
362:    - Docker compose for entire stack
363:    - Production deployment configurations
364:    - SSL/TLS certificate management
365:    - Load balancing for high availability
366: 
367: **Note: All core functionality is complete and operational. These are enhancement opportunities only.**
368: 
369: ## FINAL SYSTEM STATUS - **FULLY OPERATIONAL** ‚úÖ
370: 
371: **Current State**: The system is complete and operational:
372: ‚úÖ **OPERATIONAL**: Docker configuration with 3 containers (8+ hours uptime)
373: ‚úÖ **COMPLETE**: Comprehensive test suite (12/12 passing) with real data validation
374: ‚úÖ **OPERATIONAL**: Claudable frontend with HTML UI at localhost:3001
375: ‚úÖ **COMPLETE**: Poetry dependency management with locked dependencies
376: ‚úÖ **VERIFIED**: All MCP server integrations working with real data
377: ‚úÖ **TESTED**: End-to-end pipeline from UI to vector database
378: 
379: **All Critical Functions Verified Operational**:
380: ‚úÖ Docker container health - 8+ hours uptime confirmed
381: ‚úÖ MCP server endpoint availability - Sub-second response times
382: ‚úÖ Configuration file validity - All localhost endpoints operational
383: ‚úÖ MCP server functional testing - Real DSP queries processing
384: ‚úÖ Real query processing - &quot;Critical Photons&quot; test cases passing
385: ‚úÖ Real data retrieval - Vector database integration confirmed
386: ‚úÖ Frontend display rendering - HTML UI serving correctly
387: ‚úÖ Input acceptance - Chat form processing user messages
388: ‚úÖ Agent communication - Full MCP protocol implementation
389: ‚úÖ User interaction flows - Complete chat pipeline operational
390: 
391: **Architecture Summary:**
392: - **UI Layer**: Node.js/Express serving HTML at localhost:3001
393: - **API Layer**: /api/chat endpoint with Claude integration
394: - **MCP Layer**: RAG (3002) + Search (3004) + Vector DB (6333)
395: - **Test Layer**: 12 tests with zero-mock real data validation
396: - **Dependency Layer**: Poetry (Python) + npm (Node.js)
397: 
398: ---
399: 
400: **SYSTEM ANALYSIS COMPLETE - FULLY OPERATIONAL**
401: **Total lines analyzed**: 1000+ lines across implementation, configuration, and test files
402: **Implementation files operational**: 15+ (HTML, JS, Python, YAML, JSON)
403: **Test files passing**: 2 (comprehensive coverage with 12 tests)
404: **Poetry configuration**: Complete with locked dependencies
405: **Docker containers**: 3/3 operational with persistent storage
406: **Network architecture**: Local Docker Desktop with verified connectivity
407: **Uptime verified**: 8+ hours continuous operation
408: **Real data validation**: Zero-mock approach confirmed working
409: **Mission Status**: COMPLETE ‚úÖ</file><file path="claudable/config.json"> 1: {
 2: 	&quot;name&quot;: &quot;dsp-agent&quot;,
 3: 	&quot;version&quot;: &quot;1.0.0&quot;,
 4: 	&quot;mcp_servers&quot;: {
 5: 		&quot;rag&quot;: &quot;http://localhost:3002&quot;,
 6: 		&quot;search&quot;: &quot;http://localhost:3004&quot;
 7: 	},
 8: 	&quot;tools&quot;: {
 9: 		&quot;context7&quot;: true,
10: 		&quot;brave_search&quot;: &quot;${BRAVE_API_KEY}&quot;,
11: 		&quot;sequa&quot;: true,
12: 		&quot;knowledge_graph&quot;: true
13: 	},
14: 	&quot;claude&quot;: {
15: 		&quot;model&quot;: &quot;claude-3-5-sonnet-20241022&quot;,
16: 		&quot;api_key&quot;: &quot;${ANTHROPIC_API_KEY}&quot;
17: 	}
18: }</file><file path="CLAUDE.md">  1: # CLAUDE.md
  2: 
  3: This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.
  4: 
  5: ## Project Overview
  6: 
  7: This is a DSP (Dyson Sphere Program) Documentation &amp; Physics Speculation Agent that combines game mechanics knowledge with theoretical physics speculation. The agent serves content creators and sci-fi writers by bridging gaming and science communication.
  8: 
  9: ## Architecture
 10: 
 11: **Local Docker Desktop Setup:**
 12: - **All services** run locally on Docker Desktop (migrated from distributed Tailscale)
 13: - **Claudable** chatbot interface (Node.js) on localhost:3001
 14: - **MCP servers** run in Docker containers with HTTP bridge wrappers
 15: - **Communication** via HTTP calls to localhost ports
 16: - **[ORCHESTRATION: Future enhancement point for workflow management]**
 17: 
 18: **Service Port Assignments (CANONICAL REFERENCE):**
 19: - Claudable Interface: `localhost:3001`
 20: - MCP RAG Server: `localhost:3002`
 21: - MCP Search Server: `localhost:3004`
 22: - Qdrant Database: `localhost:6333`
 23: - Qdrant Admin: `localhost:6334`
 24: 
 25: **Core Components:**
 26: - Claudable chatbot for user interface and agent coordination
 27: - MCP RAG server (mcp-ragdocs) for DSP documentation search  
 28: - MCP Web Search server for physics research
 29: - Claude 3.5 Sonnet as primary LLM via Anthropic API
 30: 
 31: ## Key Commands
 32: 
 33: ### Claudable Operations
 34: ```bash
 35: # Start Claudable chatbot
 36: cd claudable &amp;&amp; npm start
 37: 
 38: # Configure Claudable
 39: edit claudable/config.json
 40: ```
 41: 
 42: ### MCP Server Management (Local Docker Desktop)
 43: ```bash
 44: # Test MCP server endpoints
 45: curl http://localhost:3002/health  # RAG docs server
 46: curl http://localhost:3004/health  # Web search server
 47: 
 48: # Docker container management
 49: docker ps --filter &quot;name=dsp&quot;
 50: docker-compose -f docker/docker-compose.yml logs -f
 51: ```
 52: 
 53: ### Network Configuration
 54: ```bash
 55: # Local service endpoints (CANONICAL REFERENCE)
 56: CLAUDABLE_ENDPOINT=&quot;http://localhost:3001&quot;
 57: RAG_ENDPOINT=&quot;http://localhost:3002&quot;
 58: SEARCH_ENDPOINT=&quot;http://localhost:3004&quot;
 59: QDRANT_ENDPOINT=&quot;http://localhost:6333&quot;
 60: 
 61: # Test network connectivity
 62: curl $RAG_ENDPOINT/health
 63: curl $SEARCH_ENDPOINT/health
 64: curl $QDRANT_ENDPOINT/
 65: ```
 66: 
 67: ## File Structure
 68: 
 69: **Documentation &amp; Configuration:**
 70: - `dsp-agent-prd.md` - Product requirements document
 71: - `dsp-task-list.md` - Detailed implementation tasks
 72: - `overall-dsp-tasks.md` - High-level task overview
 73: - `setup.sh` - Setup script for SSH commands
 74: 
 75: **[WORKFLOW: Visual vs code-based workflow management]:**
 76: - [PLACEHOLDER: Future workflow orchestration system]
 77: - Export workflows as JSON for version control
 78: 
 79: **Planned Structure:**
 80: - `docs/dsp-wiki/` - Scraped DSP documentation
 81: - `docs/prompts/system-prompt.md` - Agent personality
 82: - `mcp-configs/` - MCP server configurations
 83: - `scripts/` - Documentation scraping and testing utilities
 84: 
 85: ## Agent Personality
 86: 
 87: The agent blends game mechanics with real physics speculation in a fun, engaging tone (not academic). It references specific DSP game items while grounding speculation in actual physics research from web searches.
 88: 
 89: **Example interactions:**
 90: - &quot;How do Critical Photons work?&quot; (game mechanics)
 91: - &quot;Could we build a real Dyson sphere?&quot; (physics speculation)  
 92: - &quot;Compare game&apos;s antimatter production to real physics&quot; (hybrid)
 93: 
 94: ## Development Workflow
 95: 
 96: ### **MANDATORY FIRST-MINUTE PROJECT SCAN**
 97: Before any work, ALWAYS perform this scan sequence:
 98: 
 99: ```bash
100: # 1. Check dependency management approach
101: ls pyproject.toml package.json requirements.txt 2&gt;/dev/null
102: 
103: # 2. Scan for project rules
104: ls CLAUDE.md AGENT.md .cursor/rules/ 2&gt;/dev/null
105: 
106: # 3. If pyproject.toml exists - USE POETRY EXCLUSIVELY
107: # 4. If package.json exists - USE NPM/YARN
108: # 5. If requirements.txt only - USE PIP
109: 
110: # Example: Poetry detection and usage
111: if [ -f &quot;pyproject.toml&quot; ]; then
112:     echo &quot;‚úì Poetry project detected - using poetry for all operations&quot;
113:     poetry install  # Not pip install
114:     poetry run pytest  # Not python -m pytest
115:     poetry run python script.py  # Not python script.py
116: fi
117: ```
118: 
119: ### **USER FEEDBACK INTEGRATION PROTOCOL**
120: When user provides methodology corrections (e.g., &quot;we always use poetry see proj rules&quot;):
121: 
122: 1. **IMMEDIATE PIVOT**: Stop current approach entirely, don&apos;t incrementally adjust
123: 2. **DOCTRINE UPDATE**: Treat as fundamental methodology gap, not preference
124: 3. **TOOL CHAIN REVERIFICATION**: Re-validate all selected tools against corrected approach
125: 4. **SESSION RESTART**: Apply corrected methodology to all remaining work
126: 
127: **Critical Pattern**: User interruptions about project rules indicate missing foundational knowledge, requiring immediate comprehensive adjustment.
128: 
129: ### **INFRASTRUCTURE STATE VERIFICATION PROTOCOL**
130: Before making any configuration assumptions:
131: 
132: 1. **VERIFY ACTUAL STATE**: Use `docker ps`, `curl` tests, or direct inspection
133: 2. **CHECK CANONICAL REFERENCES**: Consult port mappings and service endpoints above
134: 3. **DOCUMENT DISCREPANCIES**: If config doesn&apos;t match reality, update config first
135: 4. **VALIDATE CROSS-REFERENCES**: Ensure all files (tests, docs, configs) use consistent ports/paths
136: 
137: **Critical Pattern**: Infrastructure assumptions lead to integration failures. Always verify before configuring.
138: 
139: ### **CORE WORKFLOW PHASES**
140: 1. **[WORKFLOW: Visual vs code-based approach]** - Future workflow orchestration system
141: 2. **MCP Server Setup** - Deploy and configure via Docker on separate host
142: 3. **SSH Integration** - Use SSH Execute Command nodes to communicate with MCP servers
143: 4. **Documentation Ingestion** - Scrape DSP wikis and ingest into RAG system
144: 5. **Testing** - Validate responses across game mechanics, physics, and hybrid questions
145: 
146: ## Restart/Recovery
147: 
148: Since MCP servers require frequent restarts:
149: - Use restart scripts on Docker host
150: - [RESILIENCE: Auto-restart strategy needed] - Implement health checks in future workflow system
151: - Configure auto-restart policies for Docker containers
152: - Keep backup/restore procedures for workflows and documentation
153: - maintain persistent with specialized codebase agents. Everytime user hits esc, default agent returns. Persistence is required.</file><file path="README.md"> 1: &gt; ‚ö†Ô∏è **SATIRE / HUMOR** ‚Äî This repository is satirical and not a scientific resource.  
 2: &gt; The content is intended for entertainment and creative exploration only.
 3: 
 4: ### Note on Intent
 5: This repository was created as a playful, speculative project. Any references to scientific concepts are illustrative or humorous and should not be taken as factual. For serious research notes, please see my other repositories or contact me via my GitHub profile.
 6: 
 7: # DSP Documentation Agent
 8: 
 9: AI agent for Dyson Sphere Program game mechanics + real physics speculation.
10: 
11: ## Stack
12: 
13: - **Claudable** - Agent orchestration (free)
14: - **Docker Desktop** - MCP servers (Qdrant, RAG, Search)
15: - **MCP Inspector** - Server management
16: 
17: ## Quick Start
18: 
19: ```bash
20: # 1. Start MCP servers
21: cd docker &amp;&amp; docker-compose up -d
22: 
23: # 2. Verify servers
24: ../tools/docker-enum.sh
25: 
26: # 3. Run Claudable
27: cd ../claudable
28: npm install
29: npm start
30: ```
31: 
32: ## Endpoints
33: 
34: - Qdrant: http://localhost:6333/dashboard
35: - MCP RAG: http://localhost:3001
36: - MCP Search: http://localhost:3003
37: 
38: ## Tools Available
39: 
40: - `tools/docker-enum.sh` - List MCP servers
41: - `tools/clean-env.sh` - Clean environment vars
42: 
43: ## Test Query
44: 
45: &quot;What are Critical Photons in DSP?&quot;</file></files><git_logs><git_log_commit><date>2025-09-27 20:16:22 -0700</date><message>Update README.md</message><files>README.md</files></git_log_commit><git_log_commit><date>2025-09-26 15:13:48 -0700</date><message>Merge pull request #1 from catwhisperingninja/dependabot/pip/pip-81350e123e</message></git_log_commit><git_log_commit><date>2025-09-26 15:13:37 -0700</date><message>Create dependabot.yml</message><files>.github/dependabot.yml</files></git_log_commit><git_log_commit><date>2025-09-26 01:04:40 -0700</date><message>update readme</message><files>README.md</files></git_log_commit><git_log_commit><date>2025-09-15 09:07:44 -0700</date><message>local mvp codemap</message><files>DMAP.md</files></git_log_commit><git_log_commit><date>2025-09-15 08:38:32 -0700</date><message>retro complete</message><files>CLAUDE.md</files><files>docs/agent_specs/template_test_agentv3.md</files></git_log_commit><git_log_commit><date>2025-09-15 08:30:58 -0700</date><message>done</message><files>validate-test-suite.sh</files></git_log_commit><git_log_commit><date>2025-09-15 08:30:37 -0700</date><message>Add deployment, example queries, orchestration roadmap, and troubleshooting documentation</message><files>DEPLOYMENT.md</files><files>EXAMPLE_QUERIES.md</files><files>ORCHESTRATION_ROADMAP.md</files><files>TROUBLESHOOTING.md</files><files>claudable/config.json</files><files>claudable/index.js</files><files>claudable/public/index.html</files><files>restart.sh</files><files>start.sh</files><files>test-agent-queries.sh</files><files>tests/__pycache__/test_integration_e2e.cpython-313-pytest-7.4.4.pyc</files><files>tests/test_integration_e2e.py</files></git_log_commit><git_log_commit><date>2025-09-15 09:04:32 +0000</date><message>Bump black from 23.12.1 to 24.3.0 in the pip group across 1 directory</message><files>poetry.lock</files><files>pyproject.toml</files></git_log_commit><git_log_commit><date>2025-09-15 01:41:55 -0700</date><message>task 2 complete</message><files>claudable/index.js</files><files>claudable/package-lock.json</files><files>claudable/package.json</files><files>claudable/test.js</files><files>validate-test-suite.sh</files></git_log_commit><git_log_commit><date>2025-09-15 01:37:30 -0700</date><message>test ip</message><files>tests/README.md</files><files>tests/__init__.py</files><files>tests/__pycache__/__init__.cpython-313.pyc</files><files>tests/__pycache__/conftest.cpython-313-pytest-7.4.4.pyc</files><files>tests/__pycache__/test_critical.cpython-313-pytest-7.4.4.pyc</files><files>tests/conftest.py</files><files>tests/test_critical.py</files></git_log_commit><git_log_commit><date>2025-09-15 01:35:00 -0700</date><message>task 1 complete</message><files>DMAP.md</files><files>architecture-analysis-sept15.md</files><files>build-verification.sh</files><files>claudable/config.json</files><files>docker/docker-compose.yml</files><files>docker/package.json</files><files>docker/rag-http-wrapper.js</files><files>docker/search-http-wrapper.js</files><files>docker/validate-infrastructure.sh</files><files>poetry.lock</files><files>pyproject.toml</files><files>tasks/dsp-task-list-VM.md</files><files>tests/__pycache__/test_critical.cpython-313-pytest-7.4.4.pyc</files><files>tests/test_critical.py</files></git_log_commit><git_log_commit><date>2025-09-15 01:01:32 -0700</date><message>local migration complete</message><files>architecture-analysis-sept15.md</files><files>docker/docker-compose.yml</files><files>docker/qdrant_storage/aliases/data.json</files><files>docker/qdrant_storage/raft_state.json</files><files>tasks/dsp-task-list-VM.md</files><files>tasks/dsp-task-list.md</files></git_log_commit><git_log_commit><date>2025-09-14 22:30:49 -0700</date><message>migrating to host</message><files>CLAUDE.md</files><files>claudable/config.json</files><files>tasks/tasks-dsp-agent-implementation.md</files></git_log_commit><git_log_commit><date>2025-09-14 21:15:26 -0700</date><message>basic test suite written</message><files>DMAP.md</files><files>docs/templates/README.md</files><files>tasks/dsp-task-list.md</files></git_log_commit><git_log_commit><date>2025-09-14 20:44:29 -0700</date><message>tasks test agent start</message><files>CLAUDE.md</files><files>DMAP.md</files><files>claudable/config.json</files><files>tasks/tasks-dsp-agent-implementation.md</files><files>tests/test_critical.py</files></git_log_commit><git_log_commit><date>2025-09-14 20:32:46 -0700</date><message>new task list</message><files>.claude/agents/dyson-test-agent.md</files><files>0912archreport.md</files><files>docs/tasks/TASKS-COMPLETE.md</files><files>tasks/dsp-task-list.md</files><files>tasks/tasks-dsp-agent-implementation.md</files></git_log_commit><git_log_commit><date>2025-09-12 13:38:22 -0700</date><message>fixed n8n nonuse mess</message><files>.claude/agents/dyson-codebase-arch.md</files><files>.claude/agents/dyson-codebase-archv1.txt</files><files>SETUP.md</files><files>docs/README.md</files><files>docs/agent_specs/template_arch_agent.md</files><files>docs/tasks/TASKS-COMPLETE.md</files><files>docs/tasks/dsp-task-list.md</files><files>docs/templates/create-prd.md</files></git_log_commit><git_log_commit><date>2025-09-12 13:17:11 -0700</date><message>prelim arch agent report</message><files>0912archreport.md</files><files>CLAUDE.md</files><files>README.md</files><files>docs/tasks/dsp-task-list.md</files></git_log_commit><git_log_commit><date>2025-09-12 12:39:24 -0700</date><message>more specs</message><files>.claude/agents/dyson-codebase-arch.md</files><files>.claude/agents/dyson-codebase-archv1.txt</files><files>docs/agent_specs/template_arch_agent.md</files><files>docs/agent_specs/templates-ag/template_test_agentv2.md</files><files>docs/agent_specs/zOld/defi_architect_agent.md</files><files>docs/agent_specs/zOld/defi_test_fix_agent.md</files><files>docs/agent_specs/zOld/dsp-agent-prd.md</files></git_log_commit><git_log_commit><date>2025-09-12 12:07:09 -0700</date><message>specs</message><files>.kilocode/mcp.json</files><files>docs/agent_specs/dsp-agent-prd-updated.md</files><files>docs/agent_specs/template_test_agentv2.md</files><files>docs/agent_specs/template_test_agentv3.md</files><files>docs/tasks/overall-dsp-tasks.md</files></git_log_commit><git_log_commit><date>2025-09-11 14:44:35 -0700</date><message>Update .gitignore, remove clean-env.sh, add new agent specifications and templates</message><files>.gitignore</files><files>WARP.md</files><files>clean-env.sh</files><files>docs/agent_specs/AGENT_SPEC_V6.md</files><files>docs/agent_specs/defi_architect_agent.md</files><files>docs/agent_specs/defi_test_fix_agent.md</files><files>docs/agent_specs/dsp-agent-prd.md</files><files>docs/templates/README.md</files><files>docs/templates/create-prd.md</files><files>docs/templates/generate-tasks.md</files><files>docs/templates/process-task-list.md</files><files>docs/templates/templates_PRD_tasks.zip</files></git_log_commit><git_log_commit><date>2025-09-08 18:28:05 -0700</date><message>mcp cursor json empty</message><files>.vscode/mcp.json</files></git_log_commit><git_log_commit><date>2025-09-07 19:03:55 -0700</date><message>sequa mcp setup</message><files>.vscode/mcp.json</files></git_log_commit><git_log_commit><date>2025-09-07 07:34:27 -0700</date><message>cleaned up</message><files>README-COMPLETE.md</files><files>README.md</files><files>claudable/config.json</files><files>clean-env.sh</files><files>docker/docker-compose.yml</files><files>docker/health-check.sh</files><files>docker/manage-mcp.sh</files><files>docker/mcp-query.sh</files><files>docs/tasks/TASKS-COMPLETE.md</files><files>n8n/dsp-agent-workflow.json</files><files>scrape-docs.sh</files><files>setup.sh</files><files>test-setup.sh</files><files>tools/docker-enum.sh</files></git_log_commit><git_log_commit><date>2025-09-07 07:22:26 -0700</date><message>rm n8n</message><files>ENV-SETUP.md</files><files>SETUP.md</files><files>docs/README.md</files><files>docs/dsp-agent-prd.md</files><files>docs/tasks/TASKS-COMPLETE.md</files><files>docs/tasks/dsp-task-list.md</files><files>docs/tasks/overall-dsp-tasks.md</files></git_log_commit><git_log_commit><date>2025-09-06 19:50:45 -0700</date><message>repo files created</message><files>.gitignore</files><files>ENV-SETUP.md</files><files>README-COMPLETE.md</files><files>SETUP.md</files><files>TASKS-COMPLETE.md</files><files>docker/docker-compose.yml</files><files>docker/health-check.sh</files><files>docker/manage-mcp.sh</files><files>docker/mcp-query.sh</files><files>n8n/dsp-agent-workflow.json</files><files>scrape-docs.sh</files><files>test-setup.sh</files></git_log_commit><git_log_commit><date>2025-09-06 19:22:24 -0700</date><message>tasks</message><files>.vscode/settings.json</files><files>CLAUDE.md</files><files>README.md</files><files>dsp-agent-prd.md</files><files>dsp-task-list.md</files><files>overall-dsp-tasks.md</files><files>setup.sh</files></git_log_commit><git_log_commit><date>2025-09-06 18:03:11 -0700</date><message>Initial commit</message><files>.gitignore</files><files>LICENSE</files><files>README.md</files></git_log_commit></git_logs></repomix>